{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/envs/vertex/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from data_provider.m4 import M4Meta\n",
    "from models import Autoformer, DLinear\n",
    "from models import TimeLLM_vertex as TimeLLM\n",
    "\n",
    "from data_provider.data_factory import data_provider\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.losses import smape_loss\n",
    "from utils.m4_summary import M4Summary\n",
    "import os\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "from utils.tools import del_files, EarlyStopping, adjust_learning_rate, load_content, test_MS\n",
    "\n",
    "from data_provider.ean_global_channel import import_true_promo, import_all, check_saved_standardization_data, delete_saved_standardization_data\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(2.0 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))) * 100\n",
    "\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_MS(args, accelerator, model, train_loader, vali_loader, criterion):\n",
    "    x, _ = train_loader.dataset.last_insample_window()\n",
    "    y = vali_loader.dataset.timeseries\n",
    "    x = torch.tensor(x, dtype=torch.float32).to(accelerator.device)\n",
    "    print(\"Shape of X eval\", x.shape)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        B, _, C = x.shape\n",
    "        dec_inp = torch.zeros((B, args.pred_len, C)).float().to(accelerator.device)\n",
    "        dec_inp = torch.cat([x[:, -args.label_len:, :], dec_inp], dim=1)\n",
    "        outputs = torch.zeros((B, args.pred_len, C)).float().to(accelerator.device)\n",
    "        id_list = np.arange(0, B, args.eval_batch_size)\n",
    "        id_list = np.append(id_list, B)\n",
    "        for i in range(len(id_list) - 1):\n",
    "            outputs[id_list[i]:id_list[i + 1], :, :] = model(\n",
    "                x[id_list[i]:id_list[i + 1]],\n",
    "                None,\n",
    "                dec_inp[id_list[i]:id_list[i + 1]],\n",
    "                None\n",
    "            )\n",
    "        accelerator.wait_for_everyone()\n",
    "        outputs = accelerator.gather_for_metrics(outputs)\n",
    "        print(\"Shape of output eval before choosing\", outputs.shape)\n",
    "        f_dim = -1 if args.features == 'MS' else 0\n",
    "        outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "        pred = outputs\n",
    "        true = torch.from_numpy(np.array(y)).to(accelerator.device)\n",
    "        true = true[:, -args.pred_len:, f_dim:]\n",
    "        print(\"Shape of y eval\", true.shape)\n",
    "        batch_y_mark = torch.ones(true.shape).to(accelerator.device)\n",
    "        true = accelerator.gather_for_metrics(true)\n",
    "        batch_y_mark = accelerator.gather_for_metrics(batch_y_mark)\n",
    "\n",
    "        loss = criterion(None, 0, pred, true, batch_y_mark)\n",
    "\n",
    "    model.train()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import LlamaConfig, LlamaModel, LlamaTokenizer, GPT2Config, GPT2Model, GPT2Tokenizer, BertConfig, \\\n",
    "    BertModel, BertTokenizer\n",
    "from layers.Embed import PatchEmbedding\n",
    "import transformers\n",
    "from layers.StandardNorm import Normalize\n",
    "from vertexai.preview import VertexModel # VertexModel\n",
    "import vertexai\n",
    "from utils.tools import del_files, EarlyStopping, adjust_learning_rate, vali, load_content, test_MS\n",
    "\n",
    "\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "\n",
    "from data_provider.data_factory import data_provider\n",
    "from utils.losses import smape_loss\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "class FlattenHead(nn.Module):\n",
    "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_vars = n_vars\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.linear = nn.Linear(nf, target_window)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module,VertexModel):\n",
    "\n",
    "    def __init__(self, configs, patch_len=16, stride=8):\n",
    "        nn.Module.__init__(self)\n",
    "        VertexModel.__init__(self)\n",
    "        self.task_name = configs.task_name\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.d_ff = configs.d_ff\n",
    "        self.top_k = 5\n",
    "        self.d_llm = configs.llm_dim\n",
    "        self.patch_len = configs.patch_len\n",
    "        self.stride = configs.stride\n",
    "        self.args = configs\n",
    "\n",
    "        if configs.llm_model == 'LLAMA':\n",
    "            # self.llama_config = LlamaConfig.from_pretrained('/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/')\n",
    "            self.llama_config = LlamaConfig.from_pretrained('huggyllama/llama-7b')\n",
    "            self.llama_config.num_hidden_layers = configs.llm_layers\n",
    "            self.llama_config.output_attentions = True\n",
    "            self.llama_config.output_hidden_states = True\n",
    "            try:\n",
    "                self.llm_model = LlamaModel.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    config=self.llama_config,\n",
    "                    # load_in_4bit=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads model from HF is not already done\n",
    "                print(\"Local model files not found. Attempting to download...\")\n",
    "                self.llm_model = LlamaModel.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.llama_config,\n",
    "                    # load_in_4bit=True\n",
    "                )\n",
    "            try:\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/tokenizer.model\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads the tokenizer from HF if not already done\n",
    "                print(\"Local tokenizer files not found. Atempting to download them..\")\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/tokenizer.model\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "        elif configs.llm_model == 'GPT2':\n",
    "            self.gpt2_config = GPT2Config.from_pretrained('openai-community/gpt2')\n",
    "\n",
    "            self.gpt2_config.num_hidden_layers = configs.llm_layers\n",
    "            self.gpt2_config.output_attentions = True\n",
    "            self.gpt2_config.output_hidden_states = True\n",
    "            try:\n",
    "                self.llm_model = GPT2Model.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    config=self.gpt2_config,\n",
    "                )\n",
    "            except EnvironmentError:  # downloads model from HF is not already done\n",
    "                print(\"Local model files not found. Attempting to download...\")\n",
    "                self.llm_model = GPT2Model.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.gpt2_config,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads the tokenizer from HF if not already done\n",
    "                print(\"Local tokenizer files not found. Atempting to download them..\")\n",
    "                self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "        elif configs.llm_model == 'BERT':\n",
    "            self.bert_config = BertConfig.from_pretrained('google-bert/bert-base-uncased')\n",
    "\n",
    "            self.bert_config.num_hidden_layers = configs.llm_layers\n",
    "            self.bert_config.output_attentions = True\n",
    "            self.bert_config.output_hidden_states = True\n",
    "            try:\n",
    "                self.llm_model = BertModel.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    config=self.bert_config,\n",
    "                )\n",
    "            except EnvironmentError:  # downloads model from HF is not already done\n",
    "                print(\"Local model files not found. Attempting to download...\")\n",
    "                self.llm_model = BertModel.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.bert_config,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                self.tokenizer = BertTokenizer.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads the tokenizer from HF if not already done\n",
    "                print(\"Local tokenizer files not found. Atempting to download them..\")\n",
    "                self.tokenizer = BertTokenizer.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "        else:\n",
    "            raise Exception('LLM model is not defined')\n",
    "\n",
    "        if self.tokenizer.eos_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        else:\n",
    "            pad_token = '[PAD]'\n",
    "            self.tokenizer.add_special_tokens({'pad_token': pad_token})\n",
    "            self.tokenizer.pad_token = pad_token\n",
    "\n",
    "        for param in self.llm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if configs.prompt_domain:\n",
    "            self.description = configs.content\n",
    "        else:\n",
    "            self.description = 'The Electricity Transformer Temperature (ETT) is a crucial indicator in the electric power long-term deployment.'\n",
    "\n",
    "        self.dropout = nn.Dropout(configs.dropout)\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            configs.d_model, self.patch_len, self.stride, configs.dropout)\n",
    "\n",
    "        self.word_embeddings = self.llm_model.get_input_embeddings().weight\n",
    "        self.vocab_size = self.word_embeddings.shape[0]\n",
    "        self.num_tokens = 1000\n",
    "        self.mapping_layer = nn.Linear(self.vocab_size, self.num_tokens)\n",
    "\n",
    "        self.reprogramming_layer = ReprogrammingLayer(configs.d_model, configs.n_heads, self.d_ff, self.d_llm)\n",
    "\n",
    "        self.patch_nums = int((configs.seq_len - self.patch_len) / self.stride + 2)\n",
    "        self.head_nf = self.d_ff * self.patch_nums\n",
    "\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            self.output_projection = FlattenHead(configs.enc_in, self.head_nf, self.pred_len,\n",
    "                                                 head_dropout=configs.dropout)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.normalize_layers = Normalize(configs.enc_in, affine=False)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "            return dec_out[:, -self.pred_len:, :]\n",
    "        return None\n",
    "\n",
    "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "\n",
    "        x_enc = self.normalize_layers(x_enc, 'norm')\n",
    "\n",
    "        B, T, N = x_enc.size()\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n",
    "\n",
    "        min_values = torch.min(x_enc, dim=1)[0]\n",
    "        max_values = torch.max(x_enc, dim=1)[0]\n",
    "        medians = torch.median(x_enc, dim=1).values\n",
    "        lags = self.calcute_lags(x_enc)\n",
    "        trends = x_enc.diff(dim=1).sum(dim=1)\n",
    "\n",
    "        prompt = []\n",
    "        for b in range(x_enc.shape[0]):\n",
    "            min_values_str = str(min_values[b].tolist()[0])\n",
    "            max_values_str = str(max_values[b].tolist()[0])\n",
    "            median_values_str = str(medians[b].tolist()[0])\n",
    "            lags_values_str = str(lags[b].tolist())\n",
    "            prompt_ = (\n",
    "                f\"<|start_prompt|>Dataset description: {self.description}\"\n",
    "                f\"Task description: forecast the next {str(self.pred_len)} steps given the previous {str(self.seq_len)} steps information; \"\n",
    "                \"Input statistics: \"\n",
    "                f\"min value {min_values_str}, \"\n",
    "                f\"max value {max_values_str}, \"\n",
    "                f\"median value {median_values_str}, \"\n",
    "                f\"the trend of input is {'upward' if trends[b] > 0 else 'downward'}, \"\n",
    "                f\"top 5 lags are : {lags_values_str}<|<end_prompt>|>\"\n",
    "            )\n",
    "\n",
    "            prompt.append(prompt_)\n",
    "\n",
    "        x_enc = x_enc.reshape(B, N, T).permute(0, 2, 1).contiguous() # B, T, N\n",
    "\n",
    "        prompt = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).input_ids\n",
    "        prompt_embeddings = self.llm_model.get_input_embeddings()(prompt.to(x_enc.device))  # (batch, prompt_token, dim)\n",
    "\n",
    "        source_embeddings = self.mapping_layer(self.word_embeddings.permute(1, 0)).permute(1, 0)\n",
    "\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous() # B N T\n",
    "        enc_out, n_vars = self.patch_embedding(x_enc.to(torch.bfloat16))\n",
    "        enc_out = self.reprogramming_layer(enc_out, source_embeddings, source_embeddings)\n",
    "        llama_enc_out = torch.cat([prompt_embeddings, enc_out], dim=1)\n",
    "        dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state\n",
    "        dec_out = dec_out[:, :, :self.d_ff]\n",
    "\n",
    "        dec_out = torch.reshape(\n",
    "            dec_out, (-1, n_vars, dec_out.shape[-2], dec_out.shape[-1]))\n",
    "        dec_out = dec_out.permute(0, 1, 3, 2).contiguous()\n",
    "\n",
    "        dec_out = self.output_projection(dec_out[:, :, :, -self.patch_nums:])\n",
    "        dec_out = dec_out.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        dec_out = self.normalize_layers(dec_out, 'denorm')\n",
    "\n",
    "        return dec_out\n",
    "\n",
    "    @vertexai.preview.developer.mark.train()\n",
    "    def train_model(self, train_loader, test_loader, vali_loader, path):\n",
    "        # import torch.multiprocessing as mp\n",
    "        # mp.set_start_method('spawn', force=True)\n",
    "        ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "        deepspeed_plugin = DeepSpeedPlugin(hf_ds_config='./ds_config_zero2.json')\n",
    "        accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
    "        print(\"Accelerator initialized:\", accelerator)\n",
    "\n",
    "        # ## Load datasets \n",
    "        # train_data, train_loader = data_provider(self.args, 'train')\n",
    "        # vali_data, vali_loader = data_provider(self.args, 'val')\n",
    "        # test_data, test_loader = data_provider(self.args, 'test')\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        self.args.content = load_content(self.args)\n",
    "        time_now = time.time()\n",
    "\n",
    "        train_steps = len(train_loader)\n",
    "        early_stopping = EarlyStopping(accelerator=accelerator, patience=self.args.patience)\n",
    "\n",
    "        model_optim = optim.Adam(self.parameters(), lr=self.args.learning_rate)\n",
    "        criterion = smape_loss()\n",
    "\n",
    "\n",
    "        if self.args.lradj == 'COS':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=20, eta_min=1e-8)\n",
    "        else:\n",
    "            scheduler = lr_scheduler.OneCycleLR(optimizer=model_optim,\n",
    "                                                steps_per_epoch=train_steps,\n",
    "                                                pct_start=self.args.pct_start,\n",
    "                                                epochs=self.args.train_epochs,\n",
    "                                                max_lr=self.args.learning_rate)\n",
    "    \n",
    "\n",
    "        train_loader, vali_loader, self, model_optim, scheduler = accelerator.prepare(train_loader,\n",
    "                vali_loader, self, model_optim, scheduler)       \n",
    "\n",
    "        for epoch in range(self.args.train_epochs):\n",
    "            # train_data, train_loader = data_provider(self.args, 'train')\n",
    "            # train_loader = accelerator.prepare(train_loader)\n",
    "            \n",
    "            iter_count = 0\n",
    "            train_loss = []\n",
    "\n",
    "            self.train()\n",
    "            epoch_time = time.time()\n",
    "\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "                iter_count += 1\n",
    "                model_optim.zero_grad()\n",
    "                batch_x = batch_x.float().to(accelerator.device)\n",
    "\n",
    "                batch_y = batch_y.float().to(accelerator.device)\n",
    "                batch_y_mark = batch_y_mark.float().to(accelerator.device)\n",
    "\n",
    "                # decoder input\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float().to(accelerator.device)\n",
    "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(\n",
    "                    accelerator.device)\n",
    "\n",
    "                outputs = self(batch_x, None, dec_inp, None)\n",
    "\n",
    "                f_dim = -1 if self.args.features == 'MS' else 0\n",
    "                outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -self.args.pred_len:, f_dim:]\n",
    "\n",
    "                batch_y_mark = batch_y_mark[:, -self.args.pred_len:, f_dim:]\n",
    "                loss = criterion(batch_x, 0, outputs, batch_y, batch_y_mark) # 0 cuz we don't need it\n",
    "\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    accelerator.print(\n",
    "                        \"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item())\n",
    "                    )\n",
    "                    speed = (time.time() - time_now) / iter_count\n",
    "                    left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n",
    "                    accelerator.print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                    iter_count = 0\n",
    "                    time_now = time.time()\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                model_optim.step()\n",
    "\n",
    "                if self.args.lradj == 'TST':\n",
    "                    adjust_learning_rate(accelerator, model_optim, scheduler, epoch + 1, self.args, printout=False)\n",
    "                    scheduler.step()\n",
    "\n",
    "            accelerator.print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "            train_loss = np.average(train_loss)\n",
    "            print('########################################################################')\n",
    "            vali_loss = test_MS(self.args, accelerator, self, train_loader, vali_loader, criterion)\n",
    "            test_loss = vali_loss\n",
    "            accelerator.print(\n",
    "                \"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                    epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "            early_stopping(vali_loss, self, path)  # model saving\n",
    "            if early_stopping.early_stop:\n",
    "                accelerator.print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "            if self.args.lradj != 'TST':\n",
    "                adjust_learning_rate(accelerator, model_optim, scheduler, epoch + 1, self.args, printout=True)\n",
    "            else:\n",
    "                accelerator.print('Updating learning rate to {}'.format(scheduler.get_last_lr()[0]))\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        print(\"Training is Over\")\n",
    "\n",
    "    \n",
    "    def calcute_lags(self, x_enc):\n",
    "        q_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, dim=-1)\n",
    "        mean_value = torch.mean(corr, dim=1)\n",
    "        _, lags = torch.topk(mean_value, self.top_k, dim=-1)\n",
    "        return lags\n",
    "\n",
    "\n",
    "\n",
    "class ReprogrammingLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_keys=None, d_llm=None, attention_dropout=0.1):\n",
    "        super(ReprogrammingLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_llm, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_llm, d_keys * n_heads)\n",
    "        self.out_projection = nn.Linear(d_keys * n_heads, d_llm)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, target_embedding, source_embedding, value_embedding):\n",
    "        B, L, _ = target_embedding.shape\n",
    "        S, _ = source_embedding.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        target_embedding = self.query_projection(target_embedding).view(B, L, H, -1)\n",
    "        source_embedding = self.key_projection(source_embedding).view(S, H, -1)\n",
    "        value_embedding = self.value_projection(value_embedding).view(S, H, -1)\n",
    "\n",
    "        out = self.reprogramming(target_embedding, source_embedding, value_embedding)\n",
    "\n",
    "        out = out.reshape(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out)\n",
    "\n",
    "    def reprogramming(self, target_embedding, source_embedding, value_embedding):\n",
    "        B, L, H, E = target_embedding.shape\n",
    "\n",
    "        scale = 1. / sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,she->bhls\", target_embedding, source_embedding)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        reprogramming_embedding = torch.einsum(\"bhls,she->blhe\", A, value_embedding)\n",
    "\n",
    "        return reprogramming_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.task_name = 'short_term_forecast'\n",
    "        self.is_training = 1\n",
    "        self.model_id = 'promo_ean_channel'\n",
    "        self.model_comment = 'EAN_Channel'\n",
    "        self.model = 'TimeLLM'\n",
    "        self.seed = 2021  # Assuming the seed is not explicitly set in the script\n",
    "        self.data = 'promo_ean_channel'\n",
    "        self.root_path = './dataset/true_promo'\n",
    "        self.data_path = 'all_product_true_promo_train.csv'\n",
    "        self.features = 'MS'\n",
    "        self.target = 'sold_units'\n",
    "        self.loader = 'modal'  # Assuming this is a default value\n",
    "        self.freq = 'h'  # Assuming this is a default value\n",
    "        self.checkpoints = './checkpoints/'  # Assuming this is a default value\n",
    "        self.seq_len = 13  # Assuming this is a default value\n",
    "        self.label_len = 1  # Assuming this is a default value\n",
    "        self.pred_len = 17\n",
    "        self.seasonal_patterns = 'Monthly'  # Assuming this is a default value\n",
    "        self.enc_in = 9\n",
    "        self.dec_in = 9\n",
    "        self.c_out = 9\n",
    "        self.d_model = 32\n",
    "        self.n_heads = 8  # Typically set by your model configuration\n",
    "        self.e_layers = 2  # Typically set by your model configuration\n",
    "        self.d_layers = 1  # Typically set by your model configuration\n",
    "        self.d_ff = 128\n",
    "        self.moving_avg = 25  # Assuming default if not specified in the script\n",
    "        self.factor = 3\n",
    "        self.dropout = 0.1  # Assuming default if not specified\n",
    "        self.embed = 'timeF'  # Assuming default if not specified\n",
    "        self.activation = 'gelu'  # Assuming default if not specified\n",
    "        self.output_attention = False  # Assuming default if not specified\n",
    "        self.patch_len = 1\n",
    "        self.stride = 8  # Assuming default if not specified\n",
    "        self.prompt_domain = 0  # Assuming default if not specified\n",
    "        self.llm_model = 'GPT2'\n",
    "        self.llm_dim = 768\n",
    "        self.num_workers = 10  # Default setting\n",
    "        self.itr = 1\n",
    "        self.train_epochs = 2\n",
    "        self.align_epochs = 10  # Assuming default if not specified\n",
    "        self.batch_size = 1\n",
    "        self.eval_batch_size = 1  # Assuming default if not specified\n",
    "        self.patience = 10  # Assuming default if not specified\n",
    "        self.learning_rate = 0.001\n",
    "        self.des = 'Exp'\n",
    "        self.loss = 'spmae'  # Assuming default if not specified\n",
    "        self.lradj = 'type1'  # Assuming default if not specified\n",
    "        self.pct_start = 0.2  # Assuming default if not specified\n",
    "        self.use_amp = False  # Assuming default based on your environment capabilities\n",
    "        self.llm_layers = 32\n",
    "        self.percent = 100  # Assuming default if not specified\n",
    "        self.zero_percent = 0\n",
    "        self.interpolation = False\n",
    "        self.interpolation_method = False\n",
    "        self.fill_discontinuity = False\n",
    "        self.month = 11\n",
    "        self.num_weeks = 100\n",
    "        self.scale=True\n",
    "        self.embedding = True\n",
    "        self.embedding_dimension = 2\n",
    "        self.keep_non_promo = False\n",
    "        self.channel = 'Offline'\n",
    "        \n",
    "\n",
    "# Instantiate the Args\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for number of weeks to take\n",
      "number of products before preprocessing 1510\n",
      "How many ean_global_channel_type: 1510\n",
      "final data product (if changed we remove discontinuity) 1429\n",
      "prediction length: 17\n",
      "Ended up with  68\n",
      "Let's Load the Data\n",
      "number of products before preprocessing 1510\n",
      "How many ean_global_channel_type: 1332\n",
      "final data product (if changed we remove discontinuity) 1314\n",
      "prediction length: 17\n",
      "Train set saved to: dataset/true_promo/OfflineChannel_Month11_68Weeks_scaled_embedding_2/short_term_forecast_promo_ean_channel_TimeLLM_promo_ean_channel_ftMS_sl13_ll17_pl17_dm32_nh8_el2_dl1_df128_fc3_ebtimeF_Exp/train.csv\n",
      "Test set saved to: dataset/true_promo/OfflineChannel_Month11_68Weeks_scaled_embedding_2/short_term_forecast_promo_ean_channel_TimeLLM_promo_ean_channel_ftMS_sl13_ll17_pl17_dm32_nh8_el2_dl1_df128_fc3_ebtimeF_Exp/test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PROJECT_ID = \"itg-bpma-gbl-ww-np\"  # @param {type:\"string\"}\n",
    "REGION = \"europe-west1\" \n",
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}\n",
    "import vertexai\n",
    "REMOTE_JOB_NAME = \"timeseriesllm\"\n",
    "REMOTE_JOB_BUCKET = f\"{BUCKET_URI}/{REMOTE_JOB_NAME}\"\n",
    "##################################################################################################\n",
    "vertexai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=REMOTE_JOB_BUCKET,\n",
    ")\n",
    "\n",
    "##################################################################################################\n",
    "bq_client = bigquery.Client(\n",
    "    project=PROJECT_ID,  # GCP project used for running the queries and billing\n",
    ")\n",
    "\n",
    "#################################################################################################\n",
    "print(\"Looking for number of weeks to take\")\n",
    "_,_,_,pred_len = import_true_promo(\n",
    "        client=bq_client,\n",
    "        zero_percent=0,\n",
    "        month=args.month,\n",
    "        num_weeks=0,\n",
    "        channel=args.channel,\n",
    "        fill_discontinuity=args.fill_discontinuity,\n",
    "        keep_non_promo=args.keep_non_promo\n",
    "    )\n",
    "print(\"Ended up with \", 4*pred_len)\n",
    "args.num_weeks=4*pred_len\n",
    "args.pred_len = pred_len\n",
    "args.label_len = pred_len\n",
    "print(\"Let's Load the Data\")\n",
    "if args.interpolation:\n",
    "    final_data, train_set, test_set, pred_len = import_all(\n",
    "        client=bq_client,\n",
    "        zero_percent=args.zero_percent,\n",
    "        month=args.month,\n",
    "        num_weeks=args.num_weeks,\n",
    "        channel=args.channel,\n",
    "        fill_discontinuity=args.fill_discontinuity,\n",
    "        keep_non_promo=args.keep_non_promo,\n",
    "        interpolation_method=args.interpolation_method\n",
    "    )\n",
    "else :\n",
    "    final_data, train_set, test_set, pred_len = import_true_promo(\n",
    "        client=bq_client,\n",
    "        zero_percent=args.zero_percent,\n",
    "        month=args.month,\n",
    "        num_weeks=args.num_weeks,\n",
    "        channel=args.channel,\n",
    "        fill_discontinuity=args.fill_discontinuity,\n",
    "        keep_non_promo=args.keep_non_promo\n",
    "    )\n",
    "\n",
    "\n",
    "################## \n",
    "setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.des)\n",
    "# Construct the path\n",
    "base_dir = f\"dataset/\"\n",
    "if args.interpolation:\n",
    "    base_dir += f\"interpolation_{args.interpolation_method}/\"\n",
    "else : \n",
    "    base_dir += f\"true_promo/\"\n",
    "\n",
    "base_dir+= f\"{args.channel}Channel_Month{args.month}_{args.num_weeks}Weeks\"\n",
    "if args.fill_discontinuity:\n",
    "    base_dir += \"_filldiscont\"\n",
    "if args.keep_non_promo:\n",
    "    base_dir += \"_keepnonpromo\"\n",
    "if args.scale:\n",
    "    base_dir+=\"_scaled\"\n",
    "if args.embedding:\n",
    "    base_dir+=f\"_embedding_{args.embedding_dimension}\"\n",
    "base_dir += '/'+setting\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(base_dir, \"train.csv\")\n",
    "test_path = os.path.join(base_dir, \"test.csv\")\n",
    "\n",
    "\n",
    "train_set.to_csv(train_path, index=False)\n",
    "test_set.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"Train set saved to: {train_path}\")\n",
    "print(f\"Test set saved to: {test_path}\")\n",
    "if args.scale:\n",
    "     \n",
    "    args.scale_path = 'scale_path/' + base_dir[8:]\n",
    "    if check_saved_standardization_data(args.scale_path):\n",
    "        delete_saved_standardization_data(args.scale_path)\n",
    "\n",
    "\n",
    "########################################################### configuration ####################\n",
    "args.pred_len = pred_len\n",
    "args.label_len = args.pred_len\n",
    "args.seq_len = int(2*args.pred_len)\n",
    "args.root_path = base_dir\n",
    "args.data_path = 'train.csv'\n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "standarization dictionaries are created inscale_path/true_promo/OfflineChannel_Month11_68Weeks_scaled_embedding_2/short_term_forecast_promo_ean_channel_TimeLLM_promo_ean_channel_ftMS_sl13_ll17_pl17_dm32_nh8_el2_dl1_df128_fc3_ebtimeF_Exp\n",
      "scaling the data of train\n",
      "standarization is over of train\n",
      "scaling the data of test\n",
      "standarization is over of test\n",
      "Remote job created. View the job: https://console.cloud.google.com/ai/platform/locations/europe-west1/training/6702149680605691904?project=238069609727\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Remote job failed with:\ncode: 3\nmessage: \"The replica workerpool0-0 exited with a non-zero status of 1. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=238069609727&resource=ml_job%2Fjob_id%2F6702149680605691904&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%226702149680605691904%22\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain_model\u001b[38;5;241m.\u001b[39mvertex\u001b[38;5;241m.\u001b[39mremote_config\u001b[38;5;241m.\u001b[39menable_cuda \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     41\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain_model\u001b[38;5;241m.\u001b[39mvertex\u001b[38;5;241m.\u001b[39mremote_config\u001b[38;5;241m.\u001b[39maccelerator_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m---> 42\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#torch.save(model.state_dict(), path + '/' + 'checkpoint_v_test1')\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda/envs/vertex/lib/python3.10/site-packages/vertexai/preview/_workflow/driver/__init__.py:113\u001b[0m, in \u001b[0;36mVertexRemoteFunctor.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# NOTE: may also need to handle the case of\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# bound_args.arguments.get(\"self\"),\u001b[39;00m\n\u001b[1;32m    104\u001b[0m invokable \u001b[38;5;241m=\u001b[39m shared\u001b[38;5;241m.\u001b[39m_Invokable(\n\u001b[1;32m    105\u001b[0m     instance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__self__\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    106\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_method,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m     vertex_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvertex,\n\u001b[1;32m    111\u001b[0m )\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_workflow_driver\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43minvokable\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda/envs/vertex/lib/python3.10/site-packages/vertexai/preview/_workflow/driver/__init__.py:261\u001b[0m, in \u001b[0;36m_WorkFlowDriver.invoke\u001b[0;34m(self, invokable)\u001b[0m\n\u001b[1;32m    258\u001b[0m     rewrapper(result)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstate\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;129;01min\u001b[39;00m jobs\u001b[38;5;241m.\u001b[39m_JOB_ERROR_STATES:\n\u001b[0;32m--> 261\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRemote job failed with:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m result\u001b[38;5;241m.\u001b[39merror)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Remote job failed with:\ncode: 3\nmessage: \"The replica workerpool0-0 exited with a non-zero status of 1. To find out more about why your job exited please check the logs: https://console.cloud.google.com/logs/viewer?project=238069609727&resource=ml_job%2Fjob_id%2F6702149680605691904&advancedFilter=resource.type%3D%22ml_job%22%0Aresource.labels.job_id%3D%226702149680605691904%22\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.des, ii)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    " \n",
    "    model = Model(args).float()\n",
    "\n",
    "    path = os.path.join(args.checkpoints,\n",
    "                        base_dir[8:] + '_' + str(ii) + '-' + args.model_comment)  # unique checkpoint saving path\n",
    "    args.content = load_content(args)\n",
    "    if not os.path.exists(path) :# and accelerator.is_local_main_process:\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "    train_data, train_loader = data_provider(args, 'train')\n",
    "    test_data, test_loader = data_provider(args, 'test')\n",
    "\n",
    "    vertexai.preview.init(remote=True)\n",
    "    model.train_model.vertex.remote_config.container_uri = \"europe-west1-docker.pkg.dev/itg-bpma-gbl-ww-np/timeseriesforecasting/torch-train:latest\"\n",
    "    model.train_model.vertex.remote_config.enable_cuda = True\n",
    "    model.train_model.vertex.remote_config.accelerator_count = 4\n",
    "    model.train_model(train_loader, test_loader, test_loader,path)\n",
    "    #torch.save(model.state_dict(), path + '/' + 'checkpoint_v_test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vertex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
