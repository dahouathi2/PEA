{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/envs/vertex/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from data_provider.m4 import M4Meta\n",
    "from models import Autoformer, DLinear\n",
    "from models import TimeLLM_vertex as TimeLLM\n",
    "\n",
    "from data_provider.data_factory import data_provider\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils.losses import smape_loss\n",
    "from utils.m4_summary import M4Summary\n",
    "import os\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "from utils.tools import del_files, EarlyStopping, adjust_learning_rate, load_content, test_MS\n",
    "\n",
    "from data_provider.ean_global_channel import import_true_promo, import_all, check_saved_standardization_data, delete_saved_standardization_data\n",
    "from google.cloud import bigquery\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "def symmetric_mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(2.0 * np.abs(y_true - y_pred) / (np.abs(y_true) + np.abs(y_pred))) * 100\n",
    "\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def test_MS(args, accelerator, model, train_loader, vali_loader, criterion):\n",
    "    x, _ = train_loader.dataset.last_insample_window()\n",
    "    y = vali_loader.dataset.timeseries\n",
    "    x = torch.tensor(x, dtype=torch.float32).to(accelerator.device)\n",
    "    print(\"Shape of X eval\", x.shape)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        B, _, C = x.shape\n",
    "        dec_inp = torch.zeros((B, args.pred_len, C)).float().to(accelerator.device)\n",
    "        dec_inp = torch.cat([x[:, -args.label_len:, :], dec_inp], dim=1)\n",
    "        outputs = torch.zeros((B, args.pred_len, C)).float().to(accelerator.device)\n",
    "        id_list = np.arange(0, B, args.eval_batch_size)\n",
    "        id_list = np.append(id_list, B)\n",
    "        for i in range(len(id_list) - 1):\n",
    "            outputs[id_list[i]:id_list[i + 1], :, :] = model(\n",
    "                x[id_list[i]:id_list[i + 1]],\n",
    "                None,\n",
    "                dec_inp[id_list[i]:id_list[i + 1]],\n",
    "                None\n",
    "            )\n",
    "        accelerator.wait_for_everyone()\n",
    "        outputs = accelerator.gather_for_metrics(outputs)\n",
    "        print(\"Shape of output eval before choosing\", outputs.shape)\n",
    "        f_dim = -1 if args.features == 'MS' else 0\n",
    "        outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "        pred = outputs\n",
    "        true = torch.from_numpy(np.array(y)).to(accelerator.device)\n",
    "        true = true[:, -args.pred_len:, f_dim:]\n",
    "        print(\"Shape of y eval\", true.shape)\n",
    "        batch_y_mark = torch.ones(true.shape).to(accelerator.device)\n",
    "        true = accelerator.gather_for_metrics(true)\n",
    "        batch_y_mark = accelerator.gather_for_metrics(batch_y_mark)\n",
    "\n",
    "        loss = criterion(None, 0, pred, true, batch_y_mark)\n",
    "\n",
    "    model.train()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import LlamaConfig, LlamaModel, LlamaTokenizer, GPT2Config, GPT2Model, GPT2Tokenizer, BertConfig, \\\n",
    "    BertModel, BertTokenizer\n",
    "from layers.Embed import PatchEmbedding\n",
    "import transformers\n",
    "from layers.StandardNorm import Normalize\n",
    "from vertexai.preview import VertexModel # VertexModel\n",
    "import vertexai\n",
    "from utils.tools import del_files, EarlyStopping, adjust_learning_rate, vali, load_content, test_MS\n",
    "\n",
    "\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "\n",
    "from data_provider.data_factory import data_provider\n",
    "from utils.losses import smape_loss\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "class FlattenHead(nn.Module):\n",
    "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_vars = n_vars\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.linear = nn.Linear(nf, target_window)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module,VertexModel):\n",
    "\n",
    "    def __init__(self, configs, patch_len=16, stride=8):\n",
    "        nn.Module.__init__(self)\n",
    "        VertexModel.__init__(self)\n",
    "        self.task_name = configs.task_name\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.d_ff = configs.d_ff\n",
    "        self.top_k = 5\n",
    "        self.d_llm = configs.llm_dim\n",
    "        self.patch_len = configs.patch_len\n",
    "        self.stride = configs.stride\n",
    "        self.args = configs\n",
    "\n",
    "        if configs.llm_model == 'LLAMA':\n",
    "            # self.llama_config = LlamaConfig.from_pretrained('/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/')\n",
    "            self.llama_config = LlamaConfig.from_pretrained('huggyllama/llama-7b')\n",
    "            self.llama_config.num_hidden_layers = configs.llm_layers\n",
    "            self.llama_config.output_attentions = True\n",
    "            self.llama_config.output_hidden_states = True\n",
    "            try:\n",
    "                self.llm_model = LlamaModel.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    config=self.llama_config,\n",
    "                    # load_in_4bit=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads model from HF is not already done\n",
    "                print(\"Local model files not found. Attempting to download...\")\n",
    "                self.llm_model = LlamaModel.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.llama_config,\n",
    "                    # load_in_4bit=True\n",
    "                )\n",
    "            try:\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/tokenizer.model\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads the tokenizer from HF if not already done\n",
    "                print(\"Local tokenizer files not found. Atempting to download them..\")\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/tokenizer.model\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "        elif configs.llm_model == 'GPT2':\n",
    "            self.gpt2_config = GPT2Config.from_pretrained('openai-community/gpt2')\n",
    "\n",
    "            self.gpt2_config.num_hidden_layers = configs.llm_layers\n",
    "            self.gpt2_config.output_attentions = True\n",
    "            self.gpt2_config.output_hidden_states = True\n",
    "            try:\n",
    "                self.llm_model = GPT2Model.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    config=self.gpt2_config,\n",
    "                )\n",
    "            except EnvironmentError:  # downloads model from HF is not already done\n",
    "                print(\"Local model files not found. Attempting to download...\")\n",
    "                self.llm_model = GPT2Model.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.gpt2_config,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads the tokenizer from HF if not already done\n",
    "                print(\"Local tokenizer files not found. Atempting to download them..\")\n",
    "                self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "        elif configs.llm_model == 'BERT':\n",
    "            self.bert_config = BertConfig.from_pretrained('google-bert/bert-base-uncased')\n",
    "\n",
    "            self.bert_config.num_hidden_layers = configs.llm_layers\n",
    "            self.bert_config.output_attentions = True\n",
    "            self.bert_config.output_hidden_states = True\n",
    "            try:\n",
    "                self.llm_model = BertModel.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    config=self.bert_config,\n",
    "                )\n",
    "            except EnvironmentError:  # downloads model from HF is not already done\n",
    "                print(\"Local model files not found. Attempting to download...\")\n",
    "                self.llm_model = BertModel.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.bert_config,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                self.tokenizer = BertTokenizer.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads the tokenizer from HF if not already done\n",
    "                print(\"Local tokenizer files not found. Atempting to download them..\")\n",
    "                self.tokenizer = BertTokenizer.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "        else:\n",
    "            raise Exception('LLM model is not defined')\n",
    "\n",
    "        if self.tokenizer.eos_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        else:\n",
    "            pad_token = '[PAD]'\n",
    "            self.tokenizer.add_special_tokens({'pad_token': pad_token})\n",
    "            self.tokenizer.pad_token = pad_token\n",
    "\n",
    "        for param in self.llm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if configs.prompt_domain:\n",
    "            self.description = configs.content\n",
    "        else:\n",
    "            self.description = 'The Electricity Transformer Temperature (ETT) is a crucial indicator in the electric power long-term deployment.'\n",
    "\n",
    "        self.dropout = nn.Dropout(configs.dropout)\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            configs.d_model, self.patch_len, self.stride, configs.dropout)\n",
    "\n",
    "        self.word_embeddings = self.llm_model.get_input_embeddings().weight\n",
    "        self.vocab_size = self.word_embeddings.shape[0]\n",
    "        self.num_tokens = 1000\n",
    "        self.mapping_layer = nn.Linear(self.vocab_size, self.num_tokens)\n",
    "\n",
    "        self.reprogramming_layer = ReprogrammingLayer(configs.d_model, configs.n_heads, self.d_ff, self.d_llm)\n",
    "\n",
    "        self.patch_nums = int((configs.seq_len - self.patch_len) / self.stride + 2)\n",
    "        self.head_nf = self.d_ff * self.patch_nums\n",
    "\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            self.output_projection = FlattenHead(configs.enc_in, self.head_nf, self.pred_len,\n",
    "                                                 head_dropout=configs.dropout)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.normalize_layers = Normalize(configs.enc_in, affine=False)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "            return dec_out[:, -self.pred_len:, :]\n",
    "        return None\n",
    "\n",
    "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "\n",
    "        x_enc = self.normalize_layers(x_enc, 'norm')\n",
    "\n",
    "        B, T, N = x_enc.size()\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n",
    "\n",
    "        min_values = torch.min(x_enc, dim=1)[0]\n",
    "        max_values = torch.max(x_enc, dim=1)[0]\n",
    "        medians = torch.median(x_enc, dim=1).values\n",
    "        lags = self.calcute_lags(x_enc)\n",
    "        trends = x_enc.diff(dim=1).sum(dim=1)\n",
    "\n",
    "        prompt = []\n",
    "        for b in range(x_enc.shape[0]):\n",
    "            min_values_str = str(min_values[b].tolist()[0])\n",
    "            max_values_str = str(max_values[b].tolist()[0])\n",
    "            median_values_str = str(medians[b].tolist()[0])\n",
    "            lags_values_str = str(lags[b].tolist())\n",
    "            prompt_ = (\n",
    "                f\"<|start_prompt|>Dataset description: {self.description}\"\n",
    "                f\"Task description: forecast the next {str(self.pred_len)} steps given the previous {str(self.seq_len)} steps information; \"\n",
    "                \"Input statistics: \"\n",
    "                f\"min value {min_values_str}, \"\n",
    "                f\"max value {max_values_str}, \"\n",
    "                f\"median value {median_values_str}, \"\n",
    "                f\"the trend of input is {'upward' if trends[b] > 0 else 'downward'}, \"\n",
    "                f\"top 5 lags are : {lags_values_str}<|<end_prompt>|>\"\n",
    "            )\n",
    "\n",
    "            prompt.append(prompt_)\n",
    "\n",
    "        x_enc = x_enc.reshape(B, N, T).permute(0, 2, 1).contiguous() # B, T, N\n",
    "\n",
    "        prompt = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).input_ids\n",
    "        prompt_embeddings = self.llm_model.get_input_embeddings()(prompt.to(x_enc.device))  # (batch, prompt_token, dim)\n",
    "\n",
    "        source_embeddings = self.mapping_layer(self.word_embeddings.permute(1, 0)).permute(1, 0)\n",
    "\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous() # B N T\n",
    "        enc_out, n_vars = self.patch_embedding(x_enc.to(torch.bfloat16))\n",
    "        enc_out = self.reprogramming_layer(enc_out, source_embeddings, source_embeddings)\n",
    "        llama_enc_out = torch.cat([prompt_embeddings, enc_out], dim=1)\n",
    "        dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state\n",
    "        dec_out = dec_out[:, :, :self.d_ff]\n",
    "\n",
    "        dec_out = torch.reshape(\n",
    "            dec_out, (-1, n_vars, dec_out.shape[-2], dec_out.shape[-1]))\n",
    "        dec_out = dec_out.permute(0, 1, 3, 2).contiguous()\n",
    "\n",
    "        dec_out = self.output_projection(dec_out[:, :, :, -self.patch_nums:])\n",
    "        dec_out = dec_out.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        dec_out = self.normalize_layers(dec_out, 'denorm')\n",
    "\n",
    "        return dec_out\n",
    "\n",
    "    @vertexai.preview.developer.mark.train()\n",
    "    def train_model(self, train_loader, test_loader, vali_loader, path):\n",
    "        import torch.multiprocessing as mp\n",
    "        mp.set_start_method('spawn', force=True)\n",
    "        ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "        deepspeed_plugin = DeepSpeedPlugin(hf_ds_config='./ds_config_zero2.json')\n",
    "        accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
    "        print(\"Accelerator initialized:\", accelerator)\n",
    "\n",
    "        # ## Load datasets \n",
    "        # train_data, train_loader = data_provider(self.args, 'train')\n",
    "        # vali_data, vali_loader = data_provider(self.args, 'val')\n",
    "        # test_data, test_loader = data_provider(self.args, 'test')\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        self.args.content = load_content(self.args)\n",
    "        time_now = time.time()\n",
    "\n",
    "        train_steps = len(train_loader)\n",
    "        early_stopping = EarlyStopping(accelerator=accelerator, patience=self.args.patience)\n",
    "\n",
    "        model_optim = optim.Adam(self.parameters(), lr=self.args.learning_rate)\n",
    "        criterion = smape_loss()\n",
    "\n",
    "\n",
    "        if self.args.lradj == 'COS':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=20, eta_min=1e-8)\n",
    "        else:\n",
    "            scheduler = lr_scheduler.OneCycleLR(optimizer=model_optim,\n",
    "                                                steps_per_epoch=train_steps,\n",
    "                                                pct_start=self.args.pct_start,\n",
    "                                                epochs=self.args.train_epochs,\n",
    "                                                max_lr=self.args.learning_rate)\n",
    "    \n",
    "\n",
    "        train_loader, vali_loader, self, model_optim, scheduler = accelerator.prepare(train_loader,\n",
    "                vali_loader, self, model_optim, scheduler)       \n",
    "\n",
    "        for epoch in range(self.args.train_epochs):\n",
    "            # train_data, train_loader = data_provider(self.args, 'train')\n",
    "            # train_loader = accelerator.prepare(train_loader)\n",
    "            \n",
    "            iter_count = 0\n",
    "            train_loss = []\n",
    "\n",
    "            self.train()\n",
    "            epoch_time = time.time()\n",
    "\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "                iter_count += 1\n",
    "                model_optim.zero_grad()\n",
    "                batch_x = batch_x.float().to(accelerator.device)\n",
    "\n",
    "                batch_y = batch_y.float().to(accelerator.device)\n",
    "                batch_y_mark = batch_y_mark.float().to(accelerator.device)\n",
    "\n",
    "                # decoder input\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float().to(accelerator.device)\n",
    "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(\n",
    "                    accelerator.device)\n",
    "\n",
    "                outputs = self(batch_x, None, dec_inp, None)\n",
    "\n",
    "                f_dim = -1 if self.args.features == 'MS' else 0\n",
    "                outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -self.args.pred_len:, f_dim:]\n",
    "\n",
    "                batch_y_mark = batch_y_mark[:, -self.args.pred_len:, f_dim:]\n",
    "                loss = criterion(batch_x, 0, outputs, batch_y, batch_y_mark) # 0 cuz we don't need it\n",
    "\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    accelerator.print(\n",
    "                        \"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item())\n",
    "                    )\n",
    "                    speed = (time.time() - time_now) / iter_count\n",
    "                    left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n",
    "                    accelerator.print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                    iter_count = 0\n",
    "                    time_now = time.time()\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                model_optim.step()\n",
    "\n",
    "                if self.args.lradj == 'TST':\n",
    "                    adjust_learning_rate(accelerator, model_optim, scheduler, epoch + 1, self.args, printout=False)\n",
    "                    scheduler.step()\n",
    "\n",
    "            accelerator.print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "            train_loss = np.average(train_loss)\n",
    "            print('########################################################################')\n",
    "            vali_loss = test_MS(self.args, accelerator, self, train_loader, vali_loader, criterion)\n",
    "            test_loss = vali_loss\n",
    "            accelerator.print(\n",
    "                \"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                    epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "            early_stopping(vali_loss, self, path)  # model saving\n",
    "            if early_stopping.early_stop:\n",
    "                accelerator.print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "            if self.args.lradj != 'TST':\n",
    "                adjust_learning_rate(accelerator, model_optim, scheduler, epoch + 1, self.args, printout=True)\n",
    "            else:\n",
    "                accelerator.print('Updating learning rate to {}'.format(scheduler.get_last_lr()[0]))\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        print(\"Training is Over\")\n",
    "\n",
    "    \n",
    "    def calcute_lags(self, x_enc):\n",
    "        q_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, dim=-1)\n",
    "        mean_value = torch.mean(corr, dim=1)\n",
    "        _, lags = torch.topk(mean_value, self.top_k, dim=-1)\n",
    "        return lags\n",
    "\n",
    "\n",
    "\n",
    "class ReprogrammingLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_keys=None, d_llm=None, attention_dropout=0.1):\n",
    "        super(ReprogrammingLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_llm, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_llm, d_keys * n_heads)\n",
    "        self.out_projection = nn.Linear(d_keys * n_heads, d_llm)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, target_embedding, source_embedding, value_embedding):\n",
    "        B, L, _ = target_embedding.shape\n",
    "        S, _ = source_embedding.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        target_embedding = self.query_projection(target_embedding).view(B, L, H, -1)\n",
    "        source_embedding = self.key_projection(source_embedding).view(S, H, -1)\n",
    "        value_embedding = self.value_projection(value_embedding).view(S, H, -1)\n",
    "\n",
    "        out = self.reprogramming(target_embedding, source_embedding, value_embedding)\n",
    "\n",
    "        out = out.reshape(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out)\n",
    "\n",
    "    def reprogramming(self, target_embedding, source_embedding, value_embedding):\n",
    "        B, L, H, E = target_embedding.shape\n",
    "\n",
    "        scale = 1. / sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,she->bhls\", target_embedding, source_embedding)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        reprogramming_embedding = torch.einsum(\"bhls,she->blhe\", A, value_embedding)\n",
    "\n",
    "        return reprogramming_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.task_name = 'short_term_forecast'\n",
    "        self.is_training = 1\n",
    "        self.model_id = 'promo_ean_channel'\n",
    "        self.model_comment = 'EAN_Channel'\n",
    "        self.model = 'TimeLLM'\n",
    "        self.seed = 2021  # Assuming the seed is not explicitly set in the script\n",
    "        self.data = 'promo_ean_channel'\n",
    "        self.root_path = './dataset/true_promo'\n",
    "        self.data_path = 'all_product_true_promo_train.csv'\n",
    "        self.features = 'MS'\n",
    "        self.target = 'sold_units'\n",
    "        self.loader = 'modal'  # Assuming this is a default value\n",
    "        self.freq = 'h'  # Assuming this is a default value\n",
    "        self.checkpoints = './checkpoints/'  # Assuming this is a default value\n",
    "        self.seq_len = 13  # Assuming this is a default value\n",
    "        self.label_len = 1  # Assuming this is a default value\n",
    "        self.pred_len = 17\n",
    "        self.seasonal_patterns = 'Monthly'  # Assuming this is a default value\n",
    "        self.enc_in = 7\n",
    "        self.dec_in = 7\n",
    "        self.c_out = 7\n",
    "        self.d_model = 16\n",
    "        self.n_heads = 8  # Typically set by your model configuration\n",
    "        self.e_layers = 2  # Typically set by your model configuration\n",
    "        self.d_layers = 1  # Typically set by your model configuration\n",
    "        self.d_ff = 64\n",
    "        self.moving_avg = 25  # Assuming default if not specified in the script\n",
    "        self.factor = 3\n",
    "        self.dropout = 0.1  # Assuming default if not specified\n",
    "        self.embed = 'timeF'  # Assuming default if not specified\n",
    "        self.activation = 'gelu'  # Assuming default if not specified\n",
    "        self.output_attention = False  # Assuming default if not specified\n",
    "        self.patch_len = 1\n",
    "        self.stride = 8  # Assuming default if not specified\n",
    "        self.prompt_domain = 0  # Assuming default if not specified\n",
    "        self.llm_model = 'GPT2'\n",
    "        self.llm_dim = 768\n",
    "        self.num_workers = 10  # Default setting\n",
    "        self.itr = 1\n",
    "        self.train_epochs = 2\n",
    "        self.align_epochs = 10  # Assuming default if not specified\n",
    "        self.batch_size = 1\n",
    "        self.eval_batch_size = 1  # Assuming default if not specified\n",
    "        self.patience = 10  # Assuming default if not specified\n",
    "        self.learning_rate = 0.001\n",
    "        self.des = 'Exp'\n",
    "        self.loss = 'spmae'  # Assuming default if not specified\n",
    "        self.lradj = 'type1'  # Assuming default if not specified\n",
    "        self.pct_start = 0.2  # Assuming default if not specified\n",
    "        self.use_amp = False  # Assuming default based on your environment capabilities\n",
    "        self.llm_layers = 16\n",
    "        self.percent = 100  # Assuming default if not specified\n",
    "        self.zero_percent = 0\n",
    "        self.interpolation = False\n",
    "        self.interpolation_method = False\n",
    "        self.fill_discontinuity = False\n",
    "        self.month = 11\n",
    "        self.num_weeks = 100\n",
    "        self.scale=True\n",
    "        self.embedding = True\n",
    "        self.embedding_dimension = 2\n",
    "        self.keep_non_promo = False\n",
    "        self.channel = 'Offline'\n",
    "        \n",
    "\n",
    "# Instantiate the Args\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for number of weeks to take\n",
      "number of products before preprocessing 1510\n",
      "How many ean_global_channel_type: 1510\n",
      "final data product (if changed we remove discontinuity) 1429\n",
      "prediction length: 17\n",
      "Ended up with  68\n",
      "Let's Load the Data\n",
      "number of products before preprocessing 1510\n",
      "How many ean_global_channel_type: 1332\n",
      "final data product (if changed we remove discontinuity) 1314\n",
      "prediction length: 17\n",
      "Train set saved to: dataset/true_promo/OfflineChannel_Month11_68Weeks_scaled_embedding_2/short_term_forecast_promo_ean_channel_TimeLLM_promo_ean_channel_ftMS_sl13_ll17_pl17_dm16_nh8_el2_dl1_df64_fc3_ebtimeF_Exp/train.csv\n",
      "Test set saved to: dataset/true_promo/OfflineChannel_Month11_68Weeks_scaled_embedding_2/short_term_forecast_promo_ean_channel_TimeLLM_promo_ean_channel_ftMS_sl13_ll17_pl17_dm16_nh8_el2_dl1_df64_fc3_ebtimeF_Exp/test.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "PROJECT_ID = \"itg-bpma-gbl-ww-np\"  # @param {type:\"string\"}\n",
    "REGION = \"europe-west1\" \n",
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}\n",
    "import vertexai\n",
    "REMOTE_JOB_NAME = \"timeseriesllm\"\n",
    "REMOTE_JOB_BUCKET = f\"{BUCKET_URI}/{REMOTE_JOB_NAME}\"\n",
    "##################################################################################################\n",
    "vertexai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=REMOTE_JOB_BUCKET,\n",
    ")\n",
    "\n",
    "##################################################################################################\n",
    "bq_client = bigquery.Client(\n",
    "    project=PROJECT_ID,  # GCP project used for running the queries and billing\n",
    ")\n",
    "\n",
    "#################################################################################################\n",
    "print(\"Looking for number of weeks to take\")\n",
    "_,_,_,pred_len = import_true_promo(\n",
    "        client=bq_client,\n",
    "        zero_percent=0,\n",
    "        month=args.month,\n",
    "        num_weeks=0,\n",
    "        channel=args.channel,\n",
    "        fill_discontinuity=args.fill_discontinuity,\n",
    "        keep_non_promo=args.keep_non_promo\n",
    "    )\n",
    "print(\"Ended up with \", 4*pred_len)\n",
    "args.num_weeks=4*pred_len\n",
    "args.pred_len = pred_len\n",
    "args.label_len = pred_len\n",
    "print(\"Let's Load the Data\")\n",
    "if args.interpolation:\n",
    "    final_data, train_set, test_set, pred_len = import_all(\n",
    "        client=bq_client,\n",
    "        zero_percent=args.zero_percent,\n",
    "        month=args.month,\n",
    "        num_weeks=args.num_weeks,\n",
    "        channel=args.channel,\n",
    "        fill_discontinuity=args.fill_discontinuity,\n",
    "        keep_non_promo=args.keep_non_promo,\n",
    "        interpolation_method=args.interpolation_method\n",
    "    )\n",
    "else :\n",
    "    final_data, train_set, test_set, pred_len = import_true_promo(\n",
    "        client=bq_client,\n",
    "        zero_percent=args.zero_percent,\n",
    "        month=args.month,\n",
    "        num_weeks=args.num_weeks,\n",
    "        channel=args.channel,\n",
    "        fill_discontinuity=args.fill_discontinuity,\n",
    "        keep_non_promo=args.keep_non_promo\n",
    "    )\n",
    "\n",
    "\n",
    "################## \n",
    "setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.des)\n",
    "# Construct the path\n",
    "base_dir = f\"dataset/\"\n",
    "if args.interpolation:\n",
    "    base_dir += f\"interpolation_{args.interpolation_method}/\"\n",
    "else : \n",
    "    base_dir += f\"true_promo/\"\n",
    "\n",
    "base_dir+= f\"{args.channel}Channel_Month{args.month}_{args.num_weeks}Weeks\"\n",
    "if args.fill_discontinuity:\n",
    "    base_dir += \"_filldiscont\"\n",
    "if args.keep_non_promo:\n",
    "    base_dir += \"_keepnonpromo\"\n",
    "if args.scale:\n",
    "    base_dir+=\"_scaled\"\n",
    "if args.embedding:\n",
    "    base_dir+=f\"_embedding_{args.embedding_dimension}\"\n",
    "base_dir += '/'+setting\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(base_dir, \"train.csv\")\n",
    "test_path = os.path.join(base_dir, \"test.csv\")\n",
    "\n",
    "\n",
    "train_set.to_csv(train_path, index=False)\n",
    "test_set.to_csv(test_path, index=False)\n",
    "\n",
    "print(f\"Train set saved to: {train_path}\")\n",
    "print(f\"Test set saved to: {test_path}\")\n",
    "if args.scale:\n",
    "     \n",
    "    args.scale_path = 'scale_path/' + base_dir[8:]\n",
    "    if check_saved_standardization_data(args.scale_path):\n",
    "        delete_saved_standardization_data(args.scale_path)\n",
    "\n",
    "\n",
    "########################################################### configuration ####################\n",
    "args.pred_len = pred_len\n",
    "args.label_len = args.pred_len\n",
    "args.seq_len = int(2*args.pred_len)\n",
    "args.root_path = base_dir\n",
    "args.data_path = 'train.csv'\n",
    "##############################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scaling the data of train\n",
      "standarization is over of train\n",
      "scaling the data of test\n",
      "standarization is over of test\n",
      "Remote job created. View the job: https://console.cloud.google.com/ai/platform/locations/europe-west1/training/6060668208682106880?project=238069609727\n",
      "Start remote execution on Vertex...\n",
      "gcs_path /gcs/your-bucket-name-itg-bpma-gbl-ww-np-unique/timeseriesllm/remote-job-2024-06-24-13-50-00-dd11e/input/test_loader not found in the metadata. Make sure global serialization metadata is loaded.\n",
      "W0624 13:54:49.946648 140527512155968 any_serializer.py:471] gcs_path /gcs/your-bucket-name-itg-bpma-gbl-ww-np-unique/timeseriesllm/remote-job-2024-06-24-13-50-00-dd11e/input/test_loader not found in the metadata. Make sure global serialization metadata is loaded.\n",
      "W0624 13:54:50.469594 140527512155968 any_serializer.py:533] the gcs_path /gcs/your-bucket-name-itg-bpma-gbl-ww-np-unique/timeseriesllm/remote-job-2024-06-24-13-50-00-dd11e/input/test_loader doesn't exist in the metadata. Please make sure the global metadata is loaded.\n",
      "the gcs_path /gcs/your-bucket-name-itg-bpma-gbl-ww-np-unique/timeseriesllm/remote-job-2024-06-24-13-50-00-dd11e/input/test_loader doesn't exist in the metadata. Please make sure the global metadata is loaded.\n",
      "Accelerator initialized: <accelerate.accelerator.Accelerator object at 0x7fcded7c4190>\n",
      "[2024-06-24 13:54:51,231] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "I0624 13:54:51.728315 140527512155968 logging.py:61] Since you passed both train and evaluation dataloader, `is_train_batch_min` (here True will decide the `train_batch_size` (1).\n",
      "[2024-06-24 13:54:51,728] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-06-24 13:54:51,728] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-06-24 13:54:51,728] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-06-24 13:54:51,852] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=10.132.15.199, master_port=29500\n",
      "[2024-06-24 13:54:51,852] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n",
      "[2024-06-24 13:54:51,956] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-06-24 13:54:51,957] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-06-24 13:54:51,957] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-06-24 13:54:51,966] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-06-24 13:54:51,967] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-06-24 13:54:51,967] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-06-24 13:54:51,967] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-06-24 13:54:51,967] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-06-24 13:54:51,967] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-06-24 13:54:51,967] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-06-24 13:54:52,343] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-06-24 13:54:52,344] [INFO] [utils.py:801:see_memory_usage] MA 0.6 GB         Max_MA 0.7 GB         CA 0.89 GB         Max_CA 1 GB \n",
      "[2024-06-24 13:54:52,344] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.67 GB, percent = 6.2%\n",
      "[2024-06-24 13:54:52,566] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-06-24 13:54:52,567] [INFO] [utils.py:801:see_memory_usage] MA 0.6 GB         Max_MA 0.79 GB         CA 1.08 GB         Max_CA 1 GB \n",
      "[2024-06-24 13:54:52,567] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.67 GB, percent = 6.2%\n",
      "[2024-06-24 13:54:52,567] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-06-24 13:54:52,791] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-06-24 13:54:52,791] [INFO] [utils.py:801:see_memory_usage] MA 0.6 GB         Max_MA 0.6 GB         CA 1.08 GB         Max_CA 1 GB \n",
      "[2024-06-24 13:54:52,792] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 3.67 GB, percent = 6.2%\n",
      "[2024-06-24 13:54:52,793] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-06-24 13:54:52,793] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-06-24 13:54:52,793] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-06-24 13:54:52,793] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-06-24 13:54:52,794] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-06-24 13:54:52,794] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-06-24 13:54:52,794] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-06-24 13:54:52,794] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-06-24 13:54:52,795] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-06-24 13:54:52,795] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-06-24 13:54:52,795] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-06-24 13:54:52,795] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-06-24 13:54:52,795] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-06-24 13:54:52,795] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-06-24 13:54:52,795] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-06-24 13:54:52,795] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7fcdec275b10>\n",
      "[2024-06-24 13:54:52,795] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-06-24 13:54:52,795] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-06-24 13:54:52,795] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-06-24 13:54:52,795] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-06-24 13:54:52,795] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-06-24 13:54:52,795] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-06-24 13:54:52,795] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-06-24 13:54:52,795] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-06-24 13:54:52,796] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-06-24 13:54:52,797] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-06-24 13:54:52,798] [INFO] [config.py:1000:print]   train_batch_size ............. 1\n",
      "[2024-06-24 13:54:52,798] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1\n",
      "[2024-06-24 13:54:52,798] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-06-24 13:54:52,798] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-06-24 13:54:52,798] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-06-24 13:54:52,798] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-06-24 13:54:52,798] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-06-24 13:54:52,798] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-06-24 13:54:52,798] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-06-24 13:54:52,798] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-06-24 13:54:52,798] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-06-24 13:54:52,798] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-06-24 13:54:52,798] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 1, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "}\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "\titers: 100, epoch: 1 | loss: 190.4846497\n",
      "\tspeed: 0.6740s/iter; left time: 1704.5076s\n",
      "\titers: 200, epoch: 1 | loss: 170.5009460\n",
      "\tspeed: 0.2189s/iter; left time: 531.7173s\n",
      "\titers: 300, epoch: 1 | loss: 126.0404739\n",
      "\tspeed: 0.2193s/iter; left time: 510.8430s\n",
      "\titers: 400, epoch: 1 | loss: 182.4406738\n",
      "\tspeed: 0.2191s/iter; left time: 488.4714s\n",
      "\titers: 500, epoch: 1 | loss: 150.0965881\n",
      "\tspeed: 0.2186s/iter; left time: 465.3287s\n",
      "\titers: 600, epoch: 1 | loss: 163.7270355\n",
      "\tspeed: 0.2189s/iter; left time: 444.2351s\n",
      "\titers: 700, epoch: 1 | loss: 159.4734497\n",
      "\tspeed: 0.2183s/iter; left time: 421.0076s\n",
      "\titers: 800, epoch: 1 | loss: 142.8028564\n",
      "\tspeed: 0.2185s/iter; left time: 399.5604s\n",
      "\titers: 900, epoch: 1 | loss: 112.0997543\n",
      "\tspeed: 0.2185s/iter; left time: 377.7728s\n",
      "\titers: 1000, epoch: 1 | loss: 140.9675598\n",
      "\tspeed: 0.2182s/iter; left time: 355.4018s\n",
      "\titers: 1100, epoch: 1 | loss: 145.1387787\n",
      "\tspeed: 0.2215s/iter; left time: 338.6898s\n",
      "\titers: 1200, epoch: 1 | loss: 131.4372406\n",
      "\tspeed: 0.2223s/iter; left time: 317.6999s\n",
      "\titers: 1300, epoch: 1 | loss: 155.2454376\n",
      "\tspeed: 0.2218s/iter; left time: 294.8271s\n",
      "Epoch: 1 cost time: 333.7710530757904\n",
      "########################################################################\n",
      "Shape of X eval torch.Size([1314, 34, 5])\n",
      "Shape of output eval before choosing torch.Size([1314, 17, 5])\n",
      "Shape of y eval torch.Size([1314, 17, 1])\n",
      "Epoch: 1, Steps: 1314 | Train Loss: 155.4574863 Vali Loss: 132.8299108 Test Loss: 132.8299108\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Updating learning rate to 0.001\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "Start remote execution on Vertex...\n",
      "\titers: 100, epoch: 2 | loss: 164.3088531\n",
      "\tspeed: 1.9587s/iter; left time: 2379.7671s\n",
      "\titers: 200, epoch: 2 | loss: 99.4895020\n",
      "\tspeed: 0.2202s/iter; left time: 245.5082s\n",
      "\titers: 300, epoch: 2 | loss: 132.2452850\n",
      "\tspeed: 0.2199s/iter; left time: 223.1539s\n",
      "\titers: 400, epoch: 2 | loss: 148.2632751\n",
      "\tspeed: 0.2198s/iter; left time: 201.1163s\n",
      "\titers: 500, epoch: 2 | loss: 154.1036682\n",
      "\tspeed: 0.2196s/iter; left time: 179.0126s\n",
      "\titers: 600, epoch: 2 | loss: 135.8519897\n",
      "\tspeed: 0.2200s/iter; left time: 157.3154s\n",
      "\titers: 700, epoch: 2 | loss: 153.4587097\n",
      "\tspeed: 0.2192s/iter; left time: 134.8266s\n",
      "\titers: 800, epoch: 2 | loss: 119.0013275\n",
      "\tspeed: 0.2195s/iter; left time: 113.0441s\n",
      "\titers: 900, epoch: 2 | loss: 166.6839294\n",
      "\tspeed: 0.2195s/iter; left time: 91.0817s\n",
      "\titers: 1000, epoch: 2 | loss: 150.6261292\n",
      "\tspeed: 0.2193s/iter; left time: 69.0926s\n",
      "\titers: 1100, epoch: 2 | loss: 124.4462738\n",
      "\tspeed: 0.2195s/iter; left time: 47.1950s\n",
      "\titers: 1200, epoch: 2 | loss: 154.5228882\n",
      "\tspeed: 0.2199s/iter; left time: 25.2869s\n",
      "\titers: 1300, epoch: 2 | loss: 106.9215393\n",
      "\tspeed: 0.2198s/iter; left time: 3.2976s\n",
      "Epoch: 2 cost time: 333.3892455101013\n",
      "########################################################################\n",
      "Shape of X eval torch.Size([1314, 34, 5])\n",
      "Shape of output eval before choosing torch.Size([1314, 17, 5])\n",
      "Shape of y eval torch.Size([1314, 17, 1])\n",
      "Epoch: 2, Steps: 1314 | Train Loss: 144.3652778 Vali Loss: 123.4421280 Test Loss: 123.4421280\n",
      "Updating learning rate to 0.0005\n",
      "Training is Over\n",
      "Remote execution is completed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.des, ii)\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    " \n",
    "    model = Model(args).float()\n",
    "\n",
    "    path = os.path.join(args.checkpoints,\n",
    "                        base_dir[8:] + '_' + str(ii) + '-' + args.model_comment)  # unique checkpoint saving path\n",
    "    args.content = load_content(args)\n",
    "    if not os.path.exists(path) :# and accelerator.is_local_main_process:\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "    train_data, train_loader = data_provider(args, 'train')\n",
    "    test_data, test_loader = data_provider(args, 'test')\n",
    "\n",
    "    vertexai.preview.init(remote=True)\n",
    "    #model.train_model.vertex.remote_config.machine_type = \"n1-highmen-4\"\n",
    "    model.train_model.vertex.remote_config.container_uri = \"europe-west1-docker.pkg.dev/itg-bpma-gbl-ww-np/timeseriesforecasting/torch-train:latest\"\n",
    "    model.train_model.vertex.remote_config.enable_cuda = True\n",
    "    model.train_model.vertex.remote_config.accelerator_count = 4\n",
    "    model.train_model(train_loader, test_loader, test_loader,path)\n",
    "    #torch.save(model.state_dict(), path + '/' + 'checkpoint_v_test1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: patch_embedding.value_embedding.tokenConv.weight | Size: torch.Size([32, 1, 3]) | Values : tensor([[[-0.8107,  0.2070, -0.7467]]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: mapping_layer.weight | Size: torch.Size([1000, 50257]) | Values : tensor([[-0.0028,  0.0010,  0.0008,  ...,  0.0027,  0.0015,  0.0031]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: mapping_layer.bias | Size: torch.Size([1000]) | Values : tensor([0.0002], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: reprogramming_layer.query_projection.weight | Size: torch.Size([1024, 32]) | Values : tensor([[ 0.1551,  0.1534, -0.0059, -0.0869,  0.0856,  0.1022, -0.0441, -0.0434,\n",
      "          0.0210, -0.1227,  0.0563,  0.0326, -0.0681, -0.0284,  0.0284, -0.0439,\n",
      "         -0.1313, -0.0647,  0.1476,  0.0233, -0.0952,  0.0208,  0.0993,  0.0400,\n",
      "          0.0304, -0.1327,  0.1055,  0.0106, -0.1602,  0.0445, -0.1622, -0.0406]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: reprogramming_layer.query_projection.bias | Size: torch.Size([1024]) | Values : tensor([-0.0678], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: reprogramming_layer.key_projection.weight | Size: torch.Size([1024, 768]) | Values : tensor([[-3.1961e-02, -2.6556e-02,  4.0471e-03,  8.3416e-03,  2.5873e-03,\n",
      "          2.2407e-02,  2.0968e-02,  1.8605e-02,  5.1723e-03,  1.6846e-02,\n",
      "          1.7547e-02,  3.3131e-02,  1.2910e-02,  1.8623e-02, -2.2617e-02,\n",
      "         -1.3647e-02,  3.4439e-02, -2.2580e-02, -1.4597e-02,  2.6449e-02,\n",
      "         -2.6812e-02,  3.1787e-03,  3.2060e-02, -2.6943e-02,  2.5656e-02,\n",
      "          9.4859e-03,  1.0976e-03, -5.8601e-03,  1.5078e-02,  3.0881e-02,\n",
      "         -1.9979e-02, -2.9222e-02, -2.0847e-02,  6.8370e-03, -1.5793e-02,\n",
      "          1.4127e-02,  2.5666e-02,  7.5134e-03,  5.6719e-03,  3.6072e-03,\n",
      "          2.2118e-02, -4.9078e-03, -2.3043e-02, -1.6731e-02, -1.0382e-02,\n",
      "         -2.5710e-02,  1.0566e-02, -2.8646e-03,  2.2400e-02,  1.2162e-02,\n",
      "         -1.9486e-02,  1.3971e-03, -5.3128e-03,  1.2665e-02, -3.1452e-02,\n",
      "          2.5951e-02, -3.1806e-02,  3.5246e-02, -1.7662e-02, -2.5748e-02,\n",
      "         -3.1840e-03, -3.4446e-02,  4.6570e-03, -1.6511e-02, -1.8238e-02,\n",
      "         -1.5469e-02, -1.9466e-03,  1.7520e-02, -2.6256e-02,  2.2899e-02,\n",
      "         -1.2233e-02, -2.1300e-02, -5.2323e-03, -1.3318e-02, -1.7055e-02,\n",
      "         -3.5636e-02, -2.3580e-02, -1.2234e-02,  1.5867e-02, -2.7754e-02,\n",
      "         -3.3004e-02,  3.0625e-02,  1.8877e-02, -1.7492e-02, -3.2783e-02,\n",
      "         -1.0989e-02, -5.0419e-03,  1.3431e-02,  2.1608e-02,  1.5176e-03,\n",
      "         -3.7377e-03,  1.0250e-02,  3.0840e-02,  3.2277e-02, -2.3128e-02,\n",
      "         -3.2180e-03, -9.9535e-03, -1.7415e-02, -2.8325e-02,  3.0016e-02,\n",
      "         -1.3061e-02,  3.2156e-02,  2.1894e-02,  1.3350e-02,  6.9373e-03,\n",
      "          5.8194e-03, -1.6866e-02, -2.8486e-02,  3.5159e-02,  1.8979e-02,\n",
      "          1.9023e-02,  2.6154e-02,  1.5098e-02,  1.4519e-03, -2.0849e-02,\n",
      "         -1.6199e-02,  2.6971e-02,  2.5677e-02,  2.8890e-02,  2.7296e-02,\n",
      "          1.7119e-02, -1.4592e-02, -5.2842e-03,  3.3074e-02, -2.3173e-02,\n",
      "         -8.2647e-04, -3.4761e-02,  3.0563e-02,  3.5786e-02, -3.1147e-02,\n",
      "         -2.1123e-02,  2.2855e-02,  1.2126e-02, -1.3143e-02, -2.4118e-02,\n",
      "          2.7273e-02,  6.0037e-03, -2.5443e-02, -2.8586e-02,  1.4109e-02,\n",
      "         -1.2030e-02, -1.5915e-02, -2.0212e-02, -4.8365e-03,  6.7907e-03,\n",
      "          1.2348e-02, -4.5236e-03, -2.0942e-02,  2.5632e-02,  1.7925e-02,\n",
      "          1.8558e-02,  1.5178e-02,  3.6380e-03,  2.0048e-02, -1.1408e-02,\n",
      "          1.7018e-02, -6.3636e-04, -2.6152e-02,  1.6163e-02,  1.8514e-02,\n",
      "          2.2668e-02, -1.1753e-02,  9.8895e-03, -2.5479e-02,  1.3070e-02,\n",
      "         -1.5043e-03,  6.4223e-03, -2.5540e-02, -7.4379e-03, -2.3720e-02,\n",
      "          1.2758e-02, -2.3695e-02,  5.7781e-03,  1.9487e-02, -1.9199e-02,\n",
      "          3.0249e-04,  2.3715e-02, -2.6264e-02, -1.5291e-02, -3.5959e-02,\n",
      "          2.1363e-02, -1.9248e-02, -1.7521e-02,  3.5535e-02,  8.5748e-03,\n",
      "         -3.4191e-03,  3.1156e-02,  3.5926e-02, -1.2481e-02,  1.2789e-03,\n",
      "          1.5838e-02, -2.5158e-02, -2.8652e-02,  2.6794e-02, -3.2265e-04,\n",
      "         -4.9290e-03,  4.8120e-03, -5.5915e-03,  1.0323e-02, -2.4227e-02,\n",
      "         -1.4323e-02,  1.3194e-02, -1.7260e-02,  2.5864e-02, -8.2310e-03,\n",
      "         -2.5210e-04,  1.1301e-02,  1.9008e-02, -2.9031e-02,  2.3557e-03,\n",
      "          3.3828e-04,  3.4554e-02,  3.7440e-03, -1.2663e-02, -1.4076e-02,\n",
      "         -5.4352e-04,  2.2819e-02,  1.6708e-02,  9.0365e-03, -2.7492e-02,\n",
      "          4.0342e-03, -6.9758e-03,  2.0455e-02,  2.8158e-02,  3.2873e-02,\n",
      "          1.9144e-02, -3.0557e-02,  1.8674e-02, -1.2451e-02,  8.7409e-03,\n",
      "         -2.6429e-02,  1.2535e-02,  2.4323e-02,  1.2805e-02,  3.1818e-02,\n",
      "          3.0139e-02, -8.5387e-03,  3.4591e-02, -9.1237e-03,  6.2443e-03,\n",
      "         -1.6727e-02,  3.4947e-03,  3.0812e-02,  3.0027e-02, -1.6086e-02,\n",
      "          3.1608e-02, -1.0295e-02, -1.0275e-02,  4.6758e-03, -1.8756e-02,\n",
      "          1.7905e-02,  3.2558e-02,  4.0592e-03,  2.3838e-02, -2.3303e-02,\n",
      "          3.0392e-02,  2.6355e-02, -3.2709e-02, -1.8247e-03, -1.3940e-03,\n",
      "          3.2693e-02,  2.8492e-02, -2.2879e-02, -1.4007e-02, -3.0694e-02,\n",
      "         -2.3746e-02,  1.6273e-02, -1.9350e-02, -1.8558e-02,  1.9206e-02,\n",
      "          2.5712e-02,  1.1428e-02,  2.4792e-02,  6.3689e-04, -3.2658e-02,\n",
      "          3.3652e-02, -2.6899e-03, -3.7918e-03, -2.0021e-02, -1.0552e-02,\n",
      "         -1.9030e-02,  2.7034e-02, -2.8218e-02,  1.4747e-02,  1.2471e-02,\n",
      "          1.2148e-02, -2.8351e-02, -2.9581e-02, -1.2067e-02, -1.0868e-02,\n",
      "          1.1019e-02, -9.1660e-03,  6.7722e-03, -1.5197e-02, -4.8207e-03,\n",
      "          1.4660e-02,  1.6716e-02, -1.3318e-02, -1.7118e-02, -1.1777e-02,\n",
      "         -1.1635e-03, -3.5332e-02, -3.0091e-02, -1.5441e-02, -1.8028e-02,\n",
      "          9.7299e-03, -9.1179e-03, -1.5078e-03, -1.9949e-04,  3.5318e-02,\n",
      "         -1.9798e-02, -3.4647e-02, -9.5125e-03, -1.6210e-02,  3.3812e-02,\n",
      "          2.1996e-02, -1.2387e-03, -1.5672e-02, -2.4603e-02,  1.5981e-03,\n",
      "          3.2583e-02, -2.1319e-02,  3.5449e-02, -1.6998e-02,  1.7350e-02,\n",
      "         -2.4013e-02,  2.1028e-02, -1.0713e-03,  3.5297e-02, -2.2680e-04,\n",
      "         -3.2552e-02,  2.5626e-02, -1.0510e-02, -1.8595e-02,  3.3541e-02,\n",
      "          3.5354e-02, -9.6845e-03, -2.9238e-03,  1.3748e-02,  2.2137e-02,\n",
      "          1.9691e-02, -1.0144e-03, -3.5325e-02, -3.5058e-02,  2.1085e-02,\n",
      "         -3.5902e-02,  1.9787e-02, -2.2263e-02,  5.3305e-03,  1.5317e-02,\n",
      "          1.6235e-02,  2.7574e-02,  1.1795e-02,  1.2978e-02, -4.7365e-03,\n",
      "         -2.4859e-02, -2.1532e-02,  1.7693e-02, -2.3238e-02,  1.4611e-02,\n",
      "         -2.6611e-02,  2.5176e-02, -1.2859e-03,  2.5211e-02,  1.4889e-02,\n",
      "          7.8789e-04, -2.9496e-02, -2.6276e-03,  1.7846e-02, -1.4011e-02,\n",
      "         -2.8268e-02, -2.8917e-02, -8.4098e-03, -2.0585e-02,  2.0296e-02,\n",
      "          2.5969e-02,  5.4627e-03, -1.9543e-02,  3.4459e-02,  3.2605e-02,\n",
      "         -1.1495e-02,  3.3814e-02, -1.0305e-02, -2.4850e-02, -8.0711e-05,\n",
      "         -1.5225e-02,  2.7902e-02, -1.8753e-02, -2.8989e-02,  2.4444e-03,\n",
      "          1.0902e-02, -3.5330e-02,  1.1816e-02,  3.3826e-02, -2.4264e-02,\n",
      "         -2.9099e-02, -1.5513e-02,  2.9063e-02,  2.4697e-02, -8.8594e-03,\n",
      "         -2.5601e-02, -2.2873e-02,  1.1547e-02,  3.2991e-03,  1.0536e-02,\n",
      "         -3.2386e-02,  2.7965e-02,  2.9803e-02, -1.9289e-03,  2.9496e-02,\n",
      "          3.3354e-02, -1.5898e-02, -2.7069e-02,  8.7291e-03,  2.7433e-02,\n",
      "         -3.5731e-02,  1.7111e-02,  2.9529e-02,  2.1759e-02, -3.0973e-03,\n",
      "          2.8732e-02,  2.4240e-02,  2.3335e-02, -2.7345e-02, -3.1689e-03,\n",
      "          2.8591e-02,  2.8996e-02,  2.4696e-02, -5.4905e-03,  1.2891e-02,\n",
      "          1.0185e-02, -1.5930e-02,  1.8656e-02,  3.0282e-02, -1.4169e-02,\n",
      "         -1.4136e-02, -3.3287e-02, -2.2215e-02,  9.9947e-03,  3.6971e-04,\n",
      "          2.6008e-02,  4.4453e-05,  1.8308e-02, -2.0036e-02, -1.1149e-02,\n",
      "          1.8269e-02, -3.5803e-02,  3.5842e-02,  3.4113e-02, -1.0011e-02,\n",
      "          1.8861e-02,  1.9757e-02, -3.0910e-02, -2.3090e-02, -1.6467e-02,\n",
      "          2.5003e-02, -2.8665e-02, -2.3847e-02, -3.3085e-02,  3.4656e-02,\n",
      "         -1.5337e-02,  3.3435e-02, -1.2763e-02, -2.6287e-02, -1.6070e-02,\n",
      "          5.7318e-03,  2.5060e-02, -3.1889e-02, -2.6095e-02, -9.5425e-03,\n",
      "         -2.8091e-02, -1.3293e-04,  3.2249e-02, -2.2424e-02,  2.0295e-02,\n",
      "         -6.5522e-03,  9.2647e-04, -2.0853e-02, -4.9805e-03,  2.3883e-02,\n",
      "         -7.1779e-03,  5.1191e-03, -3.3632e-02, -8.8531e-03,  1.8591e-02,\n",
      "          2.9311e-03,  2.7094e-02,  3.0038e-02,  4.6039e-03,  2.5848e-02,\n",
      "         -3.1622e-02, -3.4990e-02, -3.3245e-02,  2.2703e-02,  1.3866e-02,\n",
      "          1.0895e-02, -1.3087e-02, -3.1056e-02, -1.6402e-02, -3.8889e-03,\n",
      "         -6.5285e-03,  3.5138e-02,  1.7071e-02,  2.3798e-02, -1.0356e-02,\n",
      "          8.9469e-04, -3.3044e-02, -8.6059e-03,  3.2196e-02, -3.4411e-02,\n",
      "         -2.4830e-02, -1.2452e-02, -1.2226e-02,  1.4948e-02, -2.9655e-02,\n",
      "         -1.8653e-02,  1.0394e-02,  2.4744e-02,  6.5965e-03,  1.5720e-02,\n",
      "          1.4520e-04,  5.0526e-03,  2.1523e-02,  2.6725e-02,  4.3135e-03,\n",
      "          1.1897e-02, -1.3405e-02, -1.8840e-02, -3.2696e-02,  2.7164e-02,\n",
      "         -2.4050e-02, -1.2177e-02,  7.4497e-03, -9.3458e-03,  1.4577e-02,\n",
      "          1.9809e-02, -2.0107e-02, -2.0189e-02, -1.5157e-02,  2.7723e-02,\n",
      "         -1.4772e-02, -3.2392e-02,  2.0609e-02, -1.3080e-03,  2.7339e-02,\n",
      "         -3.5058e-02,  7.3266e-04, -3.5132e-02,  2.5447e-02,  2.7631e-02,\n",
      "          1.5584e-02,  2.6021e-02,  1.9929e-02,  2.6739e-02,  3.1496e-02,\n",
      "         -2.0456e-02, -2.6570e-02, -1.3070e-02, -1.9005e-02, -7.0755e-04,\n",
      "          2.9486e-02,  2.8919e-02, -3.4442e-02,  1.4152e-02, -2.2743e-02,\n",
      "         -1.7250e-02,  2.0976e-02,  1.2425e-02,  1.6282e-02,  2.5272e-02,\n",
      "         -2.7082e-02,  2.5071e-02,  2.8167e-02, -2.3835e-02,  2.1095e-02,\n",
      "         -2.8448e-02,  2.8970e-02,  1.6459e-02, -3.3904e-02,  5.6111e-03,\n",
      "         -1.3968e-02, -1.6668e-02,  4.2180e-03,  3.0887e-03,  8.3074e-03,\n",
      "         -2.6076e-02,  1.7380e-02, -3.2860e-02,  3.0322e-02, -3.2629e-02,\n",
      "         -1.9571e-02, -2.2055e-02,  2.2245e-02, -1.6513e-02,  1.0508e-03,\n",
      "          3.5208e-02,  1.1010e-02,  3.3700e-02,  2.9089e-02, -2.6569e-02,\n",
      "         -3.3601e-02, -2.2524e-02,  8.5260e-03, -3.4054e-02, -2.7850e-02,\n",
      "         -1.5439e-02, -9.8328e-03, -1.8587e-02, -1.5136e-02,  2.3710e-02,\n",
      "         -1.8366e-02,  1.4940e-02,  3.2763e-02,  1.8994e-02,  2.6468e-02,\n",
      "         -2.8494e-02, -2.8263e-02, -1.8056e-02,  2.9889e-02, -1.5543e-02,\n",
      "         -1.8043e-02, -2.5574e-02, -8.8274e-03,  2.5662e-02, -2.2076e-02,\n",
      "          5.5367e-03,  1.5903e-02, -1.4974e-03, -1.2157e-02,  4.0364e-03,\n",
      "         -9.7692e-03, -3.7907e-03,  5.9817e-03, -3.4357e-02, -1.2719e-02,\n",
      "         -1.2302e-02, -1.5753e-02, -3.5286e-02, -3.2854e-02,  2.9567e-02,\n",
      "          1.9625e-02,  2.1162e-03,  1.5298e-03,  1.2074e-02,  5.6800e-03,\n",
      "         -2.9181e-02,  3.0900e-02, -7.2456e-04,  3.1169e-02,  3.2618e-02,\n",
      "          3.9844e-03,  1.1354e-02, -2.9110e-02, -3.4831e-02, -2.2898e-02,\n",
      "         -3.4526e-02, -6.8521e-03, -8.1811e-03, -2.7912e-02,  2.4136e-03,\n",
      "          7.0522e-03,  1.7157e-02, -1.8088e-02,  8.7813e-05, -1.0309e-02,\n",
      "         -3.5769e-03,  2.0326e-02, -1.1416e-02, -2.7381e-02,  9.4705e-03,\n",
      "         -3.2495e-02, -3.3738e-03,  1.0795e-02,  1.8028e-02, -1.9649e-02,\n",
      "         -3.4466e-02, -1.7239e-04,  1.0749e-02,  1.6135e-02, -1.1948e-02,\n",
      "         -2.6517e-02,  2.5318e-02, -1.1038e-02,  3.0615e-02, -3.1077e-02,\n",
      "         -1.4654e-02,  2.4783e-02, -2.2074e-03,  1.7656e-02,  1.8349e-03,\n",
      "          2.9647e-04,  1.6650e-02, -3.5420e-02,  1.4073e-02,  2.5195e-02,\n",
      "          8.3300e-03, -1.4707e-02,  2.6017e-02,  1.0006e-03,  3.1709e-02,\n",
      "          3.4846e-02, -3.3838e-02, -1.5974e-02,  2.9438e-02,  3.2664e-02,\n",
      "         -1.4407e-02, -2.5689e-02,  3.1695e-02, -1.7749e-02, -3.4829e-02,\n",
      "         -8.6591e-03,  2.5805e-02,  9.7756e-03,  4.5014e-03, -6.9523e-03,\n",
      "         -3.3028e-02,  1.0042e-02,  3.2915e-02,  2.2295e-02, -1.2425e-02,\n",
      "         -1.7815e-02, -1.1132e-02, -1.0480e-02, -2.3591e-02, -3.5801e-02,\n",
      "         -1.2200e-02,  2.4276e-02, -2.1097e-02,  2.5516e-02,  2.9373e-02,\n",
      "          2.1819e-02, -1.7441e-02, -1.0832e-02,  3.0883e-02, -3.2024e-02,\n",
      "          2.5177e-02,  2.5944e-02,  1.0200e-02,  1.9825e-02, -4.7022e-03,\n",
      "         -8.7784e-03,  5.6864e-03,  1.9665e-02, -5.1084e-03,  2.1688e-02,\n",
      "         -2.9716e-02,  1.0413e-03, -9.3109e-03,  3.8022e-03, -9.3957e-03,\n",
      "         -3.0323e-02, -2.1473e-02,  2.8126e-02,  1.5987e-02,  1.2882e-02,\n",
      "         -1.9801e-02, -3.0029e-02, -1.4272e-02, -2.4498e-02,  2.8392e-02,\n",
      "          2.1049e-02, -5.4470e-03, -3.5274e-02, -2.9829e-02,  2.6534e-02,\n",
      "          2.5225e-02, -1.7844e-02,  3.3122e-03]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: reprogramming_layer.key_projection.bias | Size: torch.Size([1024]) | Values : tensor([0.0105], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: reprogramming_layer.value_projection.weight | Size: torch.Size([1024, 768]) | Values : tensor([[-3.5046e-02,  7.5283e-03, -3.4976e-02, -3.4674e-02, -6.4295e-03,\n",
      "          1.0744e-02,  6.7883e-03, -2.0720e-02,  1.1125e-02, -2.5307e-02,\n",
      "         -1.2699e-02,  3.3463e-02,  3.2170e-02, -2.4903e-02, -8.1476e-04,\n",
      "          2.7577e-02, -1.8469e-02,  2.3841e-02,  9.4935e-03,  3.0998e-02,\n",
      "         -3.0075e-02, -2.2490e-02, -3.4895e-03,  2.7048e-02, -1.4230e-02,\n",
      "          6.0921e-03,  1.6178e-02, -5.6598e-03, -2.7778e-02,  3.1628e-02,\n",
      "          1.0730e-02,  7.1301e-03,  2.2025e-02,  2.9130e-02, -9.0243e-03,\n",
      "         -1.8394e-02, -1.1167e-02,  1.2621e-02,  7.7092e-03,  9.6636e-03,\n",
      "          7.4675e-03, -2.1875e-02,  3.5842e-03,  2.8732e-02,  3.2559e-02,\n",
      "          7.3541e-03,  3.7890e-03, -9.8100e-04, -3.4035e-02, -2.7063e-02,\n",
      "          3.4828e-02,  2.9146e-02, -2.8381e-02, -2.5027e-02, -1.2463e-02,\n",
      "         -3.5232e-02, -2.0167e-02, -2.1787e-02,  1.3130e-02, -6.6905e-03,\n",
      "          1.6402e-02, -1.1474e-02,  7.8435e-03, -2.7619e-02,  2.7143e-02,\n",
      "          1.8119e-02, -2.9541e-02, -1.1749e-02,  1.1016e-02,  1.0046e-02,\n",
      "         -1.9485e-02, -3.3792e-02, -3.2672e-02, -2.8135e-02,  1.3228e-02,\n",
      "         -3.4533e-03, -6.3542e-03, -9.2849e-03, -1.7587e-02,  8.4777e-03,\n",
      "         -1.6182e-02,  2.1167e-02,  9.8971e-03, -2.6887e-02,  7.6286e-03,\n",
      "          2.8827e-02, -7.3889e-03, -1.8531e-02, -1.8666e-02,  2.0801e-03,\n",
      "          1.8329e-02, -3.4164e-02,  1.2206e-02,  2.1807e-02, -3.1331e-02,\n",
      "          3.1198e-02, -1.0562e-02,  2.6663e-02,  4.0842e-03, -1.0756e-02,\n",
      "         -2.4642e-02,  2.7238e-02, -1.4441e-02,  1.5925e-02, -2.3257e-02,\n",
      "          2.5473e-02,  1.6001e-02,  3.5299e-03,  1.4361e-02,  1.3237e-02,\n",
      "          2.2010e-02, -3.4730e-02,  8.1381e-03,  2.3090e-02, -2.4520e-02,\n",
      "         -4.8470e-03, -1.6524e-02,  2.9118e-02, -3.0066e-02, -8.9105e-03,\n",
      "          9.1378e-04, -3.7085e-03,  2.3752e-02, -1.7747e-02, -5.0082e-03,\n",
      "         -1.9154e-02, -2.5383e-02,  3.0560e-02,  1.8157e-02, -2.6017e-02,\n",
      "         -3.3915e-02, -1.9659e-02,  2.8536e-02,  2.2614e-04, -1.9417e-02,\n",
      "          3.4515e-02, -3.2163e-02, -1.5379e-03,  2.4027e-02,  1.0481e-02,\n",
      "         -9.6909e-03,  5.9604e-03, -1.2963e-02, -2.9340e-02,  3.4003e-02,\n",
      "          1.1471e-02, -1.5914e-03, -3.1366e-02,  1.4968e-03,  2.4563e-02,\n",
      "          2.1810e-02,  1.7991e-02, -7.4700e-03,  8.0555e-03,  7.3160e-03,\n",
      "          2.6941e-02,  2.2049e-03,  9.2048e-03,  1.4021e-02,  2.7949e-02,\n",
      "          1.3573e-02, -2.4092e-02,  3.0570e-02, -1.6142e-02, -1.9658e-02,\n",
      "         -3.5076e-02, -9.4283e-03,  1.8871e-02,  3.3309e-02,  1.7520e-02,\n",
      "          3.0121e-02, -7.7401e-03,  2.4794e-02,  1.5192e-02, -9.4992e-03,\n",
      "         -1.3767e-02, -1.6468e-02, -1.7686e-02, -2.0176e-02, -1.2449e-02,\n",
      "         -1.6736e-02,  5.9652e-03, -2.5193e-02, -1.0695e-02, -2.9480e-02,\n",
      "          5.8932e-03, -1.6583e-02, -1.1752e-02, -1.2387e-02, -1.6646e-02,\n",
      "         -2.8620e-02, -1.9285e-02, -2.1780e-02, -2.7729e-02, -3.3892e-02,\n",
      "         -1.6352e-02,  3.0958e-02,  5.7496e-03,  2.5436e-02,  1.9732e-02,\n",
      "          3.5439e-02, -1.6370e-02, -5.2359e-03,  5.5316e-03,  1.4596e-02,\n",
      "          1.7678e-02,  2.8578e-02,  3.2312e-02,  1.4940e-02, -3.5306e-02,\n",
      "          1.6343e-02, -2.5582e-02,  2.3236e-03,  3.0747e-02,  1.0309e-03,\n",
      "         -2.0195e-02,  1.5568e-02, -1.9704e-02, -2.2818e-02, -3.5279e-02,\n",
      "         -1.5741e-02, -5.8454e-03,  1.5140e-02,  2.4210e-02, -1.0100e-02,\n",
      "          2.7682e-02, -2.4560e-03,  2.9797e-02, -3.4934e-02,  6.7863e-03,\n",
      "          1.1853e-02, -3.1297e-02,  3.4373e-02,  3.5639e-02,  2.8284e-02,\n",
      "         -2.5319e-03, -2.7183e-03,  2.1133e-02, -2.9704e-02,  2.8141e-02,\n",
      "          1.1745e-02,  1.8421e-02,  1.9943e-02, -4.1709e-03,  1.5250e-02,\n",
      "          2.1633e-02, -2.6661e-02, -1.1919e-02,  7.3755e-03,  6.1207e-03,\n",
      "         -1.3485e-02, -2.4376e-02, -3.5494e-02,  2.2457e-02, -2.7848e-02,\n",
      "         -2.1484e-02, -4.4006e-03,  8.1898e-03,  2.2349e-02, -3.5275e-02,\n",
      "          3.3750e-02,  6.4576e-03,  2.0416e-02,  2.5577e-02, -2.1019e-02,\n",
      "          1.1365e-02, -6.6662e-03, -2.6494e-03,  1.2961e-02, -3.4782e-02,\n",
      "          1.3548e-02, -8.5967e-03,  3.2714e-02, -1.6606e-02, -7.4311e-03,\n",
      "          3.0868e-02,  9.5644e-03,  8.5544e-03,  1.1642e-02,  9.0012e-04,\n",
      "          6.4219e-05,  2.8762e-02,  3.2726e-02,  1.8667e-02,  2.7446e-02,\n",
      "         -5.9236e-03, -2.1656e-02, -1.0626e-02,  2.5084e-02,  2.3921e-02,\n",
      "         -3.5164e-02,  2.7385e-02,  2.7638e-02, -3.3348e-02, -3.0818e-02,\n",
      "         -5.4808e-03, -5.3036e-03,  8.1950e-03, -1.5616e-02, -1.0896e-02,\n",
      "          5.6204e-03,  1.1998e-02, -3.5012e-04,  1.4936e-02, -3.0030e-02,\n",
      "          8.3138e-03, -7.2357e-03,  2.5539e-02, -1.9523e-02,  2.1915e-02,\n",
      "         -9.6245e-03, -9.2328e-03, -1.7539e-02, -1.8289e-02, -1.8703e-02,\n",
      "          1.6133e-02,  1.4120e-02,  1.6929e-02, -3.3483e-02, -5.1286e-03,\n",
      "         -2.0641e-02,  1.9449e-02,  3.0611e-02, -2.3178e-03, -2.9792e-02,\n",
      "         -2.4670e-02,  1.7488e-02,  1.6813e-03,  7.2565e-03, -1.8554e-02,\n",
      "          2.5361e-03, -1.9830e-02, -1.8931e-02, -1.2329e-02,  1.2831e-02,\n",
      "         -1.0224e-02,  2.4965e-02, -1.9142e-02, -8.1216e-03,  1.3325e-02,\n",
      "         -2.4098e-02,  3.3708e-02,  2.1035e-02, -2.5478e-02,  2.3147e-02,\n",
      "          8.8361e-03,  2.3970e-02, -7.9142e-03,  1.8984e-02, -5.3059e-03,\n",
      "          1.3592e-02, -3.5252e-02, -2.8768e-02, -1.6282e-02,  3.2225e-02,\n",
      "          3.0371e-02,  8.9895e-03, -1.9518e-02, -2.9844e-02,  3.4294e-02,\n",
      "          5.7662e-03,  3.1963e-02,  7.8247e-03,  1.5199e-02, -9.9490e-03,\n",
      "         -1.0088e-03, -3.5872e-02, -1.9479e-02, -7.9414e-03, -2.7622e-02,\n",
      "          5.2097e-03, -3.5962e-02,  3.1072e-02,  5.0069e-03, -1.2220e-02,\n",
      "          2.4022e-02,  8.7507e-03, -2.5268e-02,  3.5662e-02, -1.8608e-02,\n",
      "          1.7290e-02, -1.8506e-02, -2.1033e-02, -7.9280e-03, -1.6166e-02,\n",
      "          2.9763e-02,  2.5346e-02, -1.0304e-02,  3.8526e-03, -1.8420e-02,\n",
      "          3.5573e-03,  3.4625e-02, -5.0540e-03,  6.5837e-03,  4.6825e-04,\n",
      "          7.5118e-03,  2.8384e-02,  1.5038e-03,  6.3702e-03,  3.3335e-02,\n",
      "         -2.6664e-02, -3.0439e-02,  5.8255e-03,  2.2488e-02,  1.0667e-03,\n",
      "          8.0759e-03, -1.5271e-02,  2.8329e-02,  8.6155e-04,  2.0305e-02,\n",
      "          7.7238e-03,  1.6776e-02,  3.3117e-02, -3.8448e-03, -2.6573e-02,\n",
      "         -1.1925e-03, -1.9030e-02,  2.0250e-03, -1.4662e-02,  2.5748e-02,\n",
      "         -3.4522e-02,  7.9106e-03,  2.5144e-03, -1.0377e-02,  1.7564e-02,\n",
      "         -1.4602e-02,  2.0132e-02,  2.5926e-02, -1.0507e-02, -2.2293e-02,\n",
      "          3.1488e-02,  9.9522e-03,  2.2322e-02,  1.8517e-02, -1.1073e-02,\n",
      "         -8.0247e-03, -1.3931e-02,  2.3869e-02, -3.1839e-02,  2.3518e-02,\n",
      "         -2.4976e-02, -2.6171e-02,  3.0217e-02, -1.4557e-04, -3.4274e-02,\n",
      "         -8.8118e-03, -2.3859e-02,  3.2100e-02,  1.8758e-02, -3.4360e-02,\n",
      "         -6.4644e-03,  2.0443e-02, -2.5979e-02,  1.5099e-02,  6.2914e-03,\n",
      "         -1.9479e-02,  2.1717e-02, -5.0004e-03, -1.2260e-02,  6.6862e-03,\n",
      "         -2.9841e-02, -2.9378e-02, -3.2239e-03, -5.4100e-03,  3.1062e-02,\n",
      "         -7.1859e-03, -7.7730e-03,  3.0293e-02, -2.8518e-02,  5.4811e-03,\n",
      "          5.1987e-03,  1.3864e-02, -2.3057e-02,  1.5730e-03,  2.7122e-02,\n",
      "          3.5146e-02,  2.8652e-02,  2.2291e-02, -1.2718e-02, -1.2067e-02,\n",
      "          4.2951e-04,  3.2528e-02, -3.2240e-03, -2.0746e-02, -2.4431e-02,\n",
      "          2.0393e-02, -3.3317e-02,  3.1650e-02,  2.8232e-02, -2.6380e-02,\n",
      "          3.1952e-02, -2.0634e-02, -7.9277e-03,  3.3934e-02,  1.6822e-02,\n",
      "         -1.9217e-02,  2.9738e-02, -3.0775e-02,  5.6607e-03, -3.5985e-02,\n",
      "          2.7701e-02,  3.3128e-02,  3.5806e-02,  2.7954e-02, -2.3954e-02,\n",
      "          1.2134e-02,  1.1647e-02, -2.2866e-02,  2.5800e-02, -2.6161e-02,\n",
      "         -1.4642e-03,  1.0609e-02, -1.0956e-02, -2.5674e-02, -1.3618e-02,\n",
      "         -3.5488e-02,  1.0656e-02,  1.1315e-03, -3.2876e-02, -9.8092e-03,\n",
      "         -2.8762e-02, -9.7685e-03, -3.4331e-02, -3.3038e-02, -1.3084e-02,\n",
      "          3.1981e-03, -2.5370e-02,  1.2811e-02,  2.1268e-02, -1.3567e-02,\n",
      "          1.9378e-02,  2.3402e-03,  2.2529e-02, -6.9713e-03,  1.0355e-02,\n",
      "         -3.5096e-02,  3.4486e-02, -2.0042e-03,  2.6490e-02, -3.0850e-02,\n",
      "          1.4329e-02, -2.4065e-02,  1.7800e-02,  1.6209e-02,  3.0973e-02,\n",
      "         -2.7871e-02, -3.0267e-02,  1.0338e-03,  7.5179e-03, -2.8778e-02,\n",
      "          7.1960e-03, -2.1631e-02,  3.4047e-02,  1.8890e-02,  3.2670e-02,\n",
      "          4.3299e-03, -4.7715e-03, -3.1812e-02, -2.1467e-02,  3.5771e-02,\n",
      "          2.0511e-02,  2.7815e-02, -3.5780e-02, -1.2045e-02, -2.2358e-02,\n",
      "         -9.3071e-03,  1.6166e-02, -3.5393e-02, -3.5242e-03, -3.5338e-02,\n",
      "         -1.4288e-02,  1.2247e-03,  5.5024e-03, -2.9316e-02, -2.6249e-02,\n",
      "         -1.4501e-02, -1.8944e-03,  2.6715e-02,  2.6127e-02,  2.4527e-03,\n",
      "         -3.2194e-02,  1.7847e-02, -3.4720e-02, -6.3145e-03, -1.8255e-02,\n",
      "          2.9297e-02,  1.4443e-02, -2.4097e-02, -3.2182e-02,  2.0929e-02,\n",
      "          1.1629e-04, -6.6713e-03, -3.5565e-02,  3.3612e-02,  2.5562e-02,\n",
      "          3.5963e-03,  2.9147e-02,  1.6262e-02,  3.4012e-02, -6.4641e-03,\n",
      "          2.5674e-02, -2.4949e-02, -3.2300e-02, -2.8879e-02,  1.9851e-02,\n",
      "          1.0729e-02,  1.2190e-02,  2.6451e-02,  6.0782e-03,  2.3490e-02,\n",
      "          6.3208e-04,  6.2301e-03, -2.2691e-02,  1.7442e-02,  1.4483e-02,\n",
      "         -1.3709e-02,  8.6757e-03, -2.5130e-02,  1.9323e-02,  1.9102e-02,\n",
      "         -1.2254e-02, -2.1201e-02, -2.7418e-02, -1.7245e-02,  1.2071e-02,\n",
      "         -2.2247e-02,  2.7779e-02, -3.3591e-02, -2.8314e-02,  1.9550e-02,\n",
      "          2.9932e-03,  2.4959e-02,  1.5587e-02, -1.2431e-02,  1.4643e-02,\n",
      "         -2.1547e-02, -5.8432e-03, -1.7304e-02, -2.1822e-02,  2.0596e-02,\n",
      "         -7.8072e-03,  2.9798e-02,  1.6484e-02, -7.9908e-03, -2.7581e-02,\n",
      "         -1.7117e-02, -6.4190e-04,  2.9927e-04, -3.5615e-02,  6.2371e-03,\n",
      "          1.1091e-04, -3.1715e-02,  6.0793e-03, -5.1404e-03, -6.6220e-03,\n",
      "         -1.6376e-02,  1.8689e-02,  2.2168e-02,  7.3732e-03, -3.1254e-02,\n",
      "         -5.7102e-03, -3.1351e-02, -3.9174e-03, -1.4145e-02, -2.3762e-02,\n",
      "          1.0028e-02,  1.2184e-02,  3.4579e-02,  3.1400e-03,  1.8155e-02,\n",
      "         -3.0343e-02,  1.9555e-02, -3.0278e-02,  1.8369e-02,  6.9276e-04,\n",
      "         -2.4810e-02, -7.0596e-03, -2.6150e-03, -1.4285e-02, -3.0699e-02,\n",
      "         -3.2977e-02,  2.0294e-02,  1.5512e-02,  2.2077e-02,  4.6424e-03,\n",
      "         -2.7593e-02, -2.1593e-02,  1.8654e-02, -2.1764e-02, -2.8856e-02,\n",
      "         -7.1321e-03, -2.7216e-02,  2.6835e-02, -2.5217e-02, -7.0071e-03,\n",
      "         -3.5353e-03, -3.2492e-02,  2.5179e-02,  3.3087e-02, -7.5624e-04,\n",
      "          5.8395e-03,  2.0701e-02,  3.2669e-02,  1.2546e-02,  2.8457e-02,\n",
      "         -1.2402e-02,  2.5773e-02,  2.3960e-02, -8.6967e-03,  2.0016e-02,\n",
      "          9.5149e-03,  3.4632e-03,  9.5956e-03,  7.5758e-03,  2.2107e-02,\n",
      "          2.0800e-02, -2.3105e-02, -9.7516e-04, -5.3501e-03,  1.3338e-02,\n",
      "         -2.8059e-02,  1.9001e-02,  3.2814e-04,  6.7878e-04,  3.5065e-02,\n",
      "         -1.4756e-02, -2.9948e-02, -1.5432e-02,  8.4987e-03,  1.0050e-02,\n",
      "         -2.4523e-02,  3.2765e-02,  3.3169e-03,  1.0869e-02,  1.8045e-02,\n",
      "          3.3794e-02,  3.2095e-02,  2.6728e-02,  1.6596e-02, -2.3470e-02,\n",
      "         -2.5320e-03,  2.9864e-02,  3.1569e-02, -8.2327e-03, -3.5238e-02,\n",
      "          3.5188e-02,  2.7808e-03,  2.8627e-02, -3.5225e-02, -8.1189e-03,\n",
      "          2.3522e-02,  2.8591e-02,  7.1750e-03, -1.3683e-02,  1.0965e-02,\n",
      "          2.7331e-02,  2.3644e-02,  3.3090e-02,  2.2933e-02, -1.7366e-02,\n",
      "         -2.2176e-02,  1.3435e-02, -2.7065e-02,  1.4668e-02,  1.1824e-03,\n",
      "         -8.1365e-03, -1.9715e-02, -1.6021e-02]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: reprogramming_layer.value_projection.bias | Size: torch.Size([1024]) | Values : tensor([0.0259], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: reprogramming_layer.out_projection.weight | Size: torch.Size([768, 1024]) | Values : tensor([[-0.0236,  0.0018, -0.0198,  ..., -0.0242,  0.0108,  0.0096]],\n",
      "       grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: reprogramming_layer.out_projection.bias | Size: torch.Size([768]) | Values : tensor([0.0310], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: output_projection.linear.weight | Size: torch.Size([17, 768]) | Values : tensor([[-1.1259e-02, -1.8216e-02, -3.1895e-02,  1.1669e-02, -3.0677e-02,\n",
      "         -5.9189e-03, -2.7730e-02,  3.1481e-02,  2.8209e-03,  2.4343e-02,\n",
      "         -3.3723e-02, -2.8179e-03, -2.5698e-02, -7.2369e-03,  3.3953e-02,\n",
      "          2.0000e-02, -3.3217e-02,  1.5725e-03,  2.3725e-02, -1.1057e-02,\n",
      "          5.3308e-03, -3.5231e-02, -3.5962e-02,  5.6069e-03,  1.4589e-02,\n",
      "          3.4467e-02, -1.2594e-02,  2.7164e-02,  5.0141e-03,  9.2437e-03,\n",
      "          2.7681e-02,  3.3415e-02,  3.3407e-02,  2.1282e-02,  1.5422e-02,\n",
      "          1.3477e-03,  1.9563e-02, -1.9528e-02, -3.4204e-02,  2.3715e-02,\n",
      "         -1.4701e-02,  4.8491e-03,  2.5757e-02,  1.2516e-02, -3.6559e-03,\n",
      "         -5.0249e-03,  3.4554e-02, -3.0618e-02, -2.9916e-02, -1.5828e-02,\n",
      "         -6.0874e-03,  1.9000e-02,  1.9719e-02,  1.2508e-02, -1.2135e-02,\n",
      "         -3.4959e-03,  3.4833e-02,  1.5760e-02,  6.1613e-03,  2.4892e-02,\n",
      "          5.2390e-04,  3.3654e-02, -4.0702e-03, -2.8570e-03,  6.6115e-03,\n",
      "          1.1128e-02,  2.9449e-02, -2.2064e-02, -3.5510e-02, -2.3116e-03,\n",
      "          6.4967e-03, -5.1152e-04,  1.0748e-02,  2.4650e-02,  2.8628e-02,\n",
      "         -1.6984e-02,  8.7520e-03, -1.7249e-02,  1.8402e-02,  1.0526e-02,\n",
      "         -3.1625e-02,  2.1065e-02,  2.7404e-02,  1.6887e-02,  4.4378e-03,\n",
      "          1.2795e-02,  3.3605e-02, -3.0604e-02,  6.6635e-03, -2.1665e-02,\n",
      "         -1.0849e-02,  3.6001e-02,  1.3288e-02, -2.4468e-02,  2.6795e-02,\n",
      "         -2.0711e-02,  3.5623e-02,  4.8187e-03,  8.9132e-03,  1.2219e-02,\n",
      "         -3.2159e-02,  9.1809e-03, -1.4748e-02, -2.0712e-02, -5.2089e-03,\n",
      "          2.4645e-04, -4.4417e-03,  2.3849e-02,  2.7794e-02,  7.1308e-04,\n",
      "         -3.1860e-02, -3.4046e-02, -4.5006e-03,  1.4067e-02,  2.9848e-02,\n",
      "          1.1425e-02,  2.3015e-02, -3.2641e-02, -5.0066e-03, -5.2305e-03,\n",
      "          2.0427e-02, -2.5713e-02, -1.7525e-02, -8.3915e-03,  6.5751e-03,\n",
      "         -3.7545e-04,  2.6779e-03, -2.9053e-02,  2.1931e-02,  9.4966e-03,\n",
      "          1.8341e-02, -2.4555e-02, -5.3620e-03,  2.3240e-02,  3.5525e-02,\n",
      "          2.6729e-02, -2.7348e-02, -1.3618e-02, -3.3458e-02,  1.1792e-02,\n",
      "         -2.2117e-02,  2.3704e-02, -1.2345e-02, -3.4811e-03, -3.0727e-02,\n",
      "          2.9511e-02, -3.0798e-02,  6.5889e-03, -2.3541e-02,  3.5603e-02,\n",
      "          3.0334e-02, -2.4728e-02,  2.2526e-02, -7.1366e-03,  3.1592e-03,\n",
      "          2.2441e-02, -1.6600e-02,  2.4255e-04,  2.9248e-02,  1.6016e-02,\n",
      "          7.4429e-03, -1.0760e-02,  3.1561e-02,  1.7687e-02, -2.5851e-02,\n",
      "          1.9823e-02,  6.3673e-03, -3.0290e-02,  2.6274e-02,  4.6560e-03,\n",
      "         -2.8928e-02, -2.9019e-02,  2.5367e-04, -1.8754e-02,  8.4557e-04,\n",
      "          1.7549e-02, -1.7890e-02,  1.4981e-02,  3.0041e-02, -4.9261e-03,\n",
      "          9.8857e-03, -1.9745e-02,  2.7251e-02, -2.7324e-02,  1.6085e-02,\n",
      "         -2.2169e-02, -3.5689e-02,  9.1899e-03, -2.4239e-02,  1.1444e-02,\n",
      "          2.0095e-02,  2.6396e-02, -8.5013e-03,  3.5252e-02, -2.7021e-02,\n",
      "         -5.4149e-03,  1.6208e-02,  1.2498e-02,  2.2321e-03, -2.3652e-03,\n",
      "          2.8214e-02,  1.7698e-02, -2.8991e-03, -1.4416e-02, -2.8321e-02,\n",
      "          3.0669e-02,  9.1027e-03, -1.0700e-02, -1.6068e-02, -1.0333e-02,\n",
      "          3.0740e-02,  5.8854e-03, -8.5707e-03, -9.8485e-03,  1.4796e-02,\n",
      "          1.7269e-02, -1.3678e-02, -2.9684e-04, -3.4803e-02, -2.1507e-02,\n",
      "         -1.0506e-02, -3.4204e-02,  7.5248e-03, -2.2857e-02, -3.0272e-02,\n",
      "         -7.4921e-03, -3.2886e-02,  4.6953e-03, -1.2512e-02, -1.6260e-02,\n",
      "          2.3118e-02,  2.8867e-03,  1.8515e-02,  2.6491e-02, -2.2028e-02,\n",
      "          1.2314e-02,  8.0061e-03,  1.1659e-02, -3.2419e-02,  2.4578e-02,\n",
      "          4.2570e-03, -6.3884e-03, -3.1001e-03, -3.7901e-03,  1.9442e-02,\n",
      "         -1.3646e-02, -3.8195e-04,  8.0231e-04, -2.3288e-02,  2.0042e-02,\n",
      "         -1.4564e-02,  1.7685e-02, -3.5741e-02, -5.6961e-03,  4.7474e-03,\n",
      "          2.3492e-03,  1.2483e-04, -3.0759e-02, -1.2753e-02, -1.2730e-02,\n",
      "          2.4113e-02, -2.1079e-03, -1.7154e-02, -4.1903e-03,  1.1570e-02,\n",
      "          2.9237e-02, -1.2135e-02,  9.9948e-03,  9.5228e-03, -2.5014e-03,\n",
      "          1.0494e-03,  1.1531e-02,  3.5269e-03,  2.1144e-04, -3.4265e-02,\n",
      "         -1.8300e-02,  3.1978e-02,  2.4325e-02,  2.1922e-02, -1.6785e-02,\n",
      "         -1.1030e-03,  1.8021e-02,  3.0933e-03,  1.9077e-02, -3.3408e-02,\n",
      "          5.3325e-03,  3.2521e-02,  1.4734e-02, -4.0077e-03,  2.7461e-02,\n",
      "         -1.9257e-02,  1.4646e-02,  3.2608e-02, -7.9270e-04,  2.7750e-02,\n",
      "         -1.2365e-02, -2.5875e-02, -2.1342e-02,  7.7201e-04, -2.1752e-02,\n",
      "         -3.5466e-02, -4.0839e-03, -3.5903e-02, -2.9033e-02, -2.2300e-02,\n",
      "          2.3956e-02, -2.9710e-02, -8.9989e-03, -3.3379e-02, -2.4702e-02,\n",
      "         -2.1830e-02,  2.4224e-02, -1.4017e-02,  3.3307e-02,  1.7282e-02,\n",
      "          2.9379e-03,  2.4509e-02,  1.5693e-03, -2.4111e-02,  1.7272e-02,\n",
      "         -2.9762e-02, -2.9294e-02, -9.9164e-03,  2.0061e-02, -2.9425e-02,\n",
      "         -2.6450e-02, -2.6567e-02, -2.0557e-02,  1.6030e-02,  1.2318e-02,\n",
      "         -1.7597e-02, -8.5051e-04,  3.1218e-02,  1.0751e-03, -6.9864e-03,\n",
      "         -1.3283e-02,  1.5484e-02,  2.3316e-02,  9.5431e-03,  4.7891e-03,\n",
      "         -3.3134e-02,  9.6572e-03,  3.4989e-02, -2.3936e-02, -3.5610e-02,\n",
      "         -3.1189e-02, -3.5310e-02,  3.2839e-02,  2.0300e-02, -8.2763e-03,\n",
      "          7.9416e-04,  7.3571e-03, -7.7581e-03,  1.9511e-03, -2.9084e-02,\n",
      "          3.3314e-02,  1.9186e-02, -2.2619e-03, -1.5637e-02, -2.8998e-02,\n",
      "         -2.2009e-02,  1.6123e-02, -2.2261e-03,  1.5706e-02, -2.9191e-02,\n",
      "          1.9309e-02,  1.1610e-02,  2.6767e-02, -3.0868e-02,  1.0661e-02,\n",
      "          2.4753e-02,  3.5919e-02, -1.0953e-02, -3.2025e-02, -1.4135e-02,\n",
      "          2.8720e-02, -3.4193e-02,  3.0229e-02,  1.9127e-02, -2.8754e-02,\n",
      "         -5.2299e-03, -3.2321e-02,  1.3435e-02, -2.0856e-02, -9.3771e-04,\n",
      "          2.5160e-03,  2.3850e-02,  3.1575e-02, -1.9611e-02,  1.6851e-03,\n",
      "          1.7254e-04,  2.0511e-02,  5.6663e-03,  1.5088e-02, -3.6354e-03,\n",
      "         -3.1791e-02,  3.0011e-02, -1.0847e-02, -3.1120e-02, -2.9756e-02,\n",
      "          2.2049e-02, -2.7599e-02,  3.3181e-02,  2.2685e-02, -5.4068e-03,\n",
      "          1.2972e-02, -2.5924e-02, -3.1113e-02, -2.3097e-02, -2.0258e-02,\n",
      "         -1.8816e-02,  2.5099e-02,  1.3987e-02, -3.1557e-02,  3.3933e-02,\n",
      "         -7.9793e-03,  2.0638e-02,  3.1325e-02, -2.6389e-02,  1.3279e-02,\n",
      "          7.4219e-03, -7.4821e-03, -4.6039e-03, -6.0329e-03,  1.7585e-02,\n",
      "          1.7131e-02, -3.1838e-02,  1.5296e-02, -8.9640e-03,  1.8169e-02,\n",
      "          1.8055e-02,  1.3910e-02,  5.4962e-04,  3.1630e-02,  3.1259e-02,\n",
      "          3.3153e-02, -5.0470e-03, -3.1443e-02,  1.2861e-02, -9.7792e-03,\n",
      "         -2.7260e-02,  1.9569e-02,  1.6378e-02,  3.3080e-02,  1.8522e-02,\n",
      "         -2.7849e-02,  5.2177e-03,  2.8025e-02, -2.1014e-02,  3.1150e-02,\n",
      "         -4.5168e-03,  1.0853e-02,  1.3647e-03, -3.1761e-02,  2.8968e-02,\n",
      "         -2.6372e-02, -1.6081e-02,  3.4354e-02, -3.5572e-02,  3.3788e-02,\n",
      "          8.6874e-04,  1.3286e-03,  6.3380e-03, -3.5868e-02, -1.3891e-02,\n",
      "         -9.4924e-03, -1.5709e-03,  3.2285e-02, -2.6467e-02,  3.6011e-02,\n",
      "         -8.8837e-03, -7.7706e-03,  4.9821e-03, -3.4080e-02, -2.9284e-02,\n",
      "         -1.5791e-02, -3.2673e-02,  3.1354e-02, -1.5017e-02, -6.6498e-03,\n",
      "         -2.8298e-02,  3.2834e-02, -1.6266e-02,  1.1340e-02, -9.3635e-03,\n",
      "         -3.5406e-02,  4.8145e-03,  2.8011e-02, -8.5219e-03, -2.5694e-02,\n",
      "          8.7015e-03, -1.0203e-02, -3.3361e-02, -4.6747e-03,  1.5435e-02,\n",
      "         -3.1086e-02, -1.3246e-02, -2.4293e-03, -1.8702e-02, -1.5451e-02,\n",
      "          1.8211e-02, -2.9211e-04, -2.3785e-02, -5.6741e-03,  1.7199e-02,\n",
      "         -2.1678e-02,  9.0961e-03, -2.7938e-03, -1.9609e-02, -3.0219e-02,\n",
      "          2.4478e-02,  6.4739e-03,  3.2210e-02,  6.3178e-03,  1.8753e-02,\n",
      "         -2.5658e-02,  2.8607e-02, -2.9117e-02,  5.4162e-03,  1.2925e-02,\n",
      "         -2.2731e-02,  1.5795e-02, -1.5740e-02, -1.2580e-02,  2.7911e-02,\n",
      "          3.1131e-02, -2.9817e-02,  3.2956e-02, -1.6217e-02, -2.7675e-02,\n",
      "          2.9144e-02, -1.7639e-03,  5.1051e-03,  2.8886e-02, -2.2627e-02,\n",
      "         -2.8350e-02,  1.9179e-02,  2.7055e-02,  2.1912e-02, -1.1186e-02,\n",
      "         -1.8280e-02,  8.2910e-03, -1.5278e-02,  1.0702e-02,  1.7565e-02,\n",
      "         -1.1405e-02, -1.8754e-02, -1.2915e-02,  2.5455e-02,  3.4779e-02,\n",
      "         -3.2130e-02, -3.1539e-02, -3.1475e-02, -1.1778e-02,  5.7157e-03,\n",
      "         -2.5368e-02,  2.1955e-02, -7.2020e-03,  5.6910e-03,  2.2555e-02,\n",
      "         -3.4727e-02,  1.3790e-03, -2.4643e-02,  2.0025e-02,  2.2747e-02,\n",
      "         -2.5251e-02,  3.1516e-03,  2.5155e-02,  2.1116e-02, -2.7841e-02,\n",
      "          1.7639e-02,  1.4138e-02, -1.1110e-02,  4.9476e-03,  2.7543e-02,\n",
      "          9.1185e-03,  2.7847e-03,  3.4839e-02,  2.4948e-02, -2.5202e-02,\n",
      "         -2.6086e-02, -1.1281e-02,  3.0317e-02,  9.2379e-04, -2.0166e-02,\n",
      "          3.1770e-02,  4.1297e-03, -1.9644e-02, -3.4468e-02,  7.6402e-03,\n",
      "          2.6479e-02,  1.6563e-02, -8.6496e-03,  1.2121e-02,  6.6398e-03,\n",
      "          1.1988e-02,  2.2643e-03, -2.6952e-02,  4.3187e-03, -1.4633e-02,\n",
      "          2.8270e-02,  2.5148e-04, -2.6962e-02,  2.3840e-02, -2.3974e-02,\n",
      "          2.1272e-02,  1.9025e-02, -2.3734e-02, -2.8811e-03,  2.8554e-02,\n",
      "         -2.0467e-02,  3.6702e-03, -1.4338e-03, -1.9839e-02,  1.5537e-02,\n",
      "         -2.8870e-02, -2.3011e-03, -7.6496e-03, -2.1565e-02,  1.1049e-03,\n",
      "          2.9025e-03, -2.9883e-02,  1.9120e-02,  5.1427e-03,  1.2480e-02,\n",
      "          3.5431e-02,  2.9460e-02, -1.2216e-02, -8.4680e-03,  3.3182e-02,\n",
      "         -1.7386e-02, -1.2748e-02, -3.5712e-02, -2.8971e-02, -2.4326e-02,\n",
      "          2.0348e-02,  7.1279e-03,  3.0154e-05,  2.1528e-02, -1.3859e-02,\n",
      "         -3.1854e-02, -1.7075e-02, -2.9872e-02, -2.0231e-02, -2.0101e-02,\n",
      "          1.6710e-02, -1.4466e-02, -2.2443e-02,  8.1796e-03,  1.9688e-02,\n",
      "         -2.2833e-02,  8.5944e-03, -1.9398e-02,  1.9385e-02, -4.4234e-03,\n",
      "         -2.2539e-02, -1.3247e-02, -1.1546e-02,  3.3049e-02,  1.6550e-03,\n",
      "         -2.0864e-02, -1.9542e-03, -2.3986e-02, -1.0329e-02,  3.3646e-02,\n",
      "          2.9854e-03, -7.4755e-03,  1.1979e-02,  2.6708e-02, -1.4657e-02,\n",
      "          2.6265e-02,  2.2823e-02, -5.4008e-03, -1.8987e-02,  4.5071e-03,\n",
      "          9.4920e-03,  3.0059e-02,  1.5735e-03,  3.0051e-02, -4.0188e-03,\n",
      "         -9.7506e-03, -1.1016e-03,  2.7284e-02, -8.6769e-03, -2.2827e-02,\n",
      "          3.5140e-02,  3.3570e-02,  2.0240e-02,  3.5728e-02,  1.6205e-02,\n",
      "         -4.3763e-03,  7.3876e-03,  2.0845e-02,  3.0652e-03, -2.7037e-02,\n",
      "         -3.4396e-02, -2.1765e-03, -1.4552e-02, -7.6829e-03,  3.2535e-02,\n",
      "         -1.8076e-02, -2.8713e-02, -7.5115e-03,  7.4926e-03,  3.4661e-03,\n",
      "          1.1773e-02, -3.5410e-02,  6.4148e-03,  1.5473e-02,  2.0993e-02,\n",
      "         -1.6089e-02, -3.8098e-04, -3.0115e-02,  2.3664e-02, -1.2482e-02,\n",
      "          2.6123e-03,  1.5853e-02,  8.3571e-04,  1.9681e-02, -2.4167e-02,\n",
      "         -1.3612e-02,  1.3834e-02,  3.1672e-02, -1.2982e-02, -2.4632e-02,\n",
      "          7.4668e-03,  1.5346e-02, -1.0064e-02,  5.0829e-03, -1.8702e-02,\n",
      "          1.3234e-02, -1.2806e-02, -2.2677e-02, -1.6642e-04,  8.8104e-03,\n",
      "         -7.5377e-03, -3.0449e-02, -1.3154e-02, -2.8344e-02,  2.1376e-02,\n",
      "         -3.0809e-02, -6.5156e-03,  4.0899e-03, -1.5013e-02,  2.0078e-02,\n",
      "         -2.5389e-03,  4.3046e-03,  1.7172e-02, -2.5166e-02,  2.5418e-02,\n",
      "         -1.5173e-02, -7.2936e-03,  2.0805e-02,  3.5100e-02, -5.9843e-03,\n",
      "          2.7544e-02, -2.2347e-02,  7.6689e-03,  2.3784e-02,  3.4103e-02,\n",
      "          1.8361e-02,  3.0398e-02,  9.1697e-03,  3.4681e-02, -1.6284e-02,\n",
      "          2.0487e-02, -2.7991e-03, -1.4448e-02]], grad_fn=<SliceBackward0>) \n",
      "\n",
      "Layer: output_projection.linear.bias | Size: torch.Size([17]) | Values : tensor([-0.0201], grad_fn=<SliceBackward0>) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for name , param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"Layer: {name} | Size: {param.size()} | Values : {param[:1]} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vertex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
