{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/envs/vertex/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import torch\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "from data_provider.m4 import M4Meta\n",
    "from models import Autoformer, DLinear\n",
    "\n",
    "from data_provider.data_factory import data_provider\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "from utils.losses import smape_loss\n",
    "from utils.m4_summary import M4Summary\n",
    "import os\n",
    "\n",
    "os.environ['CURL_CA_BUNDLE'] = ''\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:64\"\n",
    "\n",
    "from utils.tools import del_files, EarlyStopping, adjust_learning_rate, load_content, test_MS\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Time-LLM')\n",
    "\n",
    "fix_seed = 2021\n",
    "random.seed(fix_seed)\n",
    "torch.manual_seed(fix_seed)\n",
    "np.random.seed(fix_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_MS(args, accelerator, model, train_loader, vali_loader, criterion):\n",
    "    x, _ = train_loader.dataset.last_insample_window()\n",
    "    y = vali_loader.dataset.timeseries\n",
    "    x = torch.tensor(x, dtype=torch.float32).to(accelerator.device)\n",
    "    print(\"Shape of X eval\", x.shape)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        B, _, C = x.shape\n",
    "        dec_inp = torch.zeros((B, args.pred_len, C)).float().to(accelerator.device)\n",
    "        dec_inp = torch.cat([x[:, -args.label_len:, :], dec_inp], dim=1)\n",
    "        outputs = torch.zeros((B, args.pred_len, C)).float().to(accelerator.device)\n",
    "        id_list = np.arange(0, B, args.eval_batch_size)\n",
    "        id_list = np.append(id_list, B)\n",
    "        for i in range(len(id_list) - 1):\n",
    "            outputs[id_list[i]:id_list[i + 1], :, :] = model(\n",
    "                x[id_list[i]:id_list[i + 1]],\n",
    "                None,\n",
    "                dec_inp[id_list[i]:id_list[i + 1]],\n",
    "                None\n",
    "            )\n",
    "        accelerator.wait_for_everyone()\n",
    "        outputs = accelerator.gather_for_metrics(outputs)\n",
    "        print(\"Shape of output eval before choosing\", outputs.shape)\n",
    "        f_dim = -1 if args.features == 'MS' else 0\n",
    "        outputs = outputs[:, -args.pred_len:, f_dim:]\n",
    "        pred = outputs\n",
    "        true = torch.from_numpy(np.array(y)).to(accelerator.device)\n",
    "        print(\"Shape of y eval\", true.shape)\n",
    "        batch_y_mark = torch.ones(true.shape).to(accelerator.device)\n",
    "        true = accelerator.gather_for_metrics(true)\n",
    "        batch_y_mark = accelerator.gather_for_metrics(batch_y_mark)\n",
    "\n",
    "        loss = criterion(None, 0, pred, true, batch_y_mark)\n",
    "\n",
    "    model.train()\n",
    "    return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import LlamaConfig, LlamaModel, LlamaTokenizer, GPT2Config, GPT2Model, GPT2Tokenizer, BertConfig, \\\n",
    "    BertModel, BertTokenizer\n",
    "from layers.Embed import PatchEmbedding\n",
    "import transformers\n",
    "from layers.StandardNorm import Normalize\n",
    "from vertexai.preview import VertexModel # VertexModel\n",
    "import vertexai\n",
    "from utils.tools import del_files, EarlyStopping, adjust_learning_rate, vali, load_content\n",
    "\n",
    "\n",
    "from torch import optim\n",
    "from torch.optim import lr_scheduler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from accelerate import Accelerator, DeepSpeedPlugin\n",
    "from accelerate import DistributedDataParallelKwargs\n",
    "\n",
    "from data_provider.data_factory import data_provider\n",
    "from utils.losses import smape_loss\n",
    "\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "class FlattenHead(nn.Module):\n",
    "    def __init__(self, n_vars, nf, target_window, head_dropout=0):\n",
    "        super().__init__()\n",
    "        self.n_vars = n_vars\n",
    "        self.flatten = nn.Flatten(start_dim=-2)\n",
    "        self.linear = nn.Linear(nf, target_window)\n",
    "        self.dropout = nn.Dropout(head_dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Model(nn.Module,VertexModel):\n",
    "\n",
    "    def __init__(self, configs, patch_len=16, stride=8):\n",
    "        nn.Module.__init__(self)\n",
    "        VertexModel.__init__(self)\n",
    "        self.task_name = configs.task_name\n",
    "        self.pred_len = configs.pred_len\n",
    "        self.seq_len = configs.seq_len\n",
    "        self.d_ff = configs.d_ff\n",
    "        self.top_k = 5\n",
    "        self.d_llm = configs.llm_dim\n",
    "        self.patch_len = configs.patch_len\n",
    "        self.stride = configs.stride\n",
    "        self.args = configs\n",
    "\n",
    "        if configs.llm_model == 'LLAMA':\n",
    "            # self.llama_config = LlamaConfig.from_pretrained('/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/')\n",
    "            self.llama_config = LlamaConfig.from_pretrained('huggyllama/llama-7b')\n",
    "            self.llama_config.num_hidden_layers = configs.llm_layers\n",
    "            self.llama_config.output_attentions = True\n",
    "            self.llama_config.output_hidden_states = True\n",
    "            try:\n",
    "                self.llm_model = LlamaModel.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    config=self.llama_config,\n",
    "                    # load_in_4bit=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads model from HF is not already done\n",
    "                print(\"Local model files not found. Attempting to download...\")\n",
    "                self.llm_model = LlamaModel.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.llama_config,\n",
    "                    # load_in_4bit=True\n",
    "                )\n",
    "            try:\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/tokenizer.model\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads the tokenizer from HF if not already done\n",
    "                print(\"Local tokenizer files not found. Atempting to download them..\")\n",
    "                self.tokenizer = LlamaTokenizer.from_pretrained(\n",
    "                    # \"/mnt/alps/modelhub/pretrained_model/LLaMA/7B_hf/tokenizer.model\",\n",
    "                    'huggyllama/llama-7b',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "        elif configs.llm_model == 'GPT2':\n",
    "            self.gpt2_config = GPT2Config.from_pretrained('openai-community/gpt2')\n",
    "\n",
    "            self.gpt2_config.num_hidden_layers = configs.llm_layers\n",
    "            self.gpt2_config.output_attentions = True\n",
    "            self.gpt2_config.output_hidden_states = True\n",
    "            try:\n",
    "                self.llm_model = GPT2Model.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    config=self.gpt2_config,\n",
    "                )\n",
    "            except EnvironmentError:  # downloads model from HF is not already done\n",
    "                print(\"Local model files not found. Attempting to download...\")\n",
    "                self.llm_model = GPT2Model.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.gpt2_config,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads the tokenizer from HF if not already done\n",
    "                print(\"Local tokenizer files not found. Atempting to download them..\")\n",
    "                self.tokenizer = GPT2Tokenizer.from_pretrained(\n",
    "                    'openai-community/gpt2',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "        elif configs.llm_model == 'BERT':\n",
    "            self.bert_config = BertConfig.from_pretrained('google-bert/bert-base-uncased')\n",
    "\n",
    "            self.bert_config.num_hidden_layers = configs.llm_layers\n",
    "            self.bert_config.output_attentions = True\n",
    "            self.bert_config.output_hidden_states = True\n",
    "            try:\n",
    "                self.llm_model = BertModel.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True,\n",
    "                    config=self.bert_config,\n",
    "                )\n",
    "            except EnvironmentError:  # downloads model from HF is not already done\n",
    "                print(\"Local model files not found. Attempting to download...\")\n",
    "                self.llm_model = BertModel.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False,\n",
    "                    config=self.bert_config,\n",
    "                )\n",
    "\n",
    "            try:\n",
    "                self.tokenizer = BertTokenizer.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=True\n",
    "                )\n",
    "            except EnvironmentError:  # downloads the tokenizer from HF if not already done\n",
    "                print(\"Local tokenizer files not found. Atempting to download them..\")\n",
    "                self.tokenizer = BertTokenizer.from_pretrained(\n",
    "                    'google-bert/bert-base-uncased',\n",
    "                    trust_remote_code=True,\n",
    "                    local_files_only=False\n",
    "                )\n",
    "        else:\n",
    "            raise Exception('LLM model is not defined')\n",
    "\n",
    "        if self.tokenizer.eos_token:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        else:\n",
    "            pad_token = '[PAD]'\n",
    "            self.tokenizer.add_special_tokens({'pad_token': pad_token})\n",
    "            self.tokenizer.pad_token = pad_token\n",
    "\n",
    "        for param in self.llm_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        if configs.prompt_domain:\n",
    "            self.description = configs.content\n",
    "        else:\n",
    "            self.description = 'The Electricity Transformer Temperature (ETT) is a crucial indicator in the electric power long-term deployment.'\n",
    "\n",
    "        self.dropout = nn.Dropout(configs.dropout)\n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(\n",
    "            configs.d_model, self.patch_len, self.stride, configs.dropout)\n",
    "\n",
    "        self.word_embeddings = self.llm_model.get_input_embeddings().weight\n",
    "        self.vocab_size = self.word_embeddings.shape[0]\n",
    "        self.num_tokens = 1000\n",
    "        self.mapping_layer = nn.Linear(self.vocab_size, self.num_tokens)\n",
    "\n",
    "        self.reprogramming_layer = ReprogrammingLayer(configs.d_model, configs.n_heads, self.d_ff, self.d_llm)\n",
    "\n",
    "        self.patch_nums = int((configs.seq_len - self.patch_len) / self.stride + 2)\n",
    "        self.head_nf = self.d_ff * self.patch_nums\n",
    "\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            self.output_projection = FlattenHead(configs.enc_in, self.head_nf, self.pred_len,\n",
    "                                                 head_dropout=configs.dropout)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        self.normalize_layers = Normalize(configs.enc_in, affine=False)\n",
    "\n",
    "    def forward(self, x_enc, x_mark_enc, x_dec, x_mark_dec, mask=None):\n",
    "        if self.task_name == 'long_term_forecast' or self.task_name == 'short_term_forecast':\n",
    "            dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)\n",
    "            return dec_out[:, -self.pred_len:, :]\n",
    "        return None\n",
    "\n",
    "    def forecast(self, x_enc, x_mark_enc, x_dec, x_mark_dec):\n",
    "\n",
    "        x_enc = self.normalize_layers(x_enc, 'norm')\n",
    "\n",
    "        B, T, N = x_enc.size()\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous().reshape(B * N, T, 1)\n",
    "\n",
    "        min_values = torch.min(x_enc, dim=1)[0]\n",
    "        max_values = torch.max(x_enc, dim=1)[0]\n",
    "        medians = torch.median(x_enc, dim=1).values\n",
    "        lags = self.calcute_lags(x_enc)\n",
    "        trends = x_enc.diff(dim=1).sum(dim=1)\n",
    "\n",
    "        prompt = []\n",
    "        for b in range(x_enc.shape[0]):\n",
    "            min_values_str = str(min_values[b].tolist()[0])\n",
    "            max_values_str = str(max_values[b].tolist()[0])\n",
    "            median_values_str = str(medians[b].tolist()[0])\n",
    "            lags_values_str = str(lags[b].tolist())\n",
    "            prompt_ = (\n",
    "                f\"<|start_prompt|>Dataset description: {self.description}\"\n",
    "                f\"Task description: forecast the next {str(self.pred_len)} steps given the previous {str(self.seq_len)} steps information; \"\n",
    "                \"Input statistics: \"\n",
    "                f\"min value {min_values_str}, \"\n",
    "                f\"max value {max_values_str}, \"\n",
    "                f\"median value {median_values_str}, \"\n",
    "                f\"the trend of input is {'upward' if trends[b] > 0 else 'downward'}, \"\n",
    "                f\"top 5 lags are : {lags_values_str}<|<end_prompt>|>\"\n",
    "            )\n",
    "\n",
    "            prompt.append(prompt_)\n",
    "\n",
    "        x_enc = x_enc.reshape(B, N, T).permute(0, 2, 1).contiguous() # B, T, N\n",
    "\n",
    "        prompt = self.tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True, max_length=2048).input_ids\n",
    "        prompt_embeddings = self.llm_model.get_input_embeddings()(prompt.to(x_enc.device))  # (batch, prompt_token, dim)\n",
    "\n",
    "        source_embeddings = self.mapping_layer(self.word_embeddings.permute(1, 0)).permute(1, 0)\n",
    "\n",
    "        x_enc = x_enc.permute(0, 2, 1).contiguous() # B N T\n",
    "        enc_out, n_vars = self.patch_embedding(x_enc.to(torch.bfloat16))\n",
    "        enc_out = self.reprogramming_layer(enc_out, source_embeddings, source_embeddings)\n",
    "        llama_enc_out = torch.cat([prompt_embeddings, enc_out], dim=1)\n",
    "        dec_out = self.llm_model(inputs_embeds=llama_enc_out).last_hidden_state\n",
    "        dec_out = dec_out[:, :, :self.d_ff]\n",
    "\n",
    "        dec_out = torch.reshape(\n",
    "            dec_out, (-1, n_vars, dec_out.shape[-2], dec_out.shape[-1]))\n",
    "        dec_out = dec_out.permute(0, 1, 3, 2).contiguous()\n",
    "\n",
    "        dec_out = self.output_projection(dec_out[:, :, :, -self.patch_nums:])\n",
    "        dec_out = dec_out.permute(0, 2, 1).contiguous()\n",
    "\n",
    "        dec_out = self.normalize_layers(dec_out, 'denorm')\n",
    "\n",
    "        return dec_out\n",
    "\n",
    "    @vertexai.preview.developer.mark.train()\n",
    "    def train_model(self, path):\n",
    "        # import torch.multiprocessing as mp\n",
    "        # mp.set_start_method('spawn', force=True)\n",
    "        ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "        deepspeed_plugin = DeepSpeedPlugin(hf_ds_config='./ds_config_zero2.json')\n",
    "        accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)\n",
    "        print(\"Accelerator initialized:\", accelerator)\n",
    "\n",
    "        ## Load datasets \n",
    "        train_data, train_loader = data_provider(self.args, 'train')\n",
    "        vali_data, vali_loader = data_provider(self.args, 'val')\n",
    "        test_data, test_loader = data_provider(self.args, 'test')\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        self.args.content = load_content(self.args)\n",
    "        time_now = time.time()\n",
    "\n",
    "        train_steps = len(train_loader)\n",
    "        early_stopping = EarlyStopping(accelerator=accelerator, patience=self.args.patience)\n",
    "\n",
    "        model_optim = optim.Adam(self.parameters(), lr=self.args.learning_rate)\n",
    "        criterion = smape_loss()\n",
    "\n",
    "\n",
    "        if self.args.lradj == 'COS':\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(model_optim, T_max=20, eta_min=1e-8)\n",
    "        else:\n",
    "            scheduler = lr_scheduler.OneCycleLR(optimizer=model_optim,\n",
    "                                                steps_per_epoch=train_steps,\n",
    "                                                pct_start=self.args.pct_start,\n",
    "                                                epochs=self.args.train_epochs,\n",
    "                                                max_lr=self.args.learning_rate)\n",
    "    \n",
    "\n",
    "        vali_loader, self, model_optim, scheduler = accelerator.prepare(\n",
    "                vali_loader, self, model_optim, scheduler)       \n",
    "\n",
    "        for epoch in range(self.args.train_epochs):\n",
    "            train_data, train_loader = data_provider(self.args, 'train')\n",
    "            train_loader = accelerator.prepare(train_loader)\n",
    "            \n",
    "            iter_count = 0\n",
    "            train_loss = []\n",
    "\n",
    "            self.train()\n",
    "            epoch_time = time.time()\n",
    "\n",
    "            for i, (batch_x, batch_y, batch_x_mark, batch_y_mark) in enumerate(train_loader):\n",
    "                iter_count += 1\n",
    "                model_optim.zero_grad()\n",
    "                batch_x = batch_x.float().to(accelerator.device)\n",
    "\n",
    "                batch_y = batch_y.float().to(accelerator.device)\n",
    "                batch_y_mark = batch_y_mark.float().to(accelerator.device)\n",
    "\n",
    "                # decoder input\n",
    "                dec_inp = torch.zeros_like(batch_y[:, -self.args.pred_len:, :]).float().to(accelerator.device)\n",
    "                dec_inp = torch.cat([batch_y[:, :self.args.label_len, :], dec_inp], dim=1).float().to(\n",
    "                    accelerator.device)\n",
    "\n",
    "                outputs = self(batch_x, None, dec_inp, None)\n",
    "\n",
    "                f_dim = -1 if self.args.features == 'MS' else 0\n",
    "                outputs = outputs[:, -self.args.pred_len:, f_dim:]\n",
    "                batch_y = batch_y[:, -self.args.pred_len:, f_dim:]\n",
    "\n",
    "                batch_y_mark = batch_y_mark[:, -self.args.pred_len:, f_dim:]\n",
    "                loss = criterion(batch_x, 0, outputs, batch_y, batch_y_mark) # 0 cuz we don't need it\n",
    "\n",
    "                train_loss.append(loss.item())\n",
    "\n",
    "                if (i + 1) % 100 == 0:\n",
    "                    accelerator.print(\n",
    "                        \"\\titers: {0}, epoch: {1} | loss: {2:.7f}\".format(i + 1, epoch + 1, loss.item())\n",
    "                    )\n",
    "                    speed = (time.time() - time_now) / iter_count\n",
    "                    left_time = speed * ((self.args.train_epochs - epoch) * train_steps - i)\n",
    "                    accelerator.print('\\tspeed: {:.4f}s/iter; left time: {:.4f}s'.format(speed, left_time))\n",
    "                    iter_count = 0\n",
    "                    time_now = time.time()\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                model_optim.step()\n",
    "\n",
    "                if self.args.lradj == 'TST':\n",
    "                    adjust_learning_rate(accelerator, model_optim, scheduler, epoch + 1, self.args, printout=False)\n",
    "                    scheduler.step()\n",
    "\n",
    "            accelerator.print(\"Epoch: {} cost time: {}\".format(epoch + 1, time.time() - epoch_time))\n",
    "            train_loss = np.average(train_loss)\n",
    "            print('########################################################################')\n",
    "            vali_loss = test_MS(self.args, accelerator, self, train_loader, vali_loader, criterion)\n",
    "            test_loss = vali_loss\n",
    "            accelerator.print(\n",
    "                \"Epoch: {0}, Steps: {1} | Train Loss: {2:.7f} Vali Loss: {3:.7f} Test Loss: {4:.7f}\".format(\n",
    "                    epoch + 1, train_steps, train_loss, vali_loss, test_loss))\n",
    "            early_stopping(vali_loss, self, path)  # model saving\n",
    "            if early_stopping.early_stop:\n",
    "                accelerator.print(\"Early stopping\")\n",
    "                break\n",
    "\n",
    "            if self.args.lradj != 'TST':\n",
    "                adjust_learning_rate(accelerator, model_optim, scheduler, epoch + 1, self.args, printout=True)\n",
    "            else:\n",
    "                accelerator.print('Updating learning rate to {}'.format(scheduler.get_last_lr()[0]))\n",
    "\n",
    "        accelerator.wait_for_everyone()\n",
    "        print(\"Training is Over\")\n",
    "\n",
    "    \n",
    "    def calcute_lags(self, x_enc):\n",
    "        q_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        k_fft = torch.fft.rfft(x_enc.permute(0, 2, 1).contiguous(), dim=-1)\n",
    "        res = q_fft * torch.conj(k_fft)\n",
    "        corr = torch.fft.irfft(res, dim=-1)\n",
    "        mean_value = torch.mean(corr, dim=1)\n",
    "        _, lags = torch.topk(mean_value, self.top_k, dim=-1)\n",
    "        return lags\n",
    "\n",
    "\n",
    "\n",
    "class ReprogrammingLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_keys=None, d_llm=None, attention_dropout=0.1):\n",
    "        super(ReprogrammingLayer, self).__init__()\n",
    "\n",
    "        d_keys = d_keys or (d_model // n_heads)\n",
    "\n",
    "        self.query_projection = nn.Linear(d_model, d_keys * n_heads)\n",
    "        self.key_projection = nn.Linear(d_llm, d_keys * n_heads)\n",
    "        self.value_projection = nn.Linear(d_llm, d_keys * n_heads)\n",
    "        self.out_projection = nn.Linear(d_keys * n_heads, d_llm)\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(attention_dropout)\n",
    "\n",
    "    def forward(self, target_embedding, source_embedding, value_embedding):\n",
    "        B, L, _ = target_embedding.shape\n",
    "        S, _ = source_embedding.shape\n",
    "        H = self.n_heads\n",
    "\n",
    "        target_embedding = self.query_projection(target_embedding).view(B, L, H, -1)\n",
    "        source_embedding = self.key_projection(source_embedding).view(S, H, -1)\n",
    "        value_embedding = self.value_projection(value_embedding).view(S, H, -1)\n",
    "\n",
    "        out = self.reprogramming(target_embedding, source_embedding, value_embedding)\n",
    "\n",
    "        out = out.reshape(B, L, -1)\n",
    "\n",
    "        return self.out_projection(out)\n",
    "\n",
    "    def reprogramming(self, target_embedding, source_embedding, value_embedding):\n",
    "        B, L, H, E = target_embedding.shape\n",
    "\n",
    "        scale = 1. / sqrt(E)\n",
    "\n",
    "        scores = torch.einsum(\"blhe,she->bhls\", target_embedding, source_embedding)\n",
    "\n",
    "        A = self.dropout(torch.softmax(scale * scores, dim=-1))\n",
    "        reprogramming_embedding = torch.einsum(\"bhls,she->blhe\", A, value_embedding)\n",
    "\n",
    "        return reprogramming_embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.task_name = 'short_term_forecast'\n",
    "        self.is_training = 1\n",
    "        self.model_id = 'promo_ean_channel'\n",
    "        self.model_comment = 'EAN_Channel'\n",
    "        self.model = 'TimeLLM'\n",
    "        self.seed = 2021  # Assuming the seed is not explicitly set in the script\n",
    "        self.data = 'promo_ean_channel'\n",
    "        self.root_path = './dataset/true_promo'\n",
    "        self.data_path = 'all_product_true_promo_train.csv'\n",
    "        self.features = 'MS'\n",
    "        self.target = 'sold_units'\n",
    "        self.loader = 'modal'  # Assuming this is a default value\n",
    "        self.freq = 'h'  # Assuming this is a default value\n",
    "        self.checkpoints = './checkpoints/'  # Assuming this is a default value\n",
    "        self.seq_len = 13  # Assuming this is a default value\n",
    "        self.label_len = 1  # Assuming this is a default value\n",
    "        self.pred_len = 17\n",
    "        self.seasonal_patterns = 'Monthly'  # Assuming this is a default value\n",
    "        self.enc_in = 9\n",
    "        self.dec_in = 9\n",
    "        self.c_out = 9\n",
    "        self.d_model = 32\n",
    "        self.n_heads = 8  # Typically set by your model configuration\n",
    "        self.e_layers = 2  # Typically set by your model configuration\n",
    "        self.d_layers = 1  # Typically set by your model configuration\n",
    "        self.d_ff = 128\n",
    "        self.moving_avg = 25  # Assuming default if not specified in the script\n",
    "        self.factor = 3\n",
    "        self.dropout = 0.1  # Assuming default if not specified\n",
    "        self.embed = 'timeF'  # Assuming default if not specified\n",
    "        self.activation = 'gelu'  # Assuming default if not specified\n",
    "        self.output_attention = False  # Assuming default if not specified\n",
    "        self.patch_len = 1\n",
    "        self.stride = 8  # Assuming default if not specified\n",
    "        self.prompt_domain = 0  # Assuming default if not specified\n",
    "        self.llm_model = 'GPT2'\n",
    "        self.llm_dim = 768\n",
    "        self.num_workers = 10  # Default setting\n",
    "        self.itr = 1\n",
    "        self.train_epochs = 2\n",
    "        self.align_epochs = 10  # Assuming default if not specified\n",
    "        self.batch_size = 1\n",
    "        self.eval_batch_size = 1  # Assuming default if not specified\n",
    "        self.patience = 10  # Assuming default if not specified\n",
    "        self.learning_rate = 0.001\n",
    "        self.des = 'Exp'\n",
    "        self.loss = 'spmae'  # Assuming default if not specified\n",
    "        self.lradj = 'type1'  # Assuming default if not specified\n",
    "        self.pct_start = 0.2  # Assuming default if not specified\n",
    "        self.use_amp = False  # Assuming default based on your environment capabilities\n",
    "        self.llm_layers = 32\n",
    "        self.percent = 100  # Assuming default if not specified\n",
    "\n",
    "# Instantiate the Args\n",
    "args = Args()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True)\n",
    "deepspeed_plugin = DeepSpeedPlugin(hf_ds_config='./ds_config_zero2.json')\n",
    "accelerator = Accelerator(kwargs_handlers=[ddp_kwargs], deepspeed_plugin=deepspeed_plugin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "PROJECT_ID = \"itg-bpma-gbl-ww-np\"  # @param {type:\"string\"}\n",
    "REGION = \"europe-west1\" \n",
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}\n",
    "import vertexai\n",
    "REMOTE_JOB_NAME = \"timeseriesllm\"\n",
    "REMOTE_JOB_BUCKET = f\"{BUCKET_URI}/{REMOTE_JOB_NAME}\"\n",
    "\n",
    "vertexai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=REMOTE_JOB_BUCKET,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.data == 'promo_ean_channel':\n",
    "    args.seq_len = int(1.75 * args.pred_len)\n",
    "    args.label_len = args.pred_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accelerator initialized: <accelerate.accelerator.Accelerator object at 0x7bde6c746740>\n",
      "[2024-06-17 10:14:06,733] [INFO] [real_accelerator.py:191:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[2024-06-17 10:14:06,956] [INFO] [logging.py:96:log_dist] [Rank -1] DeepSpeed info: version=0.14.0, git-hash=unknown, git-branch=unknown\n",
      "[2024-06-17 10:14:06,957] [INFO] [comm.py:637:init_distributed] cdb=None\n",
      "[2024-06-17 10:14:06,958] [INFO] [comm.py:652:init_distributed] Not using the DeepSpeed or dist launchers, attempting to detect MPI environment...\n",
      "[2024-06-17 10:14:07,015] [INFO] [comm.py:702:mpi_discovery] Discovered MPI settings of world_rank=0, local_rank=0, world_size=1, master_addr=10.88.0.4, master_port=29500\n",
      "[2024-06-17 10:14:07,016] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[itg-wkst-wadhah-dahouathi-ew1-pd:36662] shmem: mmap: an error occurred while determining whether or not /tmp/ompi.itg-wkst-wadhah-dahouathi-ew1-pd.1000/jf.0/1584201728/shared_mem_cuda_pool.itg-wkst-wadhah-dahouathi-ew1-pd could be created.\n",
      "[itg-wkst-wadhah-dahouathi-ew1-pd:36662] create_and_attach: unable to create shared memory BTL coordinating structure :: size 134217728 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-06-17 10:14:07,874] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False\n",
      "[2024-06-17 10:14:07,877] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer\n",
      "[2024-06-17 10:14:07,877] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer\n",
      "[2024-06-17 10:14:07,911] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = Adam\n",
      "[2024-06-17 10:14:07,911] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=Adam type=<class 'torch.optim.adam.Adam'>\n",
      "[2024-06-17 10:14:07,912] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer\n",
      "[2024-06-17 10:14:07,913] [INFO] [stage_1_and_2.py:149:__init__] Reduce bucket size 200000000\n",
      "[2024-06-17 10:14:07,914] [INFO] [stage_1_and_2.py:150:__init__] Allgather bucket size 200000000\n",
      "[2024-06-17 10:14:07,914] [INFO] [stage_1_and_2.py:151:__init__] CPU Offload: False\n",
      "[2024-06-17 10:14:07,915] [INFO] [stage_1_and_2.py:152:__init__] Round robin gradient partitioning: False\n",
      "[2024-06-17 10:14:08,258] [INFO] [utils.py:800:see_memory_usage] Before initializing optimizer states\n",
      "[2024-06-17 10:14:08,260] [INFO] [utils.py:801:see_memory_usage] MA 0.84 GB         Max_MA 0.94 GB         CA 0.95 GB         Max_CA 1 GB \n",
      "[2024-06-17 10:14:08,262] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 6.12 GB, percent = 41.8%\n",
      "[2024-06-17 10:14:08,506] [INFO] [utils.py:800:see_memory_usage] After initializing optimizer states\n",
      "[2024-06-17 10:14:08,508] [INFO] [utils.py:801:see_memory_usage] MA 0.84 GB         Max_MA 1.04 GB         CA 1.15 GB         Max_CA 1 GB \n",
      "[2024-06-17 10:14:08,509] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 6.12 GB, percent = 41.8%\n",
      "[2024-06-17 10:14:08,510] [INFO] [stage_1_and_2.py:539:__init__] optimizer state initialized\n",
      "[2024-06-17 10:14:08,764] [INFO] [utils.py:800:see_memory_usage] After initializing ZeRO optimizer\n",
      "[2024-06-17 10:14:08,766] [INFO] [utils.py:801:see_memory_usage] MA 0.84 GB         Max_MA 0.84 GB         CA 1.15 GB         Max_CA 1 GB \n",
      "[2024-06-17 10:14:08,768] [INFO] [utils.py:808:see_memory_usage] CPU Virtual Memory:  used = 6.12 GB, percent = 41.8%\n",
      "[2024-06-17 10:14:08,769] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = Adam\n",
      "[2024-06-17 10:14:08,770] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler\n",
      "[2024-06-17 10:14:08,771] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None\n",
      "[2024-06-17 10:14:08,772] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[3.9999999999999996e-05], mom=[(0.95, 0.999)]\n",
      "[2024-06-17 10:14:08,773] [INFO] [config.py:996:print] DeepSpeedEngine configuration:\n",
      "[2024-06-17 10:14:08,774] [INFO] [config.py:1000:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2024-06-17 10:14:08,775] [INFO] [config.py:1000:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2024-06-17 10:14:08,776] [INFO] [config.py:1000:print]   amp_enabled .................. False\n",
      "[2024-06-17 10:14:08,777] [INFO] [config.py:1000:print]   amp_params ................... False\n",
      "[2024-06-17 10:14:08,779] [INFO] [config.py:1000:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": \"autotuning_results\", \n",
      "    \"exps_dir\": \"autotuning_exps\", \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2024-06-17 10:14:08,780] [INFO] [config.py:1000:print]   bfloat16_enabled ............. True\n",
      "[2024-06-17 10:14:08,781] [INFO] [config.py:1000:print]   bfloat16_immediate_grad_update  False\n",
      "[2024-06-17 10:14:08,781] [INFO] [config.py:1000:print]   checkpoint_parallel_write_pipeline  False\n",
      "[2024-06-17 10:14:08,782] [INFO] [config.py:1000:print]   checkpoint_tag_validation_enabled  True\n",
      "[2024-06-17 10:14:08,783] [INFO] [config.py:1000:print]   checkpoint_tag_validation_fail  False\n",
      "[2024-06-17 10:14:08,784] [INFO] [config.py:1000:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7bde3815eef0>\n",
      "[2024-06-17 10:14:08,784] [INFO] [config.py:1000:print]   communication_data_type ...... None\n",
      "[2024-06-17 10:14:08,787] [INFO] [config.py:1000:print]   compile_config ............... enabled=False backend='inductor' kwargs={}\n",
      "[2024-06-17 10:14:08,787] [INFO] [config.py:1000:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}\n",
      "[2024-06-17 10:14:08,788] [INFO] [config.py:1000:print]   curriculum_enabled_legacy .... False\n",
      "[2024-06-17 10:14:08,789] [INFO] [config.py:1000:print]   curriculum_params_legacy ..... False\n",
      "[2024-06-17 10:14:08,790] [INFO] [config.py:1000:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}\n",
      "[2024-06-17 10:14:08,791] [INFO] [config.py:1000:print]   data_efficiency_enabled ...... False\n",
      "[2024-06-17 10:14:08,791] [INFO] [config.py:1000:print]   dataloader_drop_last ......... False\n",
      "[2024-06-17 10:14:08,792] [INFO] [config.py:1000:print]   disable_allgather ............ False\n",
      "[2024-06-17 10:14:08,793] [INFO] [config.py:1000:print]   dump_state ................... False\n",
      "[2024-06-17 10:14:08,793] [INFO] [config.py:1000:print]   dynamic_loss_scale_args ...... None\n",
      "[2024-06-17 10:14:08,794] [INFO] [config.py:1000:print]   eigenvalue_enabled ........... False\n",
      "[2024-06-17 10:14:08,795] [INFO] [config.py:1000:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2024-06-17 10:14:08,795] [INFO] [config.py:1000:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2024-06-17 10:14:08,796] [INFO] [config.py:1000:print]   eigenvalue_layer_num ......... 0\n",
      "[2024-06-17 10:14:08,797] [INFO] [config.py:1000:print]   eigenvalue_max_iter .......... 100\n",
      "[2024-06-17 10:14:08,797] [INFO] [config.py:1000:print]   eigenvalue_stability ......... 1e-06\n",
      "[2024-06-17 10:14:08,798] [INFO] [config.py:1000:print]   eigenvalue_tol ............... 0.01\n",
      "[2024-06-17 10:14:08,799] [INFO] [config.py:1000:print]   eigenvalue_verbose ........... False\n",
      "[2024-06-17 10:14:08,801] [INFO] [config.py:1000:print]   elasticity_enabled ........... False\n",
      "[2024-06-17 10:14:08,802] [INFO] [config.py:1000:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"recompute_fwd_factor\": 0.0, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2024-06-17 10:14:08,803] [INFO] [config.py:1000:print]   fp16_auto_cast ............... None\n",
      "[2024-06-17 10:14:08,804] [INFO] [config.py:1000:print]   fp16_enabled ................. False\n",
      "[2024-06-17 10:14:08,804] [INFO] [config.py:1000:print]   fp16_master_weights_and_gradients  False\n",
      "[2024-06-17 10:14:08,806] [INFO] [config.py:1000:print]   global_rank .................. 0\n",
      "[2024-06-17 10:14:08,807] [INFO] [config.py:1000:print]   grad_accum_dtype ............. None\n",
      "[2024-06-17 10:14:08,808] [INFO] [config.py:1000:print]   gradient_accumulation_steps .. 1\n",
      "[2024-06-17 10:14:08,809] [INFO] [config.py:1000:print]   gradient_clipping ............ 0.0\n",
      "[2024-06-17 10:14:08,809] [INFO] [config.py:1000:print]   gradient_predivide_factor .... 1.0\n",
      "[2024-06-17 10:14:08,810] [INFO] [config.py:1000:print]   graph_harvesting ............. False\n",
      "[2024-06-17 10:14:08,810] [INFO] [config.py:1000:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8\n",
      "[2024-06-17 10:14:08,811] [INFO] [config.py:1000:print]   initial_dynamic_scale ........ 1\n",
      "[2024-06-17 10:14:08,813] [INFO] [config.py:1000:print]   load_universal_checkpoint .... False\n",
      "[2024-06-17 10:14:08,814] [INFO] [config.py:1000:print]   loss_scale ................... 1.0\n",
      "[2024-06-17 10:14:08,814] [INFO] [config.py:1000:print]   memory_breakdown ............. False\n",
      "[2024-06-17 10:14:08,815] [INFO] [config.py:1000:print]   mics_hierarchial_params_gather  False\n",
      "[2024-06-17 10:14:08,816] [INFO] [config.py:1000:print]   mics_shard_size .............. -1\n",
      "[2024-06-17 10:14:08,817] [INFO] [config.py:1000:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False\n",
      "[2024-06-17 10:14:08,817] [INFO] [config.py:1000:print]   nebula_config ................ {\n",
      "    \"enabled\": false, \n",
      "    \"persistent_storage_path\": null, \n",
      "    \"persistent_time_interval\": 100, \n",
      "    \"num_of_version_in_retention\": 2, \n",
      "    \"enable_nebula_load\": true, \n",
      "    \"load_path\": null\n",
      "}\n",
      "[2024-06-17 10:14:08,818] [INFO] [config.py:1000:print]   optimizer_legacy_fusion ...... False\n",
      "[2024-06-17 10:14:08,819] [INFO] [config.py:1000:print]   optimizer_name ............... None\n",
      "[2024-06-17 10:14:08,819] [INFO] [config.py:1000:print]   optimizer_params ............. None\n",
      "[2024-06-17 10:14:08,820] [INFO] [config.py:1000:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}\n",
      "[2024-06-17 10:14:08,822] [INFO] [config.py:1000:print]   pld_enabled .................. False\n",
      "[2024-06-17 10:14:08,823] [INFO] [config.py:1000:print]   pld_params ................... False\n",
      "[2024-06-17 10:14:08,823] [INFO] [config.py:1000:print]   prescale_gradients ........... False\n",
      "[2024-06-17 10:14:08,824] [INFO] [config.py:1000:print]   scheduler_name ............... None\n",
      "[2024-06-17 10:14:08,825] [INFO] [config.py:1000:print]   scheduler_params ............. None\n",
      "[2024-06-17 10:14:08,825] [INFO] [config.py:1000:print]   seq_parallel_communication_data_type  torch.float32\n",
      "[2024-06-17 10:14:08,826] [INFO] [config.py:1000:print]   sparse_attention ............. None\n",
      "[2024-06-17 10:14:08,827] [INFO] [config.py:1000:print]   sparse_gradients_enabled ..... False\n",
      "[2024-06-17 10:14:08,827] [INFO] [config.py:1000:print]   steps_per_print .............. inf\n",
      "[2024-06-17 10:14:08,828] [INFO] [config.py:1000:print]   train_batch_size ............. 1\n",
      "[2024-06-17 10:14:08,829] [INFO] [config.py:1000:print]   train_micro_batch_size_per_gpu  1\n",
      "[2024-06-17 10:14:08,831] [INFO] [config.py:1000:print]   use_data_before_expert_parallel_  False\n",
      "[2024-06-17 10:14:08,832] [INFO] [config.py:1000:print]   use_node_local_storage ....... False\n",
      "[2024-06-17 10:14:08,833] [INFO] [config.py:1000:print]   wall_clock_breakdown ......... False\n",
      "[2024-06-17 10:14:08,833] [INFO] [config.py:1000:print]   weight_quantization_config ... None\n",
      "[2024-06-17 10:14:08,834] [INFO] [config.py:1000:print]   world_size ................... 1\n",
      "[2024-06-17 10:14:08,835] [INFO] [config.py:1000:print]   zero_allow_untested_optimizer  True\n",
      "[2024-06-17 10:14:08,836] [INFO] [config.py:1000:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=200000000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=200000000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1000000000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True\n",
      "[2024-06-17 10:14:08,836] [INFO] [config.py:1000:print]   zero_enabled ................. True\n",
      "[2024-06-17 10:14:08,837] [INFO] [config.py:1000:print]   zero_force_ds_cpu_optimizer .. True\n",
      "[2024-06-17 10:14:08,838] [INFO] [config.py:1000:print]   zero_optimization_stage ...... 2\n",
      "[2024-06-17 10:14:08,841] [INFO] [config.py:986:print_user_config]   json = {\n",
      "    \"bf16\": {\n",
      "        \"enabled\": true, \n",
      "        \"auto_cast\": true\n",
      "    }, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 2, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"allgather_bucket_size\": 2.000000e+08, \n",
      "        \"overlap_comm\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"reduce_bucket_size\": 2.000000e+08, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"sub_group_size\": 1.000000e+09\n",
      "    }, \n",
      "    \"gradient_accumulation_steps\": 1, \n",
      "    \"train_batch_size\": 1, \n",
      "    \"train_micro_batch_size_per_gpu\": 1, \n",
      "    \"steps_per_print\": inf, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": false\n",
      "    }, \n",
      "    \"zero_allow_untested_optimizer\": true\n",
      "}\n",
      "\titers: 100, epoch: 1 | loss: 18.8969898\n",
      "\tspeed: 0.7343s/iter; left time: 74.1654s\n",
      "Epoch: 1 cost time: 70.79673290252686\n",
      "########################################################################\n",
      "Shape of X eval torch.Size([100, 29, 12])\n",
      "Shape of output eval before choosing torch.Size([100, 17, 12])\n",
      "Shape of y eval torch.Size([100, 17, 12])\n",
      "Epoch: 1, Steps: 100 | Train Loss: 29.3534849 Vali Loss: 184.2059627 Test Loss: 184.2059627\n",
      "Updating learning rate to 0.001\n",
      "\titers: 100, epoch: 2 | loss: 68.1883392\n",
      "\tspeed: 1.1629s/iter; left time: 1.1629s\n",
      "Epoch: 2 cost time: 74.06307077407837\n",
      "########################################################################\n",
      "Shape of X eval torch.Size([100, 29, 12])\n",
      "Shape of output eval before choosing torch.Size([100, 17, 12])\n",
      "Shape of y eval torch.Size([100, 17, 12])\n",
      "Epoch: 2, Steps: 100 | Train Loss: 28.1879279 Vali Loss: 183.4095298 Test Loss: 183.4095298\n",
      "Updating learning rate to 0.0005\n",
      "Training is Over\n"
     ]
    }
   ],
   "source": [
    "for ii in range(args.itr):\n",
    "    # setting record of experiments\n",
    "    setting = '{}_{}_{}_{}_ft{}_sl{}_ll{}_pl{}_dm{}_nh{}_el{}_dl{}_df{}_fc{}_eb{}_{}_{}'.format(\n",
    "        args.task_name,\n",
    "        args.model_id,\n",
    "        args.model,\n",
    "        args.data,\n",
    "        args.features,\n",
    "        args.seq_len,\n",
    "        args.label_len,\n",
    "        args.pred_len,\n",
    "        args.d_model,\n",
    "        args.n_heads,\n",
    "        args.e_layers,\n",
    "        args.d_layers,\n",
    "        args.d_ff,\n",
    "        args.factor,\n",
    "        args.embed,\n",
    "        args.des, ii)\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    model = Model(args).float()\n",
    "\n",
    "    path = os.path.join(args.checkpoints,\n",
    "                        setting + '-' + args.model_comment)  # unique checkpoint saving path\n",
    "    args.content = load_content(args)\n",
    "    if not os.path.exists(path) and accelerator.is_local_main_process:\n",
    "        os.makedirs(path)\n",
    "\n",
    "\n",
    "\n",
    "    vertexai.preview.init(remote=True)\n",
    "    model.train_model.vertex.remote_config.container_uri = \"europe-west1-docker.pkg.dev/itg-bpma-gbl-ww-np/yb-vertext-training-rep/yb-vertext-training:latest\"\n",
    "    model.train_model.vertex.remote_config.enable_cuda = True\n",
    "    model.train_model.vertex.remote_config.accelerator_count = 4\n",
    "    model.train_model(path)\n",
    "    torch.save(model.state_dict(), path + '/' + 'checkpoint_v_test1')\n",
    "\n",
    "    \n",
    "    train_data, train_loader = data_provider(args, 'train')\n",
    "    test_data, test_loader = data_provider(args, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vertex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
