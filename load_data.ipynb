{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import bigquery\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"itg-bpma-gbl-ww-np\"  # @param {type:\"string\"}\n",
    "REGION = \"europe-west1\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bq_client = bigquery.Client(\n",
    "    project=PROJECT_ID,  # GCP project used for running the queries and billing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def import_true_promo(client, zero_percent, month, num_weeks,channel=None, fill_discontinuity=False, keep_non_promo=False):\n",
    "    def query(zero_percent, keep_non_promo = False):\n",
    "        \n",
    "\n",
    "        if keep_non_promo:\n",
    "            a = \"\"\"\n",
    "                WITH MinPromoDate AS (\n",
    "                    SELECT\n",
    "                        MIN(end_date) AS min_date\n",
    "                    FROM\n",
    "                        `itg-bpma-gbl-ww-np.bpma_ds_c2_exposed_eu_np.pnl_details_sellout_no_fakes`\n",
    "                    WHERE\n",
    "                        is_promo = TRUE\n",
    "                ),\n",
    "                TransformedData AS (\n",
    "                    SELECT\n",
    "                        start_date,\n",
    "                        end_date,\n",
    "                        sub_axis,\n",
    "                        ean,\n",
    "                        global_channel_type,\n",
    "                        seasonality_index,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date >= (SELECT min_date FROM MinPromoDate) THEN 0\n",
    "                            ELSE price_range\n",
    "                        END AS price_range,\n",
    "                        sold_units,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date >= (SELECT min_date FROM MinPromoDate) THEN ''\n",
    "                            ELSE sub_tactic\n",
    "                        END AS sub_tactic,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date < (SELECT min_date FROM MinPromoDate) THEN NULL\n",
    "                            ELSE is_promo\n",
    "                        END AS is_promo\n",
    "                    FROM\n",
    "                        `itg-bpma-gbl-ww-np.bpma_ds_c2_exposed_eu_np.pnl_details_sellout_no_fakes`\n",
    "                    WHERE\n",
    "                        ean IS NOT NULL AND\n",
    "                        end_date IS NOT NULL\n",
    "                ),\n",
    "                EANThreshold AS (\n",
    "                    SELECT\n",
    "                        ean,\n",
    "                        global_channel_type,\n",
    "                        SUM(CASE WHEN sold_units = 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS ZeroPercent\n",
    "                    FROM\n",
    "                        TransformedData\n",
    "                    GROUP BY\n",
    "                        ean,\n",
    "                        global_channel_type\n",
    "                    HAVING\n",
    "                        ZeroPercent <= {}\n",
    "                )\n",
    "                SELECT\n",
    "                    td.start_date,\n",
    "                    td.end_date,\n",
    "                    td.sub_axis,\n",
    "                    td.ean,\n",
    "                    td.global_channel_type,\n",
    "                    td.seasonality_index,\n",
    "                    td.price_range,\n",
    "                    td.is_promo,\n",
    "                    td.sub_tactic,\n",
    "                    td.sold_units\n",
    "                FROM\n",
    "                    TransformedData td\n",
    "                JOIN\n",
    "                    EANThreshold et\n",
    "                ON\n",
    "                    td.ean = et.ean\n",
    "                    AND td.global_channel_type = et.global_channel_type\n",
    "                WHERE\n",
    "                    td.end_date >= (SELECT min_date FROM MinPromoDate)\n",
    "                \"\"\".format(zero_percent)\n",
    "        else:\n",
    "            a = \"\"\"\n",
    "                WITH MinPromoDate AS (\n",
    "                    SELECT\n",
    "                        MIN(end_date) AS min_date\n",
    "                    FROM\n",
    "                        `itg-bpma-gbl-ww-np.bpma_ds_c2_exposed_eu_np.pnl_details_sellout_no_fakes`\n",
    "                    WHERE\n",
    "                        is_promo = TRUE\n",
    "                ),\n",
    "                TransformedData AS (\n",
    "                    SELECT\n",
    "                        start_date,\n",
    "                        end_date,\n",
    "                        sub_axis,\n",
    "                        ean,\n",
    "                        global_channel_type,\n",
    "                        seasonality_index,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date >= (SELECT min_date FROM MinPromoDate) THEN 0\n",
    "                            ELSE price_range\n",
    "                        END AS price_range,\n",
    "                        sold_units,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date >= (SELECT min_date FROM MinPromoDate) THEN ''\n",
    "                            ELSE sub_tactic\n",
    "                        END AS sub_tactic,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date < (SELECT min_date FROM MinPromoDate) THEN NULL\n",
    "                            ELSE is_promo\n",
    "                        END AS is_promo\n",
    "                    FROM\n",
    "                        `itg-bpma-gbl-ww-np.bpma_ds_c2_exposed_eu_np.pnl_details_sellout_no_fakes`\n",
    "                    WHERE\n",
    "                        ean IS NOT NULL AND\n",
    "                        end_date IS NOT NULL\n",
    "                ),\n",
    "                EANThreshold AS (\n",
    "                    SELECT\n",
    "                        ean,\n",
    "                        global_channel_type,\n",
    "                        SUM(CASE WHEN sold_units = 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS ZeroPercent\n",
    "                    FROM\n",
    "                        TransformedData\n",
    "                    GROUP BY\n",
    "                        ean,\n",
    "                        global_channel_type\n",
    "                    HAVING\n",
    "                        ZeroPercent <= {}\n",
    "                ),\n",
    "                PromoFilter AS (\n",
    "                    SELECT\n",
    "                        ean,\n",
    "                        global_channel_type,\n",
    "                        SUM(CASE WHEN is_promo = TRUE THEN 1 ELSE 0 END) > 0 AS has_promo\n",
    "                    FROM\n",
    "                        TransformedData\n",
    "                    GROUP BY\n",
    "                        ean,\n",
    "                        global_channel_type\n",
    "                    HAVING\n",
    "                        has_promo\n",
    "                )\n",
    "                SELECT\n",
    "                    td.start_date,\n",
    "                    td.end_date,\n",
    "                    td.sub_axis,\n",
    "                    td.ean,\n",
    "                    td.global_channel_type,\n",
    "                    td.seasonality_index,\n",
    "                    td.price_range,\n",
    "                    td.is_promo,\n",
    "                    td.sub_tactic,\n",
    "                    td.sold_units\n",
    "                FROM\n",
    "                    TransformedData td\n",
    "                JOIN\n",
    "                    EANThreshold et\n",
    "                ON\n",
    "                    td.ean = et.ean\n",
    "                    AND td.global_channel_type = et.global_channel_type\n",
    "                JOIN\n",
    "                    PromoFilter pf\n",
    "                ON\n",
    "                    td.ean = pf.ean\n",
    "                    AND td.global_channel_type = pf.global_channel_type\n",
    "                WHERE\n",
    "                    td.end_date >= (SELECT min_date FROM MinPromoDate)\n",
    "                \"\"\".format(zero_percent)\n",
    "        if channel=='Online': \n",
    "            a+=\"\"\"AND td.global_channel_type = 'Online'\n",
    "            ORDER BY\n",
    "                td.end_date;\"\"\"\n",
    "        elif channel=='Offline':\n",
    "            a+=\"\"\"AND td.global_channel_type = 'Offline'\n",
    "            ORDER BY\n",
    "                td.end_date;\"\"\"\n",
    "        else:\n",
    "            a+=\"\"\"\n",
    "            ORDER BY\n",
    "                td.end_date;\"\"\"\n",
    "        return a\n",
    "\n",
    "    data =client.query_and_wait(query(zero_percent, keep_non_promo)).to_dataframe()\n",
    "    print(\"number of products before preprocessing\", data[\"ean\"].unique().shape[0])\n",
    "    # Step 1: Count unique end dates for each EAN and each global channel type\n",
    "    unique_dates = data.groupby(['ean', 'global_channel_type'])['end_date'].nunique().reset_index()\n",
    "\n",
    "    # Step 2: Filter to find EANs where each global channel type has more than 65 unique dates\n",
    "    eans_77_dates = unique_dates[unique_dates['end_date'] >= num_weeks]\n",
    "    valid_eans = eans_77_dates.groupby('ean').filter(lambda x: len(x) == data[\"global_channel_type\"].unique().shape[0] and all(x['end_date'] >= num_weeks))\n",
    "\n",
    "    # Step 3: Filter the original DataFrame to include only these EANs\n",
    "    data = data[data['ean'].isin(valid_eans['ean'])]\n",
    "    data[\"sold_units\"] = data[\"sold_units\"].astype(float)\n",
    "    data = data.sort_values(by=[\"end_date\", \"global_channel_type\", \"ean\"])\n",
    "\n",
    "    data['ean_global_channel'] = data['ean'] + '_' + data['global_channel_type']\n",
    "    data['sub_tactic'] = data['sub_tactic'].str.lower().str.strip()\n",
    "\n",
    "    def aggregate_subtactics(series):\n",
    "        if series is None or all(pd.isnull(series)): \n",
    "            return ''\n",
    "        all_subtactics = set()\n",
    "        for items in series.dropna():\n",
    "            tactics = set(item.strip() for item in items.split(','))\n",
    "            all_subtactics.update(tactics)\n",
    "        return ', '.join(sorted(all_subtactics))\n",
    "\n",
    "    def custom_price_range(series):\n",
    "        return series.mean(skipna=True) if not series.isnull().all() else np.nan\n",
    "\n",
    "    aggregated_data = data.groupby(['start_date', 'end_date', 'ean_global_channel']).agg({\n",
    "        'is_promo': 'first',\n",
    "        'price_range': custom_price_range,\n",
    "        'sub_tactic': aggregate_subtactics,\n",
    "        'sub_axis': 'first',\n",
    "        'seasonality_index': 'first',\n",
    "        'sold_units': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "    aggregated_data.drop_duplicates(inplace=True)\n",
    "    print(\"How many ean_global_channel_type:\", aggregated_data.ean_global_channel.unique().shape[0])\n",
    "    one_hot_encoded_data = aggregated_data['sub_tactic'].str.get_dummies(', ')\n",
    "    empty_sub_tactic_indices = aggregated_data[aggregated_data['sub_tactic'] == ''].index\n",
    "    one_hot_encoded_data.loc[empty_sub_tactic_indices] = 0\n",
    "\n",
    "    final_data = pd.concat([aggregated_data, one_hot_encoded_data], axis=1)\n",
    "    final_data.drop(['sub_tactic'], axis=1, inplace=True)\n",
    "\n",
    "    def shuffle_and_sort(group):\n",
    "        shuffled_group = group.sample(frac=1).reset_index(drop=True)\n",
    "        sorted_group = shuffled_group.sort_values('end_date')\n",
    "        return sorted_group\n",
    "\n",
    "    final_data = final_data.groupby(['ean_global_channel', 'sub_axis'], group_keys=False).apply(shuffle_and_sort).reset_index(drop=True)\n",
    "    final_data.drop([\"start_date\"], axis=1, inplace=True)\n",
    "    final_data['seasonality_index'] = final_data['seasonality_index'].fillna(method='bfill')\n",
    "\n",
    "    if fill_discontinuity:\n",
    "        #  We Create a full date range for each ean_global_channel,\n",
    "        full_data = []\n",
    "        for name, group in final_data.groupby(['ean_global_channel']):\n",
    "            group['end_date'] = pd.to_datetime(group['end_date'])\n",
    "            group.set_index('end_date', inplace=True)\n",
    "            full_range = pd.date_range(start= group.index.min(), end=group.index.max(), freq='W-SAT') #'10-08-2022'\n",
    "            group = group.reindex(full_range).ffill().reset_index().rename(columns={'index': 'end_date'})\n",
    "            full_data.append(group)\n",
    "        final_data = pd.concat(full_data).reset_index(drop=True)\n",
    "\n",
    "    result = final_data.groupby('ean_global_channel')['end_date'].agg(['min', 'max']).reset_index().sort_values(by='max', ascending=False)\n",
    "    max_date_first_row = result.iloc[0][\"max\"]\n",
    "    filtered_channels = result[result['max'] < max_date_first_row]['ean_global_channel'].reset_index(drop=True)\n",
    "\n",
    "    final_data = final_data[~final_data['ean_global_channel'].isin(filtered_channels)]\n",
    "    final_data[\"end_date\"] = pd.to_datetime(final_data[\"end_date\"])\n",
    "    final_data[\"year\"] = final_data[\"end_date\"].dt.year\n",
    "    final_data[\"month\"] = final_data[\"end_date\"].dt.month\n",
    "    final_data[\"week\"] = final_data[\"end_date\"].dt.isocalendar().week\n",
    "\n",
    "    train_set = final_data.loc[((final_data['year'] == 2022) | ((final_data['year'] == 2023) & (final_data['month'] <= month)))]\n",
    "    test_set = final_data.loc[((final_data['year'] == 2023) & (final_data['month'] > month)) | (final_data['year'] == 2024)]\n",
    "\n",
    "\n",
    "    ean_test_date = test_set.groupby(\"ean_global_channel\").end_date.count().reset_index().sort_values('end_date')\n",
    "    max_date_first_row = ean_test_date.iloc[-1][\"end_date\"]\n",
    "\n",
    "    # Filter the ean_global_channel in result where max date is less than the max date of the first row\n",
    "    filtered_channels = ean_test_date[ean_test_date['end_date'] < max_date_first_row]['ean_global_channel'].reset_index(drop=True)\n",
    "\n",
    "    # Filter the original DataFrame based on the filtered ean_global_channel\n",
    "    final_data = final_data[~final_data['ean_global_channel'].isin(filtered_channels)]\n",
    "\n",
    "    train_set = final_data.loc[((final_data['year'] == 2022) | ((final_data['year'] == 2023) & (final_data['month'] <= month)))]\n",
    "    test_set = final_data.loc[((final_data['year'] == 2023) & (final_data['month'] > month)) | (final_data['year'] == 2024)]\n",
    "    print(\"final data product (if changed we remove discontinuity)\", final_data.ean_global_channel.unique().shape[0] )\n",
    "    ean_test_date = test_set.groupby(\"ean_global_channel\").end_date.count().reset_index().sort_values('end_date')\n",
    "    max_date_first_row = ean_test_date.iloc[-1][\"end_date\"]\n",
    "    min_date_first_row = ean_test_date.iloc[0][\"end_date\"]\n",
    "    print(\"prediction length:\", max_date_first_row)\n",
    "    assert min_date_first_row == max_date_first_row , \"min_date_first_row != max_date_first_row\"\n",
    "\n",
    "\n",
    "    return train_set, test_set, max_date_first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_all(client, zero_percent, month,num_weeks, channel=None, fill_discontinuity=False, keep_non_promo=False, interpolation_method=1):\n",
    "    def query(zero_percent, keep_non_promo = False):\n",
    "        if keep_non_promo:\n",
    "            a = \"\"\"\n",
    "                WITH MinPromoDate AS (\n",
    "                    SELECT\n",
    "                        MIN(end_date) AS min_date\n",
    "                    FROM\n",
    "                        `itg-bpma-gbl-ww-np.bpma_ds_c2_exposed_eu_np.pnl_details_sellout_no_fakes`\n",
    "                    WHERE\n",
    "                        is_promo = TRUE\n",
    "                ),\n",
    "                TransformedData AS (\n",
    "                    SELECT\n",
    "                        start_date,\n",
    "                        end_date,\n",
    "                        sub_axis,\n",
    "                        ean,\n",
    "                        global_channel_type,\n",
    "                        seasonality_index,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date >= (SELECT min_date FROM MinPromoDate) THEN 0\n",
    "                            ELSE price_range\n",
    "                        END AS price_range,\n",
    "                        sold_units,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date >= (SELECT min_date FROM MinPromoDate) THEN ''\n",
    "                            ELSE sub_tactic\n",
    "                        END AS sub_tactic,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date < (SELECT min_date FROM MinPromoDate) THEN NULL\n",
    "                            ELSE is_promo\n",
    "                        END AS is_promo\n",
    "                    FROM\n",
    "                        `itg-bpma-gbl-ww-np.bpma_ds_c2_exposed_eu_np.pnl_details_sellout_no_fakes`\n",
    "                    WHERE\n",
    "                        ean IS NOT NULL AND\n",
    "                        end_date IS NOT NULL\n",
    "                ),\n",
    "                EANThreshold AS (\n",
    "                    SELECT\n",
    "                        ean,\n",
    "                        global_channel_type,\n",
    "                        SUM(CASE WHEN sold_units = 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS ZeroPercent\n",
    "                    FROM\n",
    "                        TransformedData\n",
    "                    GROUP BY\n",
    "                        ean,\n",
    "                        global_channel_type\n",
    "                    HAVING\n",
    "                        ZeroPercent <= {}\n",
    "                )\n",
    "                SELECT\n",
    "                    td.start_date,\n",
    "                    td.end_date,\n",
    "                    td.sub_axis,\n",
    "                    td.ean,\n",
    "                    td.global_channel_type,\n",
    "                    td.seasonality_index,\n",
    "                    td.price_range,\n",
    "                    td.is_promo,\n",
    "                    td.sub_tactic,\n",
    "                    td.sold_units\n",
    "                FROM\n",
    "                    TransformedData td\n",
    "                JOIN\n",
    "                    EANThreshold et\n",
    "                ON\n",
    "                    td.ean = et.ean\n",
    "                    AND td.global_channel_type = et.global_channel_type\n",
    "                \"\"\".format(zero_percent)\n",
    "        else:\n",
    "            a = \"\"\"\n",
    "            WITH MinPromoDate AS (\n",
    "                SELECT\n",
    "                    MIN(end_date) AS min_date\n",
    "                FROM\n",
    "                    `itg-bpma-gbl-ww-np.bpma_ds_c2_exposed_eu_np.pnl_details_sellout_no_fakes`\n",
    "                WHERE\n",
    "                    is_promo = TRUE\n",
    "            ),\n",
    "            TransformedData AS (\n",
    "                SELECT\n",
    "                    start_date,\n",
    "                    end_date,\n",
    "                    sub_axis,\n",
    "                    ean,\n",
    "                    global_channel_type,\n",
    "                    seasonality_index,\n",
    "                    CASE\n",
    "                        WHEN is_promo = FALSE AND end_date >= (SELECT min_date FROM MinPromoDate) THEN 0\n",
    "                        ELSE price_range\n",
    "                    END AS price_range,\n",
    "                    sold_units,\n",
    "                    CASE\n",
    "                        WHEN is_promo = FALSE AND end_date >= (SELECT min_date FROM MinPromoDate) THEN ''\n",
    "                        ELSE sub_tactic\n",
    "                    END AS sub_tactic,\n",
    "                    CASE\n",
    "                        WHEN is_promo = FALSE AND end_date < (SELECT min_date FROM MinPromoDate) THEN NULL\n",
    "                        ELSE is_promo\n",
    "                    END AS is_promo\n",
    "                FROM\n",
    "                    `itg-bpma-gbl-ww-np.bpma_ds_c2_exposed_eu_np.pnl_details_sellout_no_fakes`\n",
    "                WHERE\n",
    "                    ean IS NOT NULL AND\n",
    "                    end_date IS NOT NULL\n",
    "            ),\n",
    "            EANThreshold AS (\n",
    "                SELECT\n",
    "                    ean,\n",
    "                    global_channel_type,\n",
    "                    SUM(CASE WHEN sold_units = 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS ZeroPercent\n",
    "                FROM\n",
    "                    TransformedData\n",
    "                GROUP BY\n",
    "                    ean,\n",
    "                    global_channel_type\n",
    "                HAVING\n",
    "                    ZeroPercent <= {}\n",
    "            ),\n",
    "            PromoEANs AS (\n",
    "                SELECT\n",
    "                    ean,\n",
    "                    global_channel_type,\n",
    "                    SUM(CASE WHEN is_promo = TRUE THEN 1 ELSE 0 END) > 0 AS has_promo\n",
    "                FROM\n",
    "                    TransformedData\n",
    "                GROUP BY\n",
    "                    ean,\n",
    "                    global_channel_type\n",
    "                HAVING\n",
    "                    has_promo\n",
    "            )\n",
    "            SELECT\n",
    "                td.start_date,\n",
    "                td.end_date,\n",
    "                td.sub_axis,\n",
    "                td.ean,\n",
    "                td.global_channel_type,\n",
    "                td.seasonality_index,\n",
    "                td.price_range,\n",
    "                td.is_promo,\n",
    "                td.sub_tactic,\n",
    "                td.sold_units\n",
    "            FROM\n",
    "                TransformedData td\n",
    "            JOIN\n",
    "                EANThreshold et\n",
    "            ON\n",
    "                td.ean = et.ean\n",
    "                AND td.global_channel_type = et.global_channel_type\n",
    "            JOIN\n",
    "                PromoEANs pe\n",
    "            ON\n",
    "                td.ean = pe.ean\n",
    "                AND td.global_channel_type = pe.global_channel_type\n",
    "            \"\"\".format(zero_percent)\n",
    "\n",
    "        if channel=='Online': \n",
    "            a+=\"\"\"where td.global_channel_type = 'Online'\n",
    "            ORDER BY\n",
    "                td.end_date;\"\"\"\n",
    "        elif channel=='Offline':\n",
    "            a+=\"\"\"where td.global_channel_type = 'Offline'\n",
    "            ORDER BY\n",
    "                td.end_date;\"\"\"\n",
    "        else:\n",
    "            a+=\"\"\"\n",
    "            ORDER BY\n",
    "                td.end_date;\"\"\"\n",
    "        return a\n",
    "\n",
    "    data =client.query_and_wait(query(zero_percent, keep_non_promo)).to_dataframe()\n",
    "    print(\"number of products before preprocessing\", data[\"ean\"].unique().shape[0])\n",
    "\n",
    "\n",
    "\n",
    "    # Step 1: Count unique end dates for each EAN and each global channel type\n",
    "    unique_dates = data.groupby(['ean', 'global_channel_type'])['end_date'].nunique().reset_index()\n",
    "\n",
    "    # Step 2: Filter to find EANs where each global channel type has more than 65 unique dates\n",
    "    eans_77_dates = unique_dates[unique_dates['end_date'] >= num_weeks]\n",
    "    valid_eans = eans_77_dates.groupby('ean').filter(lambda x: len(x) == data[\"global_channel_type\"].unique().shape[0] and all(x['end_date'] >= num_weeks))\n",
    "\n",
    "    # Step 3: Filter the original DataFrame to include only these EANs\n",
    "    data = data[data['ean'].isin(valid_eans['ean'])]\n",
    "    data[\"sold_units\"] = data[\"sold_units\"].astype(float)\n",
    "    data = data.sort_values(by=[\"end_date\", \"global_channel_type\", \"ean\"])\n",
    "\n",
    "    data['ean_global_channel'] = data['ean'] + '_' + data['global_channel_type']\n",
    "    data['sub_tactic'] = data['sub_tactic'].str.lower().str.strip()\n",
    "\n",
    "    def aggregate_subtactics(series):\n",
    "        if series is None or all(pd.isnull(series)): \n",
    "            return ''\n",
    "        all_subtactics = set()\n",
    "        for items in series.dropna():\n",
    "            tactics = set(item.strip() for item in items.split(','))\n",
    "            all_subtactics.update(tactics)\n",
    "        return ', '.join(sorted(all_subtactics))\n",
    "\n",
    "    def custom_price_range(series):\n",
    "        return series.mean(skipna=True) if not series.isnull().all() else np.nan\n",
    "\n",
    "    aggregated_data = data.groupby(['start_date', 'end_date', 'ean_global_channel']).agg({\n",
    "        'is_promo': 'first',\n",
    "        'price_range': custom_price_range,\n",
    "        'sub_tactic': aggregate_subtactics,\n",
    "        'sub_axis': 'first',\n",
    "        'seasonality_index': 'first',\n",
    "        'sold_units': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "    aggregated_data.drop_duplicates(inplace=True)\n",
    "    print(\"How many ean_global_channel_type:\", aggregated_data.ean_global_channel.unique().shape[0])\n",
    "    one_hot_encoded_data = aggregated_data['sub_tactic'].str.get_dummies(', ')\n",
    "    empty_sub_tactic_indices = aggregated_data[aggregated_data['sub_tactic'] == ''].index\n",
    "    one_hot_encoded_data.loc[empty_sub_tactic_indices] = 0\n",
    "\n",
    "    final_data = pd.concat([aggregated_data, one_hot_encoded_data], axis=1)\n",
    "    final_data.drop(['sub_tactic'], axis=1, inplace=True)\n",
    "\n",
    "    def shuffle_and_sort(group):\n",
    "        shuffled_group = group.sample(frac=1).reset_index(drop=True)\n",
    "        sorted_group = shuffled_group.sort_values('end_date')\n",
    "        return sorted_group\n",
    "\n",
    "    final_data = final_data.groupby(['ean_global_channel', 'sub_axis'], group_keys=False).apply(shuffle_and_sort).reset_index(drop=True)\n",
    "    final_data.drop([\"start_date\"], axis=1, inplace=True)\n",
    "    final_data['seasonality_index'] = final_data['seasonality_index'].fillna(method='bfill')\n",
    "\n",
    "    if fill_discontinuity:\n",
    "        #  We Create a full date range for each ean_global_channel,\n",
    "        full_data = []\n",
    "        for name, group in final_data.groupby(['ean_global_channel']):\n",
    "            group['end_date'] = pd.to_datetime(group['end_date'])\n",
    "            group.set_index('end_date', inplace=True)\n",
    "            full_range = pd.date_range(start= group.index.min(), end=group.index.max(), freq='W-SAT') #'10-08-2022'\n",
    "            group = group.reindex(full_range).ffill().reset_index().rename(columns={'index': 'end_date'})\n",
    "            full_data.append(group)\n",
    "        final_data = pd.concat(full_data).reset_index(drop=True)\n",
    "\n",
    "    result = final_data.groupby('ean_global_channel')['end_date'].agg(['min', 'max']).reset_index().sort_values(by='max', ascending=False)\n",
    "    max_date_first_row = result.iloc[0][\"max\"]\n",
    "    filtered_channels = result[result['max'] < max_date_first_row]['ean_global_channel'].reset_index(drop=True)\n",
    "\n",
    "    final_data = final_data[~final_data['ean_global_channel'].isin(filtered_channels)]\n",
    "    final_data[\"end_date\"] = pd.to_datetime(final_data[\"end_date\"])\n",
    "    final_data[\"year\"] = final_data[\"end_date\"].dt.year\n",
    "    final_data[\"month\"] = final_data[\"end_date\"].dt.month\n",
    "    final_data[\"week\"] = final_data[\"end_date\"].dt.isocalendar().week\n",
    "\n",
    "    train_set = final_data.loc[((final_data['year'] <= 2023) | ((final_data['year'] == 2023) & (final_data['month'] <= month)))]\n",
    "    test_set = final_data.loc[((final_data['year'] == 2023) & (final_data['month'] > month)) | (final_data['year'] == 2024)]\n",
    "\n",
    "\n",
    "    ean_test_date = test_set.groupby(\"ean_global_channel\").end_date.count().reset_index().sort_values('end_date')\n",
    "    max_date_first_row = ean_test_date.iloc[-1][\"end_date\"]\n",
    "\n",
    "    # Filter the ean_global_channel in result where max date is less than the max date of the first row\n",
    "    filtered_channels = ean_test_date[ean_test_date['end_date'] < max_date_first_row]['ean_global_channel'].reset_index(drop=True)\n",
    "\n",
    "    # Filter the original DataFrame based on the filtered ean_global_channel\n",
    "    final_data = final_data[~final_data['ean_global_channel'].isin(filtered_channels)]\n",
    "\n",
    "    train_set = final_data.loc[((final_data['year'] <= 2022) | ((final_data['year'] == 2023) & (final_data['month'] <= month)))]\n",
    "    test_set = final_data.loc[((final_data['year'] == 2023) & (final_data['month'] > month)) | (final_data['year'] == 2024)]\n",
    "    print(\"final data product (if changed we remove discontinuity)\", final_data.ean_global_channel.unique().shape[0] )\n",
    "    ean_test_date = test_set.groupby(\"ean_global_channel\").end_date.count().reset_index().sort_values('end_date')\n",
    "    max_date_first_row = ean_test_date.iloc[-1][\"end_date\"]\n",
    "    min_date_first_row = ean_test_date.iloc[0][\"end_date\"]\n",
    "    print(\"prediction length:\", max_date_first_row)\n",
    "    assert min_date_first_row == max_date_first_row , \"min_date_first_row != max_date_first_row\"\n",
    "\n",
    "    ##################################################################################################\n",
    "    #######################INTERPOLATION STEP#########################################################\n",
    "    if interpolation_method==1:\n",
    "        data=final_data.copy()\n",
    "        data['is_promo'] = data['is_promo'].apply(lambda x: 1 if x is True else (0 if x is False else np.nan))\n",
    "        # Encoding categorical variables\n",
    "        data['sub_axis_encoded'] = LabelEncoder().fit_transform(data['sub_axis'])\n",
    "        data['sold_units'] = pd.to_numeric(data['sold_units'], errors='coerce')\n",
    "\n",
    "        # Separate the dataset into training and prediction sets\n",
    "        train_df = data[data['is_promo'].notna()]\n",
    "        predict_df = data[data['is_promo'].isna()]\n",
    "\n",
    "        # Split the training data into features and labels\n",
    "        X = train_df[['sub_axis_encoded', 'sold_units']]\n",
    "        y = train_df['is_promo']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Train the model\n",
    "        xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='auc', colsample_bytree=1.0, eta=0.1, max_depth=6, min_child_weight=5, subsample=1.0)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the testing set\n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "        # Print the classification report and ROC-AUC score\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "        xgb_model.fit(X, y)\n",
    "        # Predict on the unlabeled data\n",
    "        X_predict = predict_df[['sub_axis_encoded', 'sold_units']]\n",
    "        predict_df['is_promo'] = xgb_model.predict(X_predict)\n",
    "\n",
    "        # Merge the predictions back into the original dataset\n",
    "        data.update(predict_df)\n",
    "        def update_subtactics_and_price_(df):\n",
    "            binary_columns = ['2 for a price', '3 for 2', 'bogof', 'bogshp', 'coupon', 'listing fee', 'online', 'save', 'site fee']\n",
    "            \n",
    "            # Save the original promo indices and price range for later use\n",
    "            original_promo_indices = df[(df['is_promo'] == 1) & (~df['price_range'].isna())].index\n",
    "\n",
    "            if not original_promo_indices.empty:\n",
    "                price_range_promo_true = df.loc[original_promo_indices, 'price_range'].mean()\n",
    "\n",
    "                # Find common values for binary columns using the original promo values\n",
    "                common_values_df = df.loc[original_promo_indices, binary_columns]\n",
    "\n",
    "                \n",
    "                if not common_values_df.empty:\n",
    "                    common_values = common_values_df.mode().iloc[0]\n",
    "                else:\n",
    "                    common_values = pd.Series(0, index=binary_columns)  # Default to 0 if empty\n",
    "                \n",
    "                common_values = common_values.fillna(0)  # Ensure no NaNs in common values\n",
    "\n",
    "            \n",
    "                # Update rows where is_promo is 1 and original is_promo was NaN\n",
    "                promo_indices = df[(df['is_promo'] == 1) & (df['price_range'].isna())].index\n",
    "\n",
    "                \n",
    "                df.loc[promo_indices, 'price_range'] = price_range_promo_true\n",
    "                for col in binary_columns:\n",
    "                    df.loc[promo_indices, col] = common_values[col]\n",
    "            \n",
    "            # Set subtactics and price to zero where is_promo is 0\n",
    "            non_promo_indices = df[df['is_promo'] == 0].index\n",
    "\n",
    "            \n",
    "            df.loc[non_promo_indices, binary_columns] = 0\n",
    "            df.loc[non_promo_indices, 'price_range'] = 0\n",
    "\n",
    "            if original_promo_indices.empty:\n",
    "                print(df.ean_global_channel.iloc[0])\n",
    "            return df\n",
    "        # Apply the function to update subtactics and price_range based on the new predictions\n",
    "        result = data.groupby('ean_global_channel').apply(update_subtactics_and_price_).reset_index(drop=True)\n",
    "        result = result.drop([\"sub_axis_encoded\"], axis=1)\n",
    "    else :\n",
    "        data = final_data.copy()\n",
    "        result = data.groupby('ean_global_channel').apply(process_group).reset_index(drop=True)\n",
    "        result['is_promo'] = result['predicted_promo']\n",
    "        result = result.drop([\"predicted_promo\"], axis=1)\n",
    "    \n",
    "    ##################################################################################################\n",
    "    #######################SPLITTING##################################################################\n",
    "    final_data = result.copy()\n",
    "    final_data = final_data[~final_data['ean_global_channel'].isin(filtered_channels)]\n",
    "\n",
    "    train_set = final_data.loc[((final_data['year'] <= 2022) | ((final_data['year'] == 2023) & (final_data['month'] <= month)))]\n",
    "    test_set = final_data.loc[((final_data['year'] == 2023) & (final_data['month'] > month)) | (final_data['year'] == 2024)]    \n",
    "\n",
    "    assert max_date_first_row* 3 <num_weeks, \"num weeks should be higher than 3 times prediction length\"\n",
    "    return final_data, train_set, test_set, max_date_first_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of products before preprocessing 2412\n",
      "How many ean_global_channel_type: 1928\n",
      "final data product (if changed we remove discontinuity) 1873\n",
      "prediction length: 12\n"
     ]
    }
   ],
   "source": [
    "train_set1, test_set1, prediction_len = import_true_promo(bq_client, 10, 12, 50,channel='Offline', fill_discontinuity=False, keep_non_promo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of products before preprocessing 2083\n",
      "How many ean_global_channel_type: 1801\n",
      "final data product (if changed we remove discontinuity) 1784\n",
      "prediction length: 12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_set1, test_set1, prediction_len = import_true_promo(bq_client, 10, 12, 50,channel='Offline', fill_discontinuity=True, keep_non_promo=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of products before preprocessing 2083\n",
      "How many ean_global_channel_type: 1535\n",
      "final data product (if changed we remove discontinuity) 1429\n",
      "prediction length: 21\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.61      0.12      0.19      2596\n",
      "         1.0       0.89      0.99      0.94     19244\n",
      "\n",
      "    accuracy                           0.89     21840\n",
      "   macro avg       0.75      0.55      0.57     21840\n",
      "weighted avg       0.86      0.89      0.85     21840\n",
      "\n",
      "ROC-AUC Score: 0.5529852379898531\n"
     ]
    }
   ],
   "source": [
    "final_data, train_set, test_set, prediction_len = import_all(bq_client, 10, 10, 100, channel= 'Offline', fill_discontinuity=False,\n",
    "                                                            keep_non_promo=False, interpolation_method=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of products before preprocessing 2083\n",
      "How many ean_global_channel_type: 1535\n",
      "final data product (if changed we remove discontinuity) 1508\n",
      "prediction length: 21\n"
     ]
    }
   ],
   "source": [
    "final_data, train_set, test_set, prediction_len = import_all(bq_client, 10, 10, 100, channel= 'Offline', fill_discontinuity=True,\n",
    "                                                            keep_non_promo=False, interpolation_method=2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "end_date                   0\n",
       "ean_global_channel         0\n",
       "is_promo              179754\n",
       "price_range           179754\n",
       "sub_axis                   0\n",
       "seasonality_index          0\n",
       "sold_units                 0\n",
       "2 for a price              0\n",
       "3 for 2                    0\n",
       "bogof                      0\n",
       "bogshp                     0\n",
       "coupon                     0\n",
       "listing fee                0\n",
       "online                     0\n",
       "save                       0\n",
       "site fee                   0\n",
       "year                       0\n",
       "month                      0\n",
       "week                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpolation:\n",
    "For this we only intrested in a function that takes a ean_global_channel and apply the following changes:\n",
    "- interpolate all positive is_promo and interpolate corresponding subtactics and price_rand\n",
    "- for negative is_promo we change price_range by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, recall_score, precision_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = final_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "end_date                   0\n",
       "ean_global_channel         0\n",
       "is_promo              179780\n",
       "price_range           179780\n",
       "sub_axis                   0\n",
       "seasonality_index          0\n",
       "sold_units                 0\n",
       "2 for a price              0\n",
       "3 for 2                    0\n",
       "bogof                      0\n",
       "bogshp                     0\n",
       "coupon                     0\n",
       "listing fee                0\n",
       "online                     0\n",
       "save                       0\n",
       "site fee                   0\n",
       "year                       0\n",
       "month                      0\n",
       "week                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Function to impute missing values\n",
    "def impute_missing_values(df):\n",
    "    df['is_promo'] = df['is_promo'].apply(lambda x: 1 if x is True else (-1 if x is False else np.nan))\n",
    "    return df\n",
    "\n",
    "# Function to prepare data for PU\n",
    "def prepare_data(df):\n",
    "    df = impute_missing_values(df)\n",
    "    dff = df[['price_range', 'sold_units', '2 for a price', '3 for 2', 'bogof', 'bogshp', 'coupon', 'listing fee', 'online', 'save', 'site fee', 'is_promo']].copy()\n",
    "    pos_ind = np.where(dff['is_promo'] == 1)[0]\n",
    "    if len(pos_ind) == 0:\n",
    "        return None, None, None\n",
    "    np.random.shuffle(pos_ind)\n",
    "    pos_sample_len = int(np.ceil(0.1 * len(pos_ind)))\n",
    "    pos_sample = pos_ind[:pos_sample_len]\n",
    "    \n",
    "    dff.reset_index(drop=True, inplace=True)\n",
    "    dff['class_test'] = -1\n",
    "    dff.loc[pos_sample, 'class_test'] = 1\n",
    "\n",
    "    X_data = dff['sold_units'].values.reshape(-1, 1)  # Reshape to 2D array for XGBoost\n",
    "    y_labeled = dff['class_test'].values\n",
    "    y_positive = dff['is_promo'].values\n",
    "    return X_data, y_labeled, y_positive\n",
    "\n",
    "# Function to fit PU estimator\n",
    "def fit_PU_estimator(X, y, hold_out_ratio, estimator):\n",
    "    positives = np.where(y == 1.0)[0]\n",
    "    hold_out_size = int(np.ceil(len(positives) * hold_out_ratio))\n",
    "    if hold_out_size == 0:\n",
    "        return estimator, 1.0  # Handle case where there are no hold-out samples\n",
    "    np.random.shuffle(positives)\n",
    "    hold_out = positives[:hold_out_size]\n",
    "    X_hold_out = X[hold_out]\n",
    "    X = np.delete(X, hold_out, 0)\n",
    "    y = np.delete(y, hold_out)\n",
    "    \n",
    "    estimator.fit(X, y)\n",
    "    hold_out_predictions = estimator.predict_proba(X_hold_out)[:, 1]\n",
    "    c = np.mean(hold_out_predictions)\n",
    "    return estimator, c\n",
    "\n",
    "# Function to predict PU probabilities\n",
    "def predict_PU_prob(X, estimator, prob_s1y1):\n",
    "    predicted_s = estimator.predict_proba(X)[:, 1]\n",
    "    return predicted_s / prob_s1y1\n",
    "\n",
    "# Function to perform positive unlabeling\n",
    "def positive_unlabeling(df):\n",
    "    X_data, y_labeled, y_positive = prepare_data(df)\n",
    "    if X_data is None or y_labeled is None:\n",
    "        df['predicted_promo'] = df['is_promo']\n",
    "        return df\n",
    "    y_labeled[y_labeled == -1] = 0\n",
    "    predicted = np.zeros(len(X_data))\n",
    "    learning_iterations = 24\n",
    "\n",
    "    for index in range(learning_iterations):\n",
    "        pu_estimator, probs1y1 = fit_PU_estimator(X_data, y_labeled, 0.2, XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "        predicted += predict_PU_prob(X_data, pu_estimator, probs1y1)\n",
    "    \n",
    "    y_predict = [1 if x > 0.9 else 0 for x in (predicted / learning_iterations)]\n",
    "    df['predicted_promo'] = y_predict\n",
    "    return df\n",
    "\n",
    "# Function to update subtactics and price\n",
    "def update_subtactics_and_price(df):\n",
    "    binary_columns = ['2 for a price', '3 for 2', 'bogof', 'bogshp', 'coupon', 'listing fee', 'online', 'save', 'site fee']\n",
    "    \n",
    "    # Save the true promo indices where both predicted_promo and is_promo are 1\n",
    "    true_promo_indices = df[(df['predicted_promo'] == 1) & (df['is_promo'] == 1)].index\n",
    "\n",
    "    if not true_promo_indices.empty:\n",
    "        # Compute mean price range for true promo values\n",
    "        price_range_promo_true = df.loc[true_promo_indices, 'price_range'].mean()\n",
    "\n",
    "        # Find common values for binary columns using true promo values\n",
    "        common_values_df = df.loc[true_promo_indices, binary_columns]\n",
    "        \n",
    "        if not common_values_df.empty:\n",
    "            common_values = common_values_df.mode().iloc[0]\n",
    "        else:\n",
    "            common_values = pd.Series(0, index=binary_columns)  # Default to 0 if empty\n",
    "        \n",
    "        # Ensure no NaNs in common values\n",
    "        common_values = common_values.fillna(0)\n",
    "\n",
    "        # Update rows where predicted_promo is 1 and original is_promo was NaN\n",
    "        promo_indices = df[(df['predicted_promo'] == 1) & (df['is_promo'].isna())].index\n",
    "        df.loc[promo_indices, 'price_range'] = price_range_promo_true\n",
    "        for col in binary_columns:\n",
    "            df.loc[promo_indices, col] = common_values[col]\n",
    "    \n",
    "    # Set subtactics and price to zero where predicted_promo is 0\n",
    "    non_promo_indices = df[df['predicted_promo'] == 0].index\n",
    "    df.loc[non_promo_indices, binary_columns] = 0\n",
    "    df.loc[non_promo_indices, 'price_range'] = 0\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the process to each ean_global_channel group\n",
    "def process_group(group):\n",
    "    group = positive_unlabeling(group)\n",
    "    group = update_subtactics_and_price(group)\n",
    "    return group\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the processing function to each ean_global_channel group\n",
    "result = data.groupby('ean_global_channel').apply(process_group).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "end_date                   0\n",
       "ean_global_channel         0\n",
       "is_promo              179780\n",
       "price_range                0\n",
       "sub_axis                   0\n",
       "seasonality_index          0\n",
       "sold_units                 0\n",
       "2 for a price              0\n",
       "3 for 2                    0\n",
       "bogof                      0\n",
       "bogshp                     0\n",
       "coupon                     0\n",
       "listing fee                0\n",
       "online                     0\n",
       "save                       0\n",
       "site fee                   0\n",
       "year                       0\n",
       "month                      0\n",
       "week                       0\n",
       "predicted_promo            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 264,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=final_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "end_date                   0\n",
       "ean_global_channel         0\n",
       "is_promo              179780\n",
       "price_range           179780\n",
       "sub_axis                   0\n",
       "seasonality_index          0\n",
       "sold_units                 0\n",
       "2 for a price              0\n",
       "3 for 2                    0\n",
       "bogof                      0\n",
       "bogshp                     0\n",
       "coupon                     0\n",
       "listing fee                0\n",
       "online                     0\n",
       "save                       0\n",
       "site fee                   0\n",
       "year                       0\n",
       "month                      0\n",
       "week                       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.63      0.15      0.24      2847\n",
      "         1.0       0.89      0.99      0.94     20212\n",
      "\n",
      "    accuracy                           0.88     23059\n",
      "   macro avg       0.76      0.57      0.59     23059\n",
      "weighted avg       0.86      0.88      0.85     23059\n",
      "\n",
      "ROC-AUC Score: 0.5667982435012194\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "\n",
    "data['is_promo'] = data['is_promo'].apply(lambda x: 1 if x is True else (0 if x is False else np.nan))\n",
    "# Encoding categorical variables\n",
    "data['sub_axis_encoded'] = LabelEncoder().fit_transform(data['sub_axis'])\n",
    "data['sold_units'] = pd.to_numeric(data['sold_units'], errors='coerce')\n",
    "\n",
    "# Separate the dataset into training and prediction sets\n",
    "train_df = data[data['is_promo'].notna()]\n",
    "predict_df = data[data['is_promo'].isna()]\n",
    "\n",
    "# Split the training data into features and labels\n",
    "X = train_df[['sub_axis_encoded', 'sold_units']]\n",
    "y = train_df['is_promo']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='auc', colsample_bytree=1.0, eta=0.1, max_depth=6, min_child_weight=5, subsample=1.0)\n",
    "xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the testing set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Print the classification report and ROC-AUC score\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "xgb_model.fit(X, y)\n",
    "# Predict on the unlabeled data\n",
    "X_predict = predict_df[['sub_axis_encoded', 'sold_units']]\n",
    "predict_df['is_promo'] = xgb_model.predict(X_predict)\n",
    "\n",
    "# Merge the predictions back into the original dataset\n",
    "data.update(predict_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2 for a price    0.0\n",
       "3 for 2          0.0\n",
       "bogof            0.0\n",
       "bogshp           0.0\n",
       "coupon           0.0\n",
       "listing fee      0.0\n",
       "online           1.0\n",
       "save             1.0\n",
       "site fee         1.0\n",
       "Name: 0, dtype: float64"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = data.copy()\n",
    "binary_columns = ['2 for a price', '3 for 2', 'bogof', 'bogshp', 'coupon', 'listing fee', 'online', 'save', 'site fee']\n",
    "original_promo_indices = df[(df['is_promo'] == 1) & (~df['price_range'].isna())].index\n",
    "price_range_promo_true = df.loc[original_promo_indices, 'price_range'].mean()\n",
    "common_values = df.loc[original_promo_indices, binary_columns].mode().iloc[0]\n",
    "common_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_subtactics_and_price(df):\n",
    "    binary_columns = ['2 for a price', '3 for 2', 'bogof', 'bogshp', 'coupon', 'listing fee', 'online', 'save', 'site fee']\n",
    "    \n",
    "    # Save the original promo indices and price range for later use\n",
    "    original_promo_indices = df[(df['is_promo'] == 1) & (~df['price_range'].isna())].index\n",
    "\n",
    "    if not original_promo_indices.empty:\n",
    "        price_range_promo_true = df.loc[original_promo_indices, 'price_range'].mean()\n",
    "\n",
    "        # Find common values for binary columns using the original promo values\n",
    "        common_values_df = df.loc[original_promo_indices, binary_columns]\n",
    "\n",
    "        \n",
    "        if not common_values_df.empty:\n",
    "            common_values = common_values_df.mode().iloc[0]\n",
    "        else:\n",
    "            common_values = pd.Series(0, index=binary_columns)  # Default to 0 if empty\n",
    "        \n",
    "        common_values = common_values.fillna(0)  # Ensure no NaNs in common values\n",
    "\n",
    "    \n",
    "        # Update rows where is_promo is 1 and original is_promo was NaN\n",
    "        promo_indices = df[(df['is_promo'] == 1) & (df['price_range'].isna())].index\n",
    "\n",
    "        \n",
    "        df.loc[promo_indices, 'price_range'] = price_range_promo_true\n",
    "        for col in binary_columns:\n",
    "            df.loc[promo_indices, col] = common_values[col]\n",
    "    \n",
    "    # Set subtactics and price to zero where is_promo is 0\n",
    "    non_promo_indices = df[df['is_promo'] == 0].index\n",
    "\n",
    "    \n",
    "    df.loc[non_promo_indices, binary_columns] = 0\n",
    "    df.loc[non_promo_indices, 'price_range'] = 0\n",
    "\n",
    "    if original_promo_indices.empty:\n",
    "        print(df.ean_global_channel.iloc[0])\n",
    "    return df\n",
    "# Apply the function to update subtactics and price_range based on the new predictions\n",
    "result_class = data.groupby('ean_global_channel').apply(update_subtactics_and_price).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>end_date</th>\n",
       "      <th>ean_global_channel</th>\n",
       "      <th>price_range</th>\n",
       "      <th>sub_axis</th>\n",
       "      <th>seasonality_index</th>\n",
       "      <th>sold_units</th>\n",
       "      <th>2 for a price</th>\n",
       "      <th>3 for 2</th>\n",
       "      <th>bogof</th>\n",
       "      <th>bogshp</th>\n",
       "      <th>coupon</th>\n",
       "      <th>listing fee</th>\n",
       "      <th>online</th>\n",
       "      <th>save</th>\n",
       "      <th>site fee</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>sub_axis_encoded</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>is_promo</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "      <td>208</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          end_date  ean_global_channel  price_range  sub_axis  \\\n",
       "is_promo                                                        \n",
       "0.0             13                  13           13        13   \n",
       "1.0            208                 208          208       208   \n",
       "\n",
       "          seasonality_index  sold_units  2 for a price  3 for 2  bogof  \\\n",
       "is_promo                                                                 \n",
       "0.0                      13          13             13       13     13   \n",
       "1.0                     208         208            208      208    208   \n",
       "\n",
       "          bogshp  coupon  listing fee  online  save  site fee  year  month  \\\n",
       "is_promo                                                                     \n",
       "0.0           13      13           13      13    13        13    13     13   \n",
       "1.0          208     208          208     208   208       208   208    208   \n",
       "\n",
       "          week  sub_axis_encoded  \n",
       "is_promo                          \n",
       "0.0         13                13  \n",
       "1.0        208               208  "
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_class[result_class.ean_global_channel=='800897848934_Offline'].groupby('is_promo').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "import os\n",
    "import pickle\n",
    "\n",
    "\n",
    "####################################################################################################################\n",
    "###########################################INTERPOLATION METHODS####################################################\n",
    "\n",
    "# Function to prepare data for PU\n",
    "def prepare_data(df):\n",
    "    df['is_promo'] = df['is_promo'].apply(lambda x: 1 if x is True else (-1 if x is False else np.nan))\n",
    "    dff = df[['price_range', 'sold_units', '2 for a price', '3 for 2', 'bogof', 'bogshp', 'coupon', 'listing fee', 'online', 'save', 'site fee', 'is_promo']].copy()\n",
    "    pos_ind = np.where(dff['is_promo'] == 1)[0]\n",
    "    if len(pos_ind) == 0:\n",
    "        return None, None, None\n",
    "    np.random.shuffle(pos_ind)\n",
    "    pos_sample_len = int(np.ceil(0.1 * len(pos_ind)))\n",
    "    pos_sample = pos_ind[:pos_sample_len]\n",
    "    \n",
    "    dff.reset_index(drop=True, inplace=True)\n",
    "    dff['class_test'] = -1\n",
    "    dff.loc[pos_sample, 'class_test'] = 1\n",
    "\n",
    "    X_data = dff['sold_units'].values.reshape(-1, 1)  # Reshape to 2D array for XGBoost\n",
    "    y_labeled = dff['class_test'].values\n",
    "    y_positive = dff['is_promo'].values\n",
    "    return X_data, y_labeled, y_positive\n",
    "\n",
    "# Function to fit PU estimator\n",
    "def fit_PU_estimator(X, y, hold_out_ratio, estimator):\n",
    "    positives = np.where(y == 1.0)[0]\n",
    "    hold_out_size = int(np.ceil(len(positives) * hold_out_ratio))\n",
    "    if hold_out_size == 0:\n",
    "        return estimator, 1.0  # Handle case where there are no hold-out samples\n",
    "    np.random.shuffle(positives)\n",
    "    hold_out = positives[:hold_out_size]\n",
    "    X_hold_out = X[hold_out]\n",
    "    X = np.delete(X, hold_out, 0)\n",
    "    y = np.delete(y, hold_out)\n",
    "    \n",
    "    estimator.fit(X, y)\n",
    "    hold_out_predictions = estimator.predict_proba(X_hold_out)[:, 1]\n",
    "    c = np.mean(hold_out_predictions)\n",
    "    return estimator, c\n",
    "\n",
    "# Function to predict PU probabilities\n",
    "def predict_PU_prob(X, estimator, prob_s1y1):\n",
    "    predicted_s = estimator.predict_proba(X)[:, 1]\n",
    "    return predicted_s / prob_s1y1\n",
    "\n",
    "# Function to perform positive unlabeling\n",
    "def positive_unlabeling(df):\n",
    "    X_data, y_labeled, y_positive = prepare_data(df)\n",
    "    if X_data is None or y_labeled is None:\n",
    "        df['predicted_promo'] = df['is_promo']\n",
    "        return df\n",
    "    y_labeled[y_labeled == -1] = 0\n",
    "    predicted = np.zeros(len(X_data))\n",
    "    learning_iterations = 24\n",
    "\n",
    "    for index in range(learning_iterations):\n",
    "        pu_estimator, probs1y1 = fit_PU_estimator(X_data, y_labeled, 0.2, XGBClassifier(use_label_encoder=False, eval_metric='logloss'))\n",
    "        predicted += predict_PU_prob(X_data, pu_estimator, probs1y1)\n",
    "    \n",
    "    y_predict = [1 if x > 0.9 else 0 for x in (predicted / learning_iterations)]\n",
    "    df['predicted_promo'] = y_predict\n",
    "    return df\n",
    "\n",
    "# Function to update subtactics and price\n",
    "def update_subtactics_and_price(df):\n",
    "    binary_columns = ['2 for a price', '3 for 2', 'bogof', 'bogshp', 'coupon', 'listing fee', 'online', 'save', 'site fee']\n",
    "    \n",
    "    # Save the true promo indices where both predicted_promo and is_promo are 1\n",
    "    true_promo_indices = df[(df['predicted_promo'] == 1) & (df['is_promo'] == 1)].index\n",
    "\n",
    "    if not true_promo_indices.empty:\n",
    "        # Compute mean price range for true promo values\n",
    "        price_range_promo_true = df.loc[true_promo_indices, 'price_range'].mean()\n",
    "\n",
    "        # Find common values for binary columns using true promo values\n",
    "        common_values_df = df.loc[true_promo_indices, binary_columns]\n",
    "        \n",
    "        if not common_values_df.empty:\n",
    "            common_values = common_values_df.mode().iloc[0]\n",
    "        else:\n",
    "            common_values = pd.Series(0, index=binary_columns)  # Default to 0 if empty\n",
    "        \n",
    "        # Ensure no NaNs in common values\n",
    "        common_values = common_values.fillna(0)\n",
    "\n",
    "        # Update rows where predicted_promo is 1 and original is_promo was NaN\n",
    "        promo_indices = df[(df['predicted_promo'] == 1) & (df['is_promo'].isna())].index\n",
    "        df.loc[promo_indices, 'price_range'] = price_range_promo_true\n",
    "        for col in binary_columns:\n",
    "            df.loc[promo_indices, col] = common_values[col]\n",
    "    \n",
    "    # Set subtactics and price to zero where predicted_promo is 0\n",
    "    non_promo_indices = df[df['predicted_promo'] == 0].index\n",
    "    df.loc[non_promo_indices, binary_columns] = 0\n",
    "    df.loc[non_promo_indices, 'price_range'] = 0\n",
    "\n",
    "    return df\n",
    "\n",
    "# Apply the process to each ean_global_channel group\n",
    "def process_group(group):\n",
    "    group = positive_unlabeling(group)\n",
    "    group = update_subtactics_and_price(group)\n",
    "    return group\n",
    "\n",
    "\n",
    "def import_true_promo(client, zero_percent, month, num_weeks,channel=None, fill_discontinuity=False, keep_non_promo=False):\n",
    "\n",
    "    \"\"\"\n",
    "    This function download data From gcp\n",
    "    Options:\n",
    "    - zero_percentage os sales values == 0\n",
    "    - month to do the split train/test\n",
    "    - num_weeks minimal we'll add assert num_weeks>3*prediction_length\n",
    "    - channel Both, offline, online\n",
    "    -fill_discontinuity: add the product that has discounuity in values and interpolate them\n",
    "    - keep non_propo: true means we also keep the product that has no promotions during the whole period\n",
    "    \"\"\"\n",
    "    def query(zero_percent, keep_non_promo = False):\n",
    "        \n",
    "\n",
    "        if keep_non_promo:\n",
    "            a = \"\"\"\n",
    "                WITH MinPromoDate AS (\n",
    "                    SELECT\n",
    "                        MIN(end_date) AS min_date\n",
    "                    FROM\n",
    "                        `itg-bpma-gbl-ww-np.bpma_ds_c2_exposed_eu_np.pnl_details_sellout_no_fakes`\n",
    "                    WHERE\n",
    "                        is_promo = TRUE\n",
    "                ),\n",
    "                TransformedData AS (\n",
    "                    SELECT\n",
    "                        start_date,\n",
    "                        end_date,\n",
    "                        sub_axis,\n",
    "                        ean,\n",
    "                        global_channel_type,\n",
    "                        seasonality_index,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date >= (SELECT min_date FROM MinPromoDate) THEN 0\n",
    "                            ELSE price_range\n",
    "                        END AS price_range,\n",
    "                        sold_units,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date >= (SELECT min_date FROM MinPromoDate) THEN ''\n",
    "                            ELSE sub_tactic\n",
    "                        END AS sub_tactic,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date < (SELECT min_date FROM MinPromoDate) THEN NULL\n",
    "                            ELSE is_promo\n",
    "                        END AS is_promo\n",
    "                    FROM\n",
    "                        `itg-bpma-gbl-ww-np.bpma_ds_c2_exposed_eu_np.pnl_details_sellout_no_fakes`\n",
    "                    WHERE\n",
    "                        ean IS NOT NULL AND\n",
    "                        end_date IS NOT NULL\n",
    "                ),\n",
    "                EANThreshold AS (\n",
    "                    SELECT\n",
    "                        ean,\n",
    "                        global_channel_type,\n",
    "                        SUM(CASE WHEN sold_units = 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS ZeroPercent\n",
    "                    FROM\n",
    "                        TransformedData\n",
    "                    GROUP BY\n",
    "                        ean,\n",
    "                        global_channel_type\n",
    "                    HAVING\n",
    "                        ZeroPercent <= {}\n",
    "                )\n",
    "                SELECT\n",
    "                    td.start_date,\n",
    "                    td.end_date,\n",
    "                    td.sub_axis,\n",
    "                    td.ean,\n",
    "                    td.global_channel_type,\n",
    "                    td.seasonality_index,\n",
    "                    td.price_range,\n",
    "                    td.is_promo,\n",
    "                    td.sub_tactic,\n",
    "                    td.sold_units\n",
    "                FROM\n",
    "                    TransformedData td\n",
    "                JOIN\n",
    "                    EANThreshold et\n",
    "                ON\n",
    "                    td.ean = et.ean\n",
    "                    AND td.global_channel_type = et.global_channel_type\n",
    "                WHERE\n",
    "                    td.end_date >= (SELECT min_date FROM MinPromoDate)\n",
    "                \"\"\".format(zero_percent)\n",
    "        else:\n",
    "            a = \"\"\"\n",
    "                WITH MinPromoDate AS (\n",
    "                    SELECT\n",
    "                        MIN(end_date) AS min_date\n",
    "                    FROM\n",
    "                        `itg-bpma-gbl-ww-np.bpma_ds_c2_exposed_eu_np.pnl_details_sellout_no_fakes`\n",
    "                    WHERE\n",
    "                        is_promo = TRUE\n",
    "                ),\n",
    "                TransformedData AS (\n",
    "                    SELECT\n",
    "                        start_date,\n",
    "                        end_date,\n",
    "                        sub_axis,\n",
    "                        ean,\n",
    "                        global_channel_type,\n",
    "                        seasonality_index,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date >= (SELECT min_date FROM MinPromoDate) THEN 0\n",
    "                            ELSE price_range\n",
    "                        END AS price_range,\n",
    "                        sold_units,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date >= (SELECT min_date FROM MinPromoDate) THEN ''\n",
    "                            ELSE sub_tactic\n",
    "                        END AS sub_tactic,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date < (SELECT min_date FROM MinPromoDate) THEN NULL\n",
    "                            ELSE is_promo\n",
    "                        END AS is_promo\n",
    "                    FROM\n",
    "                        `itg-bpma-gbl-ww-np.bpma_ds_c2_exposed_eu_np.pnl_details_sellout_no_fakes`\n",
    "                    WHERE\n",
    "                        ean IS NOT NULL AND\n",
    "                        end_date IS NOT NULL\n",
    "                ),\n",
    "                EANThreshold AS (\n",
    "                    SELECT\n",
    "                        ean,\n",
    "                        global_channel_type,\n",
    "                        SUM(CASE WHEN sold_units = 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS ZeroPercent\n",
    "                    FROM\n",
    "                        TransformedData\n",
    "                    GROUP BY\n",
    "                        ean,\n",
    "                        global_channel_type\n",
    "                    HAVING\n",
    "                        ZeroPercent <= {}\n",
    "                ),\n",
    "                PromoFilter AS (\n",
    "                    SELECT\n",
    "                        ean,\n",
    "                        global_channel_type,\n",
    "                        SUM(CASE WHEN is_promo = TRUE THEN 1 ELSE 0 END) > 0 AS has_promo\n",
    "                    FROM\n",
    "                        TransformedData\n",
    "                    GROUP BY\n",
    "                        ean,\n",
    "                        global_channel_type\n",
    "                    HAVING\n",
    "                        has_promo\n",
    "                )\n",
    "                SELECT\n",
    "                    td.start_date,\n",
    "                    td.end_date,\n",
    "                    td.sub_axis,\n",
    "                    td.ean,\n",
    "                    td.global_channel_type,\n",
    "                    td.seasonality_index,\n",
    "                    td.price_range,\n",
    "                    td.is_promo,\n",
    "                    td.sub_tactic,\n",
    "                    td.sold_units\n",
    "                FROM\n",
    "                    TransformedData td\n",
    "                JOIN\n",
    "                    EANThreshold et\n",
    "                ON\n",
    "                    td.ean = et.ean\n",
    "                    AND td.global_channel_type = et.global_channel_type\n",
    "                JOIN\n",
    "                    PromoFilter pf\n",
    "                ON\n",
    "                    td.ean = pf.ean\n",
    "                    AND td.global_channel_type = pf.global_channel_type\n",
    "                WHERE\n",
    "                    td.end_date >= (SELECT min_date FROM MinPromoDate)\n",
    "                \"\"\".format(zero_percent)\n",
    "        if channel=='Online': \n",
    "            a+=\"\"\"AND td.global_channel_type = 'Online'\n",
    "            ORDER BY\n",
    "                td.end_date;\"\"\"\n",
    "        elif channel=='Offline':\n",
    "            a+=\"\"\"AND td.global_channel_type = 'Offline'\n",
    "            ORDER BY\n",
    "                td.end_date;\"\"\"\n",
    "        else:\n",
    "            a+=\"\"\"\n",
    "            ORDER BY\n",
    "                td.end_date;\"\"\"\n",
    "        return a\n",
    "\n",
    "    data['ean_global_channel'] = data['ean'] + '_' + data['global_channel_type']\n",
    "    print(\"number of products before preprocessing\", data[\"ean_global_channel\"].unique().shape[0])\n",
    "\n",
    "\n",
    "    # Step 1: Count unique end dates for each ean_global_channel\n",
    "    unique_dates = data.groupby('ean_global_channel')['end_date'].nunique().reset_index()\n",
    "\n",
    "    # Step 2: Filter to find ean_global_channels with more than or equal to num_weeks unique dates\n",
    "    valid_ean_global_channels = unique_dates[unique_dates['end_date'] >= num_weeks]['ean_global_channel']\n",
    "\n",
    "    # Step 3: Filter the original DataFrame to include only these ean_global_channels\n",
    "    data = data[data['ean_global_channel'].isin(valid_ean_global_channels)]\n",
    "\n",
    "    data['sub_tactic'] = data['sub_tactic'].str.lower().str.strip()\n",
    "\n",
    "    def aggregate_subtactics(series):\n",
    "        if series is None or all(pd.isnull(series)): \n",
    "            return ''\n",
    "        all_subtactics = set()\n",
    "        for items in series.dropna():\n",
    "            tactics = set(item.strip() for item in items.split(','))\n",
    "            all_subtactics.update(tactics)\n",
    "        return ', '.join(sorted(all_subtactics))\n",
    "\n",
    "    def custom_price_range(series):\n",
    "        return series.mean(skipna=True) if not series.isnull().all() else np.nan\n",
    "\n",
    "    aggregated_data = data.groupby(['start_date', 'end_date', 'ean_global_channel']).agg({\n",
    "        'is_promo': 'first',\n",
    "        'price_range': custom_price_range,\n",
    "        'sub_tactic': aggregate_subtactics,\n",
    "        'sub_axis': 'first',\n",
    "        'seasonality_index': 'first',\n",
    "        'sold_units': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "    aggregated_data.drop_duplicates(inplace=True)\n",
    "    print(\"How many ean_global_channel_type:\", aggregated_data.ean_global_channel.unique().shape[0])\n",
    "    if aggregated_data.ean_global_channel.unique().shape[0] == 0:\n",
    "        raise ValueError(\"Error: No unique ean_global_channel values found.\")\n",
    "    one_hot_encoded_data = aggregated_data['sub_tactic'].str.get_dummies(', ')\n",
    "    empty_sub_tactic_indices = aggregated_data[aggregated_data['sub_tactic'] == ''].index\n",
    "    one_hot_encoded_data.loc[empty_sub_tactic_indices] = 0\n",
    "\n",
    "    final_data = pd.concat([aggregated_data, one_hot_encoded_data], axis=1)\n",
    "    final_data.drop(['sub_tactic'], axis=1, inplace=True)\n",
    "\n",
    "    def shuffle_and_sort(group):\n",
    "        shuffled_group = group.sample(frac=1).reset_index(drop=True)\n",
    "        sorted_group = shuffled_group.sort_values('end_date')\n",
    "        return sorted_group\n",
    "\n",
    "    final_data = final_data.groupby(['ean_global_channel', 'sub_axis'], group_keys=False).apply(shuffle_and_sort).reset_index(drop=True)\n",
    "    final_data.drop([\"start_date\"], axis=1, inplace=True)\n",
    "    final_data['seasonality_index'] = final_data['seasonality_index'].fillna(method='bfill')\n",
    "\n",
    "    if fill_discontinuity:\n",
    "        #  We Create a full date range for each ean_global_channel,\n",
    "        full_data = []\n",
    "        for name, group in final_data.groupby(['ean_global_channel']):\n",
    "            group['end_date'] = pd.to_datetime(group['end_date'])\n",
    "            group.set_index('end_date', inplace=True)\n",
    "            full_range = pd.date_range(start= group.index.min(), end=group.index.max(), freq='W-SAT') #'10-08-2022'\n",
    "            group = group.reindex(full_range).ffill().reset_index().rename(columns={'index': 'end_date'})\n",
    "            full_data.append(group)\n",
    "        final_data = pd.concat(full_data).reset_index(drop=True)\n",
    "\n",
    "    result = final_data.groupby('ean_global_channel')['end_date'].agg(['min', 'max']).reset_index().sort_values(by='max', ascending=False)\n",
    "    max_date_first_row = result.iloc[0][\"max\"]\n",
    "    filtered_channels = result[result['max'] < max_date_first_row]['ean_global_channel'].reset_index(drop=True)\n",
    "\n",
    "    final_data = final_data[~final_data['ean_global_channel'].isin(filtered_channels)]\n",
    "    final_data[\"end_date\"] = pd.to_datetime(final_data[\"end_date\"])\n",
    "    final_data[\"year\"] = final_data[\"end_date\"].dt.year\n",
    "    final_data[\"month\"] = final_data[\"end_date\"].dt.month\n",
    "    final_data[\"week\"] = final_data[\"end_date\"].dt.isocalendar().week\n",
    "\n",
    "    train_set = final_data.loc[((final_data['year'] == 2022) | ((final_data['year'] == 2023) & (final_data['month'] <= month)))]\n",
    "    test_set = final_data.loc[((final_data['year'] == 2023) & (final_data['month'] > month)) | (final_data['year'] == 2024)]\n",
    "\n",
    "\n",
    "    ean_test_date = test_set.groupby(\"ean_global_channel\").end_date.count().reset_index().sort_values('end_date')\n",
    "    max_date_first_row = ean_test_date.iloc[-1][\"end_date\"]\n",
    "\n",
    "    # Filter the ean_global_channel in result where max date is less than the max date of the first row\n",
    "    filtered_channels = ean_test_date[ean_test_date['end_date'] < max_date_first_row]['ean_global_channel'].reset_index(drop=True)\n",
    "\n",
    "    # Filter the original DataFrame based on the filtered ean_global_channel\n",
    "    final_data = final_data[~final_data['ean_global_channel'].isin(filtered_channels)]\n",
    "\n",
    "    train_set = final_data.loc[((final_data['year'] == 2022) | ((final_data['year'] == 2023) & (final_data['month'] <= month)))]\n",
    "    test_set = final_data.loc[((final_data['year'] == 2023) & (final_data['month'] > month)) | (final_data['year'] == 2024)]\n",
    "    print(\"final data product (if changed we remove discontinuity)\", final_data.ean_global_channel.unique().shape[0] )\n",
    "    ean_test_date = test_set.groupby(\"ean_global_channel\").end_date.count().reset_index().sort_values('end_date')\n",
    "    max_date_first_row = ean_test_date.iloc[-1][\"end_date\"]\n",
    "    min_date_first_row = ean_test_date.iloc[0][\"end_date\"]\n",
    "    print(\"prediction length:\", max_date_first_row)\n",
    "    assert min_date_first_row == max_date_first_row , \"min_date_first_row != max_date_first_row\"\n",
    "\n",
    "\n",
    "    return final_data, train_set, test_set, max_date_first_row\n",
    "\n",
    "\n",
    "def import_all(client, zero_percent, month,num_weeks, channel=None, fill_discontinuity=False, keep_non_promo=False, interpolation_method=True):\n",
    "    def query(zero_percent, keep_non_promo = False):\n",
    "        if keep_non_promo:\n",
    "            a = \"\"\"\n",
    "                WITH MinPromoDate AS (\n",
    "                    SELECT\n",
    "                        MIN(end_date) AS min_date\n",
    "                    FROM\n",
    "                        `itg-bpma-gbl-ww-np.bpma_ds_c2_exposed_eu_np.pnl_details_sellout_no_fakes`\n",
    "                    WHERE\n",
    "                        is_promo = TRUE\n",
    "                ),\n",
    "                TransformedData AS (\n",
    "                    SELECT\n",
    "                        start_date,\n",
    "                        end_date,\n",
    "                        sub_axis,\n",
    "                        ean,\n",
    "                        global_channel_type,\n",
    "                        seasonality_index,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date >= (SELECT min_date FROM MinPromoDate) THEN 0\n",
    "                            ELSE price_range\n",
    "                        END AS price_range,\n",
    "                        sold_units,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date >= (SELECT min_date FROM MinPromoDate) THEN ''\n",
    "                            ELSE sub_tactic\n",
    "                        END AS sub_tactic,\n",
    "                        CASE\n",
    "                            WHEN is_promo = FALSE AND end_date < (SELECT min_date FROM MinPromoDate) THEN NULL\n",
    "                            ELSE is_promo\n",
    "                        END AS is_promo\n",
    "                    FROM\n",
    "                        `itg-bpma-gbl-ww-np.bpma_ds_c2_exposed_eu_np.pnl_details_sellout_no_fakes`\n",
    "                    WHERE\n",
    "                        ean IS NOT NULL AND\n",
    "                        end_date IS NOT NULL\n",
    "                ),\n",
    "                EANThreshold AS (\n",
    "                    SELECT\n",
    "                        ean,\n",
    "                        global_channel_type,\n",
    "                        SUM(CASE WHEN sold_units = 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS ZeroPercent\n",
    "                    FROM\n",
    "                        TransformedData\n",
    "                    GROUP BY\n",
    "                        ean,\n",
    "                        global_channel_type\n",
    "                    HAVING\n",
    "                        ZeroPercent <= {}\n",
    "                )\n",
    "                SELECT\n",
    "                    td.start_date,\n",
    "                    td.end_date,\n",
    "                    td.sub_axis,\n",
    "                    td.ean,\n",
    "                    td.global_channel_type,\n",
    "                    td.seasonality_index,\n",
    "                    td.price_range,\n",
    "                    td.is_promo,\n",
    "                    td.sub_tactic,\n",
    "                    td.sold_units\n",
    "                FROM\n",
    "                    TransformedData td\n",
    "                JOIN\n",
    "                    EANThreshold et\n",
    "                ON\n",
    "                    td.ean = et.ean\n",
    "                    AND td.global_channel_type = et.global_channel_type\n",
    "                \"\"\".format(zero_percent)\n",
    "        else:\n",
    "            a = \"\"\"\n",
    "            WITH MinPromoDate AS (\n",
    "                SELECT\n",
    "                    MIN(end_date) AS min_date\n",
    "                FROM\n",
    "                    `itg-bpma-gbl-ww-np.bpma_ds_c2_exposed_eu_np.pnl_details_sellout_no_fakes`\n",
    "                WHERE\n",
    "                    is_promo = TRUE\n",
    "            ),\n",
    "            TransformedData AS (\n",
    "                SELECT\n",
    "                    start_date,\n",
    "                    end_date,\n",
    "                    sub_axis,\n",
    "                    ean,\n",
    "                    global_channel_type,\n",
    "                    seasonality_index,\n",
    "                    CASE\n",
    "                        WHEN is_promo = FALSE AND end_date >= (SELECT min_date FROM MinPromoDate) THEN 0\n",
    "                        ELSE price_range\n",
    "                    END AS price_range,\n",
    "                    sold_units,\n",
    "                    CASE\n",
    "                        WHEN is_promo = FALSE AND end_date >= (SELECT min_date FROM MinPromoDate) THEN ''\n",
    "                        ELSE sub_tactic\n",
    "                    END AS sub_tactic,\n",
    "                    CASE\n",
    "                        WHEN is_promo = FALSE AND end_date < (SELECT min_date FROM MinPromoDate) THEN NULL\n",
    "                        ELSE is_promo\n",
    "                    END AS is_promo\n",
    "                FROM\n",
    "                    `itg-bpma-gbl-ww-np.bpma_ds_c2_exposed_eu_np.pnl_details_sellout_no_fakes`\n",
    "                WHERE\n",
    "                    ean IS NOT NULL AND\n",
    "                    end_date IS NOT NULL\n",
    "            ),\n",
    "            EANThreshold AS (\n",
    "                SELECT\n",
    "                    ean,\n",
    "                    global_channel_type,\n",
    "                    SUM(CASE WHEN sold_units = 0 THEN 1 ELSE 0 END) * 100.0 / COUNT(*) AS ZeroPercent\n",
    "                FROM\n",
    "                    TransformedData\n",
    "                GROUP BY\n",
    "                    ean,\n",
    "                    global_channel_type\n",
    "                HAVING\n",
    "                    ZeroPercent <= {}\n",
    "            ),\n",
    "            PromoEANs AS (\n",
    "                SELECT\n",
    "                    ean,\n",
    "                    global_channel_type,\n",
    "                    SUM(CASE WHEN is_promo = TRUE THEN 1 ELSE 0 END) > 0 AS has_promo\n",
    "                FROM\n",
    "                    TransformedData\n",
    "                GROUP BY\n",
    "                    ean,\n",
    "                    global_channel_type\n",
    "                HAVING\n",
    "                    has_promo\n",
    "            )\n",
    "            SELECT\n",
    "                td.start_date,\n",
    "                td.end_date,\n",
    "                td.sub_axis,\n",
    "                td.ean,\n",
    "                td.global_channel_type,\n",
    "                td.seasonality_index,\n",
    "                td.price_range,\n",
    "                td.is_promo,\n",
    "                td.sub_tactic,\n",
    "                td.sold_units\n",
    "            FROM\n",
    "                TransformedData td\n",
    "            JOIN\n",
    "                EANThreshold et\n",
    "            ON\n",
    "                td.ean = et.ean\n",
    "                AND td.global_channel_type = et.global_channel_type\n",
    "            JOIN\n",
    "                PromoEANs pe\n",
    "            ON\n",
    "                td.ean = pe.ean\n",
    "                AND td.global_channel_type = pe.global_channel_type\n",
    "            \"\"\".format(zero_percent)\n",
    "\n",
    "        if channel=='Online': \n",
    "            a+=\"\"\"where td.global_channel_type = 'Online'\n",
    "            ORDER BY\n",
    "                td.end_date;\"\"\"\n",
    "        elif channel=='Offline':\n",
    "            a+=\"\"\"where td.global_channel_type = 'Offline'\n",
    "            ORDER BY\n",
    "                td.end_date;\"\"\"\n",
    "        else:\n",
    "            a+=\"\"\"\n",
    "            ORDER BY\n",
    "                td.end_date;\"\"\"\n",
    "        return a\n",
    "\n",
    "    data =client.query_and_wait(query(zero_percent, keep_non_promo)).to_dataframe()\n",
    "    data['ean_global_channel'] = data['ean'] + '_' + data['global_channel_type']\n",
    "    print(\"number of products before preprocessing\", data[\"ean_global_channel\"].unique().shape[0])\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Step 1: Count unique end dates for each ean_global_channel\n",
    "    unique_dates = data.groupby('ean_global_channel')['end_date'].nunique().reset_index()\n",
    "\n",
    "    # Step 2: Filter to find ean_global_channels with more than or equal to num_weeks unique dates\n",
    "    valid_ean_global_channels = unique_dates[unique_dates['end_date'] >= num_weeks]['ean_global_channel']\n",
    "\n",
    "    # Step 3: Filter the original DataFrame to include only these ean_global_channels\n",
    "    data = data[data['ean_global_channel'].isin(valid_ean_global_channels)]\n",
    "\n",
    "    # Convert 'sold_units' to float\n",
    "    data[\"sold_units\"] = data[\"sold_units\"].astype(float)\n",
    "\n",
    "    # Sort the data\n",
    "    data = data.sort_values(by=[\"end_date\", \"global_channel_type\", \"ean\"])\n",
    "    data['sub_tactic'] = data['sub_tactic'].str.lower().str.strip()\n",
    "\n",
    "    def aggregate_subtactics(series):\n",
    "        if series is None or all(pd.isnull(series)): \n",
    "            return ''\n",
    "        all_subtactics = set()\n",
    "        for items in series.dropna():\n",
    "            tactics = set(item.strip() for item in items.split(','))\n",
    "            all_subtactics.update(tactics)\n",
    "        return ', '.join(sorted(all_subtactics))\n",
    "\n",
    "    def custom_price_range(series):\n",
    "        return series.mean(skipna=True) if not series.isnull().all() else np.nan\n",
    "\n",
    "    aggregated_data = data.groupby(['start_date', 'end_date', 'ean_global_channel']).agg({\n",
    "        'is_promo': 'first',\n",
    "        'price_range': custom_price_range,\n",
    "        'sub_tactic': aggregate_subtactics,\n",
    "        'sub_axis': 'first',\n",
    "        'seasonality_index': 'first',\n",
    "        'sold_units': 'first'\n",
    "    }).reset_index()\n",
    "\n",
    "    aggregated_data.drop_duplicates(inplace=True)\n",
    "    print(\"How many ean_global_channel_type:\", aggregated_data.ean_global_channel.unique().shape[0])\n",
    "    if aggregated_data.ean_global_channel.unique().shape[0] == 0:\n",
    "        raise ValueError(\"Error: No unique ean_global_channel values found.\")\n",
    "    one_hot_encoded_data = aggregated_data['sub_tactic'].str.get_dummies(', ')\n",
    "    empty_sub_tactic_indices = aggregated_data[aggregated_data['sub_tactic'] == ''].index\n",
    "    one_hot_encoded_data.loc[empty_sub_tactic_indices] = 0\n",
    "\n",
    "    final_data = pd.concat([aggregated_data, one_hot_encoded_data], axis=1)\n",
    "    final_data.drop(['sub_tactic'], axis=1, inplace=True)\n",
    "\n",
    "    def shuffle_and_sort(group):\n",
    "        shuffled_group = group.sample(frac=1).reset_index(drop=True)\n",
    "        sorted_group = shuffled_group.sort_values('end_date')\n",
    "        return sorted_group\n",
    "\n",
    "    final_data = final_data.groupby(['ean_global_channel', 'sub_axis'], group_keys=False).apply(shuffle_and_sort).reset_index(drop=True)\n",
    "    final_data.drop([\"start_date\"], axis=1, inplace=True)\n",
    "    final_data['seasonality_index'] = final_data['seasonality_index'].fillna(method='bfill')\n",
    "\n",
    "    if fill_discontinuity:\n",
    "        #  We Create a full date range for each ean_global_channel,\n",
    "        full_data = []\n",
    "        for name, group in final_data.groupby(['ean_global_channel']):\n",
    "            group['end_date'] = pd.to_datetime(group['end_date'])\n",
    "            group.set_index('end_date', inplace=True)\n",
    "            full_range = pd.date_range(start= group.index.min(), end=group.index.max(), freq='W-SAT') #'10-08-2022'\n",
    "            group = group.reindex(full_range).ffill().reset_index().rename(columns={'index': 'end_date'})\n",
    "            full_data.append(group)\n",
    "        final_data = pd.concat(full_data).reset_index(drop=True)\n",
    "\n",
    "    result = final_data.groupby('ean_global_channel')['end_date'].agg(['min', 'max']).reset_index().sort_values(by='max', ascending=False)\n",
    "    max_date_first_row = result.iloc[0][\"max\"]\n",
    "    filtered_channels = result[result['max'] < max_date_first_row]['ean_global_channel'].reset_index(drop=True)\n",
    "\n",
    "    final_data = final_data[~final_data['ean_global_channel'].isin(filtered_channels)]\n",
    "    final_data[\"end_date\"] = pd.to_datetime(final_data[\"end_date\"])\n",
    "    final_data[\"year\"] = final_data[\"end_date\"].dt.year\n",
    "    final_data[\"month\"] = final_data[\"end_date\"].dt.month\n",
    "    final_data[\"week\"] = final_data[\"end_date\"].dt.isocalendar().week\n",
    "\n",
    "    train_set = final_data.loc[((final_data['year'] <= 2023) | ((final_data['year'] == 2023) & (final_data['month'] <= month)))]\n",
    "    test_set = final_data.loc[((final_data['year'] == 2023) & (final_data['month'] > month)) | (final_data['year'] == 2024)]\n",
    "\n",
    "\n",
    "    ean_test_date = test_set.groupby(\"ean_global_channel\").end_date.count().reset_index().sort_values('end_date')\n",
    "    max_date_first_row = ean_test_date.iloc[-1][\"end_date\"]\n",
    "\n",
    "    # Filter the ean_global_channel in result where max date is less than the max date of the first row\n",
    "    filtered_channels = ean_test_date[ean_test_date['end_date'] < max_date_first_row]['ean_global_channel'].reset_index(drop=True)\n",
    "\n",
    "    # Filter the original DataFrame based on the filtered ean_global_channel\n",
    "    final_data = final_data[~final_data['ean_global_channel'].isin(filtered_channels)]\n",
    "\n",
    "    train_set = final_data.loc[((final_data['year'] <= 2022) | ((final_data['year'] == 2023) & (final_data['month'] <= month)))]\n",
    "    test_set = final_data.loc[((final_data['year'] == 2023) & (final_data['month'] > month)) | (final_data['year'] == 2024)]\n",
    "    print(\"final data product (if changed we remove discontinuity)\", final_data.ean_global_channel.unique().shape[0] )\n",
    "    ean_test_date = test_set.groupby(\"ean_global_channel\").end_date.count().reset_index().sort_values('end_date')\n",
    "    max_date_first_row = ean_test_date.iloc[-1][\"end_date\"]\n",
    "    min_date_first_row = ean_test_date.iloc[0][\"end_date\"]\n",
    "    print(\"prediction length:\", max_date_first_row)\n",
    "    assert min_date_first_row == max_date_first_row , \"min_date_first_row != max_date_first_row\"\n",
    "\n",
    "    ##################################################################################################\n",
    "    #######################INTERPOLATION STEP#########################################################\n",
    "    print(\"Interpolation step starting now\")\n",
    "    if interpolation_method==False:\n",
    "        data=final_data.copy()\n",
    "        data['is_promo'] = data['is_promo'].apply(lambda x: 1 if x is True else (0 if x is False else np.nan))\n",
    "        # Encoding categorical variables\n",
    "        data['sub_axis_encoded'] = LabelEncoder().fit_transform(data['sub_axis'])\n",
    "        data['sold_units'] = pd.to_numeric(data['sold_units'], errors='coerce')\n",
    "\n",
    "        # Separate the dataset into training and prediction sets\n",
    "        train_df = data[data['is_promo'].notna()]\n",
    "        predict_df = data[data['is_promo'].isna()]\n",
    "\n",
    "        # Split the training data into features and labels\n",
    "        X = train_df[['sub_axis_encoded', 'sold_units']]\n",
    "        y = train_df['is_promo']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "        # Train the model\n",
    "        xgb_model = XGBClassifier(use_label_encoder=False, eval_metric='auc', colsample_bytree=1.0, eta=0.1, max_depth=6, min_child_weight=5, subsample=1.0)\n",
    "        xgb_model.fit(X_train, y_train)\n",
    "\n",
    "        # Predict on the testing set\n",
    "        y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "        # Print the classification report and ROC-AUC score\n",
    "        print(classification_report(y_test, y_pred))\n",
    "        print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_pred))\n",
    "\n",
    "        xgb_model.fit(X, y)\n",
    "        # Predict on the unlabeled data\n",
    "        X_predict = predict_df[['sub_axis_encoded', 'sold_units']]\n",
    "        predict_df['is_promo'] = xgb_model.predict(X_predict)\n",
    "\n",
    "        # Merge the predictions back into the original dataset\n",
    "        data.update(predict_df)\n",
    "        def update_subtactics_and_price_(df):\n",
    "            binary_columns = ['2 for a price', '3 for 2', 'bogof', 'bogshp', 'coupon', 'listing fee', 'online', 'save', 'site fee']\n",
    "            \n",
    "            # Save the original promo indices and price range for later use\n",
    "            original_promo_indices = df[(df['is_promo'] == 1) & (~df['price_range'].isna())].index\n",
    "\n",
    "            if not original_promo_indices.empty:\n",
    "                price_range_promo_true = df.loc[original_promo_indices, 'price_range'].mean()\n",
    "\n",
    "                # Find common values for binary columns using the original promo values\n",
    "                common_values_df = df.loc[original_promo_indices, binary_columns]\n",
    "\n",
    "                \n",
    "                if not common_values_df.empty:\n",
    "                    common_values = common_values_df.mode().iloc[0]\n",
    "                else:\n",
    "                    common_values = pd.Series(0, index=binary_columns)  # Default to 0 if empty\n",
    "                \n",
    "                common_values = common_values.fillna(0)  # Ensure no NaNs in common values\n",
    "\n",
    "            \n",
    "                # Update rows where is_promo is 1 and original is_promo was NaN\n",
    "                promo_indices = df[(df['is_promo'] == 1) & (df['price_range'].isna())].index\n",
    "\n",
    "                \n",
    "                df.loc[promo_indices, 'price_range'] = price_range_promo_true\n",
    "                for col in binary_columns:\n",
    "                    df.loc[promo_indices, col] = common_values[col]\n",
    "            \n",
    "            # Set subtactics and price to zero where is_promo is 0\n",
    "            non_promo_indices = df[df['is_promo'] == 0].index\n",
    "\n",
    "            \n",
    "            df.loc[non_promo_indices, binary_columns] = 0\n",
    "            df.loc[non_promo_indices, 'price_range'] = 0\n",
    "\n",
    "            if original_promo_indices.empty:\n",
    "                print(df.ean_global_channel.iloc[0])\n",
    "            return df\n",
    "        # Apply the function to update subtactics and price_range based on the new predictions\n",
    "        result = data.groupby('ean_global_channel').apply(update_subtactics_and_price_).reset_index(drop=True)\n",
    "        result = result.drop([\"sub_axis_encoded\"], axis=1)\n",
    "    else :\n",
    "        data = final_data.copy()\n",
    "        result = data.groupby('ean_global_channel').apply(process_group).reset_index(drop=True)\n",
    "        result['is_promo'] = result['predicted_promo']\n",
    "        result = result.drop([\"predicted_promo\"], axis=1)\n",
    "    \n",
    "    print(\"Interpolation step is done\")\n",
    "    ##################################################################################################\n",
    "    #######################SPLITTING##################################################################\n",
    "    final_data = result.copy()\n",
    "    final_data = final_data[~final_data['ean_global_channel'].isin(filtered_channels)]\n",
    "\n",
    "    train_set = final_data.loc[((final_data['year'] <= 2022) | ((final_data['year'] == 2023) & (final_data['month'] <= month)))]\n",
    "    test_set = final_data.loc[((final_data['year'] == 2023) & (final_data['month'] > month)) | (final_data['year'] == 2024)]    \n",
    "\n",
    "    assert max_date_first_row* 3 <num_weeks, \"num weeks should be higher than 3 times prediction length\"\n",
    "    return final_data, train_set, test_set, max_date_first_row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"itg-bpma-gbl-ww-np\"  # @param {type:\"string\"}\n",
    "REGION = \"europe-west1\" \n",
    "BUCKET_URI = f\"gs://your-bucket-name-{PROJECT_ID}-unique\"  # @param {type:\"string\"}\n",
    "import vertexai\n",
    "from google.cloud import bigquery\n",
    "REMOTE_JOB_NAME = \"timeseriesllm\"\n",
    "REMOTE_JOB_BUCKET = f\"{BUCKET_URI}/{REMOTE_JOB_NAME}\"\n",
    "##################################################################################################\n",
    "vertexai.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    staging_bucket=REMOTE_JOB_BUCKET,\n",
    ")\n",
    "\n",
    "##################################################################################################\n",
    "bq_client = bigquery.Client(\n",
    "    project=PROJECT_ID,  # GCP project used for running the queries and billing\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/envs/vertex/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of products before preprocessing 1510\n",
      "How many ean_global_channel_type: 858\n",
      "final data product (if changed we remove discontinuity) 824\n",
      "prediction length: 43\n",
      "Interpolation step starting now\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.16      0.26      1421\n",
      "         1.0       0.90      0.99      0.95     11253\n",
      "\n",
      "    accuracy                           0.90     12674\n",
      "   macro avg       0.80      0.58      0.60     12674\n",
      "weighted avg       0.88      0.90      0.87     12674\n",
      "\n",
      "ROC-AUC Score: 0.5762670966216031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40662/1837066132.py:720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_df['is_promo'] = xgb_model.predict(X_predict)\n",
      "/tmp/ipykernel_40662/1837066132.py:764: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  result = data.groupby('ean_global_channel').apply(update_subtactics_and_price_).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation step is done\n"
     ]
    }
   ],
   "source": [
    "final_data, train_set, test_set, prediction_len = import_all(bq_client, 0, 5, 172, channel= 'Offline', fill_discontinuity=False,\n",
    "                                                            keep_non_promo=False, interpolation_method=False )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/envs/vertex/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of products before preprocessing 2005\n",
      "How many ean_global_channel_type: 1121\n",
      "final data product (if changed we remove discontinuity) 1071\n",
      "prediction length: 43\n",
      "Interpolation step starting now\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.70      0.12      0.20      1706\n",
      "         1.0       0.91      0.99      0.95     14765\n",
      "\n",
      "    accuracy                           0.90     16471\n",
      "   macro avg       0.80      0.56      0.57     16471\n",
      "weighted avg       0.89      0.90      0.87     16471\n",
      "\n",
      "ROC-AUC Score: 0.5551859158072007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_40662/1837066132.py:720: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  predict_df['is_promo'] = xgb_model.predict(X_predict)\n",
      "/tmp/ipykernel_40662/1837066132.py:764: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  result = data.groupby('ean_global_channel').apply(update_subtactics_and_price_).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Interpolation step is done\n"
     ]
    }
   ],
   "source": [
    "final_data, train_set, test_set, prediction_len = import_all(bq_client, 0, 5, 172, channel= None, fill_discontinuity=False,\n",
    "                                                            keep_non_promo=False, interpolation_method=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/user/miniconda/envs/vertex/lib/python3.10/site-packages/google/cloud/bigquery/table.py:1727: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of products before preprocessing 2412\n",
      "How many ean_global_channel_type: 1928\n",
      "final data product (if changed we remove discontinuity) 1873\n",
      "prediction length: 12\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_set1, test_set1, prediction_len \u001b[38;5;241m=\u001b[39m import_true_promo(bq_client, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m12\u001b[39m, \u001b[38;5;241m50\u001b[39m,channel\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOffline\u001b[39m\u001b[38;5;124m'\u001b[39m, fill_discontinuity\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, keep_non_promo\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"
     ]
    }
   ],
   "source": [
    "train_set1, test_set1, prediction_len = import_true_promo(bq_client, 10, 12, 50,channel='Offline', fill_discontinuity=False, keep_non_promo=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vertex",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
